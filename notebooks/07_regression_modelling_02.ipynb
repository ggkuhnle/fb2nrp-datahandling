{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_fb2nep_7",
   "metadata": {},
   "source": [
    "# FB2NEP Workbook 7 – Confounding, DAGs, and Causal Structure\n",
    "\n",
    "Version 0.0.6\n",
    "\n",
    "This workbook builds on Workbook 6.\n",
    "\n",
    "In Workbook 6, we treated regression as a practical tool for estimating associations\n",
    "between an exposure and an outcome, and we focused on model types (linear, logistic,\n",
    "Cox), assumptions, diagnostics, and basic interpretation of coefficients (β, OR, RR, HR).\n",
    "\n",
    "In this workbook, we move from **association** to **causal thinking**. We introduce:\n",
    "\n",
    "- Confounders.\n",
    "- Colliders.\n",
    "- Mediators.\n",
    "- Directed acyclic graphs (DAGs) as a way to formalise causal assumptions.\n",
    "- Approaches to adjustment (stratification and regression).\n",
    "- The special role of **energy intake** in nutritional epidemiology.\n",
    "- A brief introduction to **counterfactual** thinking.\n",
    "\n",
    "A more formal treatment of causal inference, including modern notation and methods,\n",
    "is given in **Workbook 9**. Here we focus on intuition and on how causal structure\n",
    "affects regression analyses in practice.\n",
    "\n",
    "We will use the synthetic *FB2NEP cohort* throughout. The precise variable names may\n",
    "differ slightly from those used here; if you obtain an error (for example, `KeyError`),\n",
    "carefully check the column names of the dataset and adapt the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FB2NEP bootstrap cell (works both locally and in Colab)\n",
    "#\n",
    "# What this cell does:\n",
    "# - Ensures that we are inside the fb2nep-epi repository.\n",
    "# - In Colab: clones the repository from GitHub if necessary.\n",
    "# - Loads and runs scripts/bootstrap.py.\n",
    "# - Makes the main dataset available as the variable `df`.\n",
    "#\n",
    "# Important:\n",
    "# - You may see messages printed below (for example from pip\n",
    "#   or from the bootstrap script). This is expected.\n",
    "# - You may also see WARNINGS (often in yellow). In most cases\n",
    "#   these are harmless and can be ignored for this module.\n",
    "# - The main thing to watch for is a red error traceback\n",
    "#   (for example FileNotFoundError, ModuleNotFoundError).\n",
    "#   If that happens, please re-run this cell first. If the\n",
    "#   error persists, ask for help.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Configuration: repository location and URL\n",
    "# ------------------------------------------------------------\n",
    "# REPO_URL: address of the GitHub repository.\n",
    "# REPO_DIR: folder name that will be created when cloning.\n",
    "REPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\n",
    "REPO_DIR = \"fb2nep-epi\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Ensure we are inside the fb2nep-epi repository\n",
    "# ------------------------------------------------------------\n",
    "# In local Jupyter, you may already be inside the repository,\n",
    "# for example in fb2nep-epi/notebooks.\n",
    "#\n",
    "# In Colab, the default working directory is /content, so\n",
    "# we need to clone the repository into /content/fb2nep-epi\n",
    "# and then change into that folder.\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "# Case A: we are already in the repository (scripts/bootstrap.py exists here)\n",
    "if (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n",
    "    repo_root = cwd\n",
    "\n",
    "# Case B: we are outside the repository (for example in Colab)\n",
    "else:\n",
    "    repo_root = cwd / REPO_DIR\n",
    "\n",
    "    # Clone the repository if it is not present yet\n",
    "    if not repo_root.is_dir():\n",
    "        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n",
    "    else:\n",
    "        print(f\"Using existing repository at {repo_root}\")\n",
    "\n",
    "    # Change the working directory to the repository root\n",
    "    os.chdir(repo_root)\n",
    "    repo_root = pathlib.Path.cwd()\n",
    "\n",
    "print(f\"Repository root set to: {repo_root}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load scripts/bootstrap.py as a module and call init()\n",
    "# ------------------------------------------------------------\n",
    "# The shared bootstrap script contains all logic to:\n",
    "# - Ensure that required Python packages are installed.\n",
    "# - Ensure that the synthetic dataset exists (and generate it\n",
    "#   if needed).\n",
    "# - Load the dataset into a pandas DataFrame.\n",
    "#\n",
    "# We load the script as a normal Python module (fb2nep_bootstrap)\n",
    "# and then call its init() function.\n",
    "bootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n",
    "\n",
    "if not bootstrap_path.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {bootstrap_path}. \"\n",
    "        \"Please check that the fb2nep-epi repository structure is intact.\"\n",
    "    )\n",
    "\n",
    "# Create a module specification from the file\n",
    "spec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\n",
    "bootstrap = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"fb2nep_bootstrap\"] = bootstrap\n",
    "\n",
    "# Execute the bootstrap script in the context of this module\n",
    "spec.loader.exec_module(bootstrap)\n",
    "\n",
    "# The init() function is defined in scripts/bootstrap.py.\n",
    "# It returns:\n",
    "# - df   : the main synthetic cohort as a pandas DataFrame.\n",
    "# - CTX  : a small context object with paths, flags and settings.\n",
    "df, CTX = bootstrap.init()\n",
    "\n",
    "# Optionally expose a few additional useful variables from the\n",
    "# bootstrap module (if they exist). These are not essential for\n",
    "# most analyses, but can be helpful for advanced use.\n",
    "for name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n",
    "    if hasattr(bootstrap, name):\n",
    "        globals()[name] = getattr(bootstrap, name)\n",
    "\n",
    "print(\"Bootstrap completed successfully.\")\n",
    "print(\"The main dataset is available as the variable `df`.\")\n",
    "print(\"The context object is available as `CTX`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports and quick inspection\n",
    "============================\n",
    "\n",
    "In this cell we:\n",
    "\n",
    "- Import common packages used in this workbook.\n",
    "- Display basic information about the dataset to confirm that it is loaded.\n",
    "\n",
    "The imports are deliberately explicit. Many students using this workbook\n",
    "will not yet have much experience with Python, so we avoid implicit\n",
    "magic and keep the code readable.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import display\n",
    "from scipy.stats import norm\n",
    "\n",
    "from scripts.helpers_tables import or_from_linear_combination\n",
    "\n",
    "print(\"DataFrame shape (rows, columns):\", df.shape)\n",
    "print(\"\\nFirst five rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nVariable types (first 20 columns):\")\n",
    "display(df.dtypes.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assoc_causation_fb2nep_7",
   "metadata": {},
   "source": [
    "## 1. Association and causation\n",
    "\n",
    "Regression models (linear, logistic, Cox) quantify **associations**:\n",
    "\n",
    "- In linear regression, a coefficient describes how the *mean* outcome changes\n",
    "  with the exposure.\n",
    "- In logistic regression, we obtain *odds ratios*.\n",
    "- In Cox regression, we obtain *hazard ratios*.\n",
    "\n",
    "However, public health decisions concern **causal effects**:\n",
    "\n",
    "> *If we changed this exposure (for example, salt intake), what would happen to the outcome?*\n",
    "\n",
    "In observational data, a non-zero regression coefficient does **not** automatically\n",
    "imply a causal effect. Several types of variables can distort or create associations:\n",
    "\n",
    "- **Confounders**: common causes of exposure and outcome that, if unadjusted,\n",
    "  bias estimated effects.\n",
    "- **Colliders**: variables that are caused by two other variables; conditioning\n",
    "  on them can create spurious associations.\n",
    "- **Mediators**: variables lying *on* the causal pathway; adjusting for them can\n",
    "  remove part of a genuine effect.\n",
    "\n",
    "To move from association to causation we need to consider the **causal structure**\n",
    "of the variables. In this workbook we introduce DAGs and basic adjustment strategies.\n",
    "Workbook 9 provides a more formal framework for causal inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dags_intro_fb2nep_7",
   "metadata": {},
   "source": [
    "## 2. DAGs for model development and identification of confounders, colliders, and mediators\n",
    "\n",
    "A **directed acyclic graph (DAG)** is a diagram with arrows that represents\n",
    "assumptions about which variables cause which. It is:\n",
    "\n",
    "- **Directed**: arrows have a direction (cause → effect).\n",
    "- **Acyclic**: there are no feedback loops.\n",
    "\n",
    "DAGs are useful because they make assumptions **explicit** and allow us to reason\n",
    "about which variables we should adjust for in regression models.\n",
    "\n",
    "### 2.1 Constructing a DAG\n",
    "\n",
    "To construct a DAG for a research question:\n",
    "\n",
    "1. **List key variables**\n",
    "   - Exposure (for example, red meat intake).\n",
    "   - Outcome (for example, incident cancer).\n",
    "   - Plausible causes of exposure and outcome (for example, socio-economic status,\n",
    "     age, smoking).\n",
    "\n",
    "2. **Draw arrows according to subject-matter knowledge**\n",
    "   - If variable A can plausibly influence variable B, draw `A → B`.\n",
    "   - Do **not** add arrows simply because two variables are correlated in the data.\n",
    "\n",
    "3. **Identify paths between exposure and outcome**\n",
    "   - Causal paths (exposure → … → outcome).\n",
    "   - Non-causal “backdoor” paths (exposure ← … → outcome) that create confounding.\n",
    "\n",
    "4. **Decide on an adjustment set**\n",
    "   - Choose variables to adjust for so that all non-causal backdoor paths are blocked,\n",
    "     without conditioning on colliders or mediators.\n",
    "\n",
    "![DAG](../_assets/epi_dag.png)\n",
    "\n",
    "### 2.2 Why we do not include everything\n",
    "\n",
    "It might be tempting to adjust for **every available variable**. This is usually\n",
    "a bad idea because:\n",
    "\n",
    "- Adjusting for **colliders** can *create* bias.\n",
    "- Adjusting for **mediators** can remove part of the effect we are interested in\n",
    "  (for example, when estimating the total effect of an exposure).\n",
    "- Adjusting for variables that are neither confounders nor mediators can increase\n",
    "  variance and complicate interpretation without reducing bias.\n",
    "\n",
    "A good DAG includes **enough** variables to capture the main causal structure, but\n",
    "not every variable in the dataset. We use subject-matter knowledge and parsimony:\n",
    "include variables that are plausible causes of exposure and outcome, and that are\n",
    "important for the research question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confounders_intro_fb2nep_7",
   "metadata": {},
   "source": [
    "## 3. Confounders\n",
    "\n",
    "### 3.1 Definition and informal examples\n",
    "\n",
    "A variable is a **confounder** for the association between an exposure and an outcome if:\n",
    "\n",
    "1. It is associated with the exposure.\n",
    "2. It is a cause of, or associated with a cause of, the outcome.\n",
    "3. It is not on the causal pathway from exposure to outcome.\n",
    "\n",
    "Intuitively, a confounder is a variable that makes exposed and unexposed individuals\n",
    "systematically different, in a way that also affects the outcome.\n",
    "\n",
    "Classic informal examples include:\n",
    "\n",
    "- **Number of children and BRCA risk**:\n",
    "  women with more children tend to be older, and age affects the probability of\n",
    "  having developed breast cancer; age can confound the association between\n",
    "  “number of children” and “current breast cancer status”.\n",
    "\n",
    "- **Hair length and income**:\n",
    "  there may appear to be an association between hair length and income if, in a\n",
    "  given setting, women tend to have longer hair than men and also have different\n",
    "  average incomes; sex is a confounder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hippo_conf_example_fb2nep_7",
   "metadata": {},
   "source": [
    "### Hippo example (confounding)\n",
    "\n",
    "**Hippo size and daily grass intake**\n",
    "\n",
    "Suppose we observe that *larger hippos eat more grass per day*.  \n",
    "We might be tempted to conclude that **being large makes hippos eat more**.\n",
    "\n",
    "But consider the underlying biology:\n",
    "\n",
    "- Older hippos tend to be larger (simply through growth).\n",
    "- Older hippos also spend more hours grazing because they no longer play in the water as much as juveniles.\n",
    "\n",
    "**Age** is therefore a confounder:\n",
    "\n",
    "```text\n",
    "       age\n",
    "      /    \\\n",
    " hippo size  grass intake\n",
    "```\n",
    "\n",
    "In causal terms:\n",
    "\n",
    "Size ← Age → Grass intake\n",
    "\n",
    "If we ignore age, the association between hippo size and grass intake partly reflects\n",
    "the fact that older hippos both weigh more and eat more, not necessarily that body\n",
    "    size itself increases grazing behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f4a45",
   "metadata": {},
   "source": [
    "### 3.2 Approaches to adjustment\n",
    "\n",
    "There are two basic ways to adjust for confounders:\n",
    "\n",
    "- **Stratification**\n",
    "  - Analyse the exposure–outcome association *within levels* of the confounder.\n",
    "  - For example, estimate the association separately in high and low socio-economic\n",
    "    status (SES) groups.\n",
    "\n",
    "- **Inclusion in a regression model**\n",
    "  - Include the confounder as a **covariate** (predictor) in the regression model.\n",
    "  - For a linear model:\n",
    "    $$\n",
    "    Y = \\beta_0 + \\beta_1 X + \\beta_2 C + \\varepsilon,\n",
    "    $$\n",
    "    where $ X $ is the exposure and $ C $ is the confounder.\n",
    "  - The coefficient $ \\beta_1 $ is then interpreted as the association between\n",
    "    $ X $ and $ Y $ **for individuals with the same value of $ C $**.\n",
    "\n",
    "In practice, regression models with appropriate covariates are the most common\n",
    "approach, but stratified analyses are useful for checking assumptions and for\n",
    "illustrating confounding.\n",
    "\n",
    "In the next section we use the FB2NEP cohort to demonstrate confounding in a setting\n",
    "where the data-generating mechanism includes known confounders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confounders_example_pa_fb2nep_7",
   "metadata": {},
   "source": [
    "## 3.3 Example: physical activity, SES, and incident CVD\n",
    "\n",
    "We now consider a concrete example using the FB2NEP cohort.\n",
    "\n",
    "- **Exposure**: physical activity (`physical_activity`).\n",
    "- **Outcome**: incident cardiovascular disease during follow-up (`CVD_incident`).\n",
    "- **Potential confounder**: socio-economic status (`SES_class`).\n",
    "\n",
    "From the data generator we know that:\n",
    "\n",
    "- Physical activity (`physical_activity`) is socially patterned: higher SES and less\n",
    "  deprivation are associated with higher activity.\n",
    "- CVD risk is affected by several factors related to SES (for example, smoking,\n",
    "  SBP, BMI, diet).\n",
    "\n",
    "A simplified DAG is:\n",
    "\n",
    "```text\n",
    "SES_class  →  physical_activity\n",
    "SES_class  →  CVD_incident\n",
    "physical_activity  →  CVD_incident   (modest protective effect)\n",
    "```\n",
    "\n",
    "Here, `SES_class` is a **confounder**: it influences both physical activity and CVD.\n",
    "If we estimate the association between physical activity and CVD **without** adjusting\n",
    "for SES, the result may be biased. We therefore:\n",
    "\n",
    "1. Create a binary indicator of “high” physical activity.\n",
    "2. Fit a crude logistic regression model of CVD on high activity.\n",
    "3. Fit an SES-adjusted model.\n",
    "4. Compare the results and repeat the analysis stratified by SES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confounders_demo_pa_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helpers_tables import summarise_logit_coef\n",
    "\n",
    "OUTCOME_VAR = \"CVD_incident\"\n",
    "EXPOSURE_RAW = \"physical_activity\"      # categorical: low / moderate / high\n",
    "    \n",
    "CONF_VAR = \"SES_class\"                  # ABC1 / C2DE\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Check variables and prepare analysis dataset\n",
    "# ---------------------------------------------------------------------\n",
    "for v in [OUTCOME_VAR, EXPOSURE_RAW, CONF_VAR]:\n",
    "    if v not in df.columns:\n",
    "        raise KeyError(\n",
    "            f\"Variable '{v}' not found in df. \"\n",
    "            f\"Available columns (first 20): {list(df.columns)[:20]}\"\n",
    "        )\n",
    "\n",
    "# We define high physical activity as the exposure of interest\n",
    "df_pa = df[[OUTCOME_VAR, EXPOSURE_RAW, CONF_VAR]].dropna().copy()\n",
    "df_pa[\"highPA\"] = (df_pa[EXPOSURE_RAW] == \"high\").astype(int)\n",
    "\n",
    "print(f\"Complete-case sample size: {df_pa.shape[0]} observations\\n\")\n",
    "print(\"Distribution of physical_activity and SES_class:\\n\")\n",
    "display(pd.crosstab(df_pa[EXPOSURE_RAW], df_pa[CONF_VAR], normalize=\"columns\"))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Crude and SES-adjusted logistic regression models\n",
    "# ---------------------------------------------------------------------\n",
    "formula_crude = f\"{OUTCOME_VAR} ~ highPA\"\n",
    "formula_adj   = f\"{OUTCOME_VAR} ~ highPA + C({CONF_VAR})\"\n",
    "\n",
    "m_crude = smf.logit(formula_crude, data=df_pa).fit(disp=False)\n",
    "m_adj   = smf.logit(formula_adj,   data=df_pa).fit(disp=False)\n",
    "\n",
    "rows = []\n",
    "rows.append(\n",
    "    summarise_logit_coef(\n",
    "        m_crude,\n",
    "        var_name=\"highPA\",\n",
    "        label=\"Crude model (no SES adjustment)\"\n",
    "    )\n",
    ")\n",
    "rows.append(\n",
    "    summarise_logit_coef(\n",
    "        m_adj,\n",
    "        var_name=\"highPA\",\n",
    "        label=\"Adjusted model (including SES_class)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summary_pa = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nHigh physical activity vs incident CVD: crude and SES-adjusted models\\n\")\n",
    "display(summary_pa.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confounders_pa_interpret_fb2nep_7",
   "metadata": {},
   "source": [
    "### Interpreting crude vs SES-adjusted models (physical activity)\n",
    "\n",
    "The table summarises the association between **high physical activity** (`highPA`)\n",
    "and incident CVD:\n",
    "\n",
    "- The **crude model** compares high vs non-high physical activity without\n",
    "  adjustment for SES.\n",
    "- The **adjusted model** includes `SES_class` as a covariate.\n",
    "\n",
    "Typical patterns to look for:\n",
    "\n",
    "- If high physical activity is genuinely protective, you might expect an OR < 1.\n",
    "- Because SES is associated with both physical activity and CVD, failing to\n",
    "  adjust for SES can bias the crude OR towards or away from 1.\n",
    "\n",
    "In this synthetic cohort you will usually find that:\n",
    "\n",
    "- The **crude OR** for `highPA` is only modestly below 1 (weak protective effect).\n",
    "- The **SES-adjusted OR** is somewhat further from 1 (stronger apparent protection).\n",
    "\n",
    "This is consistent with **positive confounding**: high SES participants tend to be\n",
    "more physically active *and* at lower CVD risk for other reasons. When SES is left\n",
    "unadjusted, part of this benefit is incorrectly attributed to physical activity.\n",
    "Once SES is included in the model, the association for `highPA` more closely\n",
    "reflects the effect of physical activity *for individuals with the same SES*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stratification_pa_fb2nep_7",
   "metadata": {},
   "source": [
    "### 3.4 Stratification by SES\n",
    "\n",
    "Regression adjustment is one way to account for confounding. Another is to analyse\n",
    "the exposure–outcome association **within strata** of the confounder.\n",
    "\n",
    "Here we:\n",
    "\n",
    "1. Split the data into two strata: `SES_class = ABC1` and `SES_class = C2DE`.\n",
    "2. In each stratum, fit a logistic regression model:\n",
    "   CVD_incident ~ highPA\n",
    "3. Compare the stratum-specific odds ratios.\n",
    "\n",
    "If SES is a confounder rather than an effect modifier, we would expect the\n",
    "stratum-specific ORs for `highPA` to be **more similar** to each other and to\n",
    "the SES-adjusted OR, and different from the crude OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stratification_pa_code_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stratified analysis: highPA and CVD_incident within SES strata.\"\"\"\n",
    "\n",
    "rows_strata = []\n",
    "\n",
    "print(\"Stratified analyses by SES_class (separate models in each stratum):\\n\")\n",
    "\n",
    "for level in sorted(df_pa[CONF_VAR].unique()):\n",
    "    df_stratum = df_pa[df_pa[CONF_VAR] == level]\n",
    "    print(f\"  Stratum {level}: n = {df_stratum.shape[0]}\")\n",
    "\n",
    "    # Need variation in exposure and outcome\n",
    "    \n",
    "    if df_stratum[OUTCOME_VAR].nunique() < 2 or df_stratum[\"highPA\"].nunique() < 2:\n",
    "        print(\"    Not enough variation in outcome or exposure for logistic regression.\\n\")\n",
    "        continue\n",
    "\n",
    "    m_stratum = smf.logit(f\"{OUTCOME_VAR} ~ highPA\", data=df_stratum).fit(disp=False)\n",
    "    rows_strata.append(\n",
    "        summarise_logit_coef(\n",
    "            m_stratum,\n",
    "            var_name=\"highPA\",\n",
    "            label=f\"SES_class = {level}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "if rows_strata:\n",
    "    summary_strata_pa = pd.DataFrame(rows_strata)\n",
    "    print(\"\\nStratum-specific odds ratios for highPA (by SES_class):\\n\")\n",
    "    display(summary_strata_pa.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stratification_pa_interpret_fb2nep_7",
   "metadata": {},
   "source": [
    "### Interpreting the SES-stratified highPA–CVD associations\n",
    "\n",
    "The table above shows the association between high physical activity (`highPA`) and\n",
    "incident CVD within strata of socio-economic status (`SES_class`):\n",
    "\n",
    "- **ABC1**  \n",
    "  - OR ≈ 1.05 (95 % CI 0.91 to 1.20), p ≈ 0.52  \n",
    "  - Point estimate slightly **above** 1.0, but the confidence interval is wide and\n",
    "    clearly includes 1.0 → compatible with no association.\n",
    "\n",
    "- **C2DE**  \n",
    "  - OR ≈ 0.89 (95 % CI 0.76 to 1.03), p ≈ 0.11  \n",
    "  - Point estimate slightly **below** 1.0, again with a confidence interval that\n",
    "    includes 1.0 → also compatible with no association.\n",
    "\n",
    "A few points to emphasise:\n",
    "\n",
    "- The **effect sizes are small** in both strata, and statistically weak.  \n",
    "  There is no strong evidence that high physical activity is clearly protective or\n",
    "  clearly harmful for CVD in either SES group in this synthetic cohort.\n",
    "\n",
    "- The **directions differ** slightly (OR > 1 in ABC1, OR < 1 in C2DE), but the\n",
    "  confidence intervals are wide and largely overlapping. This means that the apparent\n",
    "  difference between strata is easily explained by **random variation**.\n",
    "\n",
    "- As a worked example, this is closer to “what often happens in practice” than to a\n",
    "  textbook Simpson’s paradox:  \n",
    "  adjusting or stratifying can change effect estimates a little, but not every\n",
    "  plausible confounder produces a dramatic shift.\n",
    "\n",
    "For here, key messages are:\n",
    "\n",
    "- Do not over-interpret small differences in odds ratios when confidence intervals are\n",
    "  wide and overlapping.\n",
    "- Stratified analyses are still useful: they make us **look** at whether the exposure–outcome\n",
    "  association is similar across subgroups, and they remind us that confounding and effect\n",
    "  modification are empirical questions, not assumptions built into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "energy_intro_fb2nep_7",
   "metadata": {},
   "source": [
    "## 4. Special case: energy intake in nutritional epidemiology\n",
    "\n",
    "### 4.1 Why total energy intake is different\n",
    "\n",
    "In nutritional epidemiology, **total energy intake** (for example, `energy_kcal`)\n",
    "is not a classical confounder in the usual sense. Instead, it is a kind of\n",
    "“scaling” variable:\n",
    "\n",
    "- Individuals who eat **more total energy** tend to consume more of many nutrients\n",
    "  and foods simply because they eat more food.\n",
    "- Many nutrients are also biologically related to energy intake (for example,\n",
    "  higher energy intake is often associated with higher body size and physical\n",
    "  activity).\n",
    "\n",
    "If we ignore total energy intake, we may incorrectly attribute the effect of\n",
    "“eating more food overall” to a specific nutrient or food.\n",
    "\n",
    "### 4.2 Common energy-adjustment methods\n",
    "\n",
    "Several approaches are used to adjust nutrient intakes for total energy:\n",
    "\n",
    "1. **Nutrient density method**\n",
    "   - Express the nutrient per unit of energy, for example g/MJ or % of energy.\n",
    "   - Example: grams of fibre per 10 MJ.\n",
    "\n",
    "2. **Residual method**\n",
    "   - Regress the nutrient of interest on total energy intake.\n",
    "   - Use the **residuals** (observed minus expected nutrient intake given energy)\n",
    "     as an energy-adjusted exposure.\n",
    "   - This removes the part of the nutrient intake that is explained by total\n",
    "     energy intake.\n",
    "\n",
    "3. **Energy-adjusted models**\n",
    "   - Include both the nutrient and total energy intake as covariates in the\n",
    "     regression model of interest.\n",
    "\n",
    "Each method has advantages and disadvantages. The residual method and energy-adjusted\n",
    "models are particularly useful when working with food-frequency questionnaires (FFQs),\n",
    "where measurement error and strong correlations between nutrients can be substantial.\n",
    "\n",
    "### 4.3 Special case of FFQs\n",
    "\n",
    "FFQs typically record **relative** frequencies of consumption over long periods.\n",
    "Reported intakes of many foods and nutrients are highly correlated, and systematic\n",
    "measurement error is common. Adjusting for total energy intake can:\n",
    "\n",
    "- Reduce measurement error that is common to many foods (for example, general\n",
    "  over-reporting or under-reporting).\n",
    "- Focus analyses on **diet composition** rather than total amount of food.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colliders_mediators_intro_fb2nep_7",
   "metadata": {},
   "source": [
    "## 5. Colliders and mediators\n",
    "\n",
    "### 5.1 Colliders\n",
    "\n",
    "A **collider** is a variable that is *caused* by two (or more) other variables.\n",
    "In a simple diagram:\n",
    "\n",
    "```text\n",
    "exercise  →\n",
    "            fitness\n",
    "genes     →\n",
    "```\n",
    "\n",
    "Here, `fitness` is a collider on the path between `exercise` and `genes`. If we\n",
    "**condition** on fitness (for example, by restricting the analysis to individuals\n",
    "with high fitness, or adjusting for fitness in a model), we can induce an\n",
    "association between exercise and genes even if none exists in the population.\n",
    "\n",
    "This is known as **collider bias** or **selection bias** when the collider is\n",
    "related to being included in the study.\n",
    "\n",
    "### 5.2 Mediators\n",
    "\n",
    "A **mediator** lies *on* the causal pathway between exposure and outcome:\n",
    "\n",
    "```text\n",
    "salt intake → blood pressure → stroke\n",
    "```\n",
    "\n",
    "If we are interested in the **total effect** of salt intake on stroke risk,\n",
    "we should **not** adjust for blood pressure, because this would remove part of\n",
    "the genuine effect (the indirect pathway through blood pressure).\n",
    "\n",
    "If we are specifically interested in the **direct effect** of salt that is not\n",
    "mediated by blood pressure, then adjusting for blood pressure is appropriate,\n",
    "but the interpretation changes.\n",
    "\n",
    "The key message is that we should adjust for **confounders**, avoid adjusting\n",
    "for **colliders**, and think carefully before adjusting for **mediators**.\n",
    "DAGs help us to reason about which variables fall into which category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediator_salt_intro_fb2nep_7",
   "metadata": {},
   "source": [
    "### 5.3 Example of mediation: salt, SBP, and incident CVD\n",
    "\n",
    "We return to the example of salt intake and CVD, now focusing on **mediation**.\n",
    "\n",
    "- **Exposure**: `salt_g_d` (daily salt intake).\n",
    "- **Mediator**: `SBP` (systolic blood pressure).\n",
    "- **Outcome**: `CVD_incident`.\n",
    "\n",
    "A simple DAG is:\n",
    "\n",
    "```text\n",
    "salt_g_d  →  SBP  →  CVD_incident\n",
    "```\n",
    "\n",
    "Salt has a modest direct effect on CVD in the data generator, but the main pathway\n",
    "is through **raising SBP**. We compare two models:\n",
    "\n",
    "1. `CVD_incident ~ salt_g_d`  (total effect: salt → CVD, including via SBP)\n",
    "2. `CVD_incident ~ salt_g_d + SBP`  (direct effect: salt → CVD, *holding SBP constant*)\n",
    "\n",
    "By comparing the odds ratios for `salt_g_d` in these two models, we can see how\n",
    "adjusting for a mediator changes the estimand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediator_salt_models_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mediation example: salt_g_d → SBP → CVD_incident.\"\"\"\n",
    "\n",
    "OUTCOME_SALT = \"CVD_incident\"\n",
    "EXPOSURE_SALT = \"salt_g_d\"\n",
    "MEDIATOR_SBP = \"SBP\"\n",
    "\n",
    "for v in [OUTCOME_SALT, EXPOSURE_SALT, MEDIATOR_SBP]:\n",
    "    if v not in df.columns:\n",
    "        raise KeyError(f\"Variable '{v}' not found in df.\")\n",
    "\n",
    "df_salt = df[[OUTCOME_SALT, EXPOSURE_SALT, MEDIATOR_SBP]].dropna().copy()\n",
    "print(f\"Complete-case sample size: {df_salt.shape[0]} observations\\n\")\n",
    "\n",
    "# Crude (total-effect-oriented) model\n",
    "m_salt_crude = smf.logit(f\"{OUTCOME_SALT} ~ {EXPOSURE_SALT}\", data=df_salt).fit(disp=False)\n",
    "\n",
    "# SBP-adjusted (direct-effect-oriented) model\n",
    "m_salt_adj = smf.logit(\n",
    "    f\"{OUTCOME_SALT} ~ {EXPOSURE_SALT} + {MEDIATOR_SBP}\",\n",
    "    data=df_salt\n",
    ").fit(disp=False)\n",
    "\n",
    "rows_salt = []\n",
    "rows_salt.append(\n",
    "    summarise_logit_coef(\n",
    "        m_salt_crude,\n",
    "        var_name=EXPOSURE_SALT,\n",
    "        label=\"Crude model (CVD_incident ~ salt_g_d)\"\n",
    "    )\n",
    ")\n",
    "rows_salt.append(\n",
    "    summarise_logit_coef(\n",
    "        m_salt_adj,\n",
    "        var_name=EXPOSURE_SALT,\n",
    "        label=\"SBP-adjusted model (CVD_incident ~ salt_g_d + SBP)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "summary_salt_med = pd.DataFrame(rows_salt)\n",
    "\n",
    "print(\"Salt intake and incident CVD: crude vs SBP-adjusted models\\n\")\n",
    "display(summary_salt_med.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediator_salt_interpret_fb2nep_7",
   "metadata": {},
   "source": [
    "### Interpreting the mediation example (salt and SBP)\n",
    "\n",
    "In this synthetic dataset you will typically find that:\n",
    "\n",
    "- The **crude OR** for `salt_g_d` is close to 1 (weak or no association).\n",
    "- The **SBP-adjusted OR** is also close to 1, sometimes slightly below or above.\n",
    "\n",
    "This reflects how the data generator was constructed:\n",
    "\n",
    "- Salt has a **small direct effect** on CVD.\n",
    "- Most of the effect of salt operates through **SBP**.\n",
    "- When we adjust for SBP, we remove the mediation pathway and focus on the direct effect.\n",
    "\n",
    "Even though the numerical effect is small, the example illustrates the key conceptual point:\n",
    "\n",
    "- If our target is the **total effect** of salt on CVD, we should *not* adjust for SBP.\n",
    "- If our target is the **direct effect** of salt, holding SBP constant, then we should\n",
    "  adjust for SBP, but we must interpret the result as a direct effect.\n",
    "\n",
    "This is different from the confounding example with `SES_class`, where adjustment\n",
    "reduces bias in estimating the causal effect of physical activity on CVD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediator_salt_pred_intro_fb2nep_7",
   "metadata": {},
   "source": [
    "### 5.4 Predicted probability of incident CVD across salt intake\n",
    "\n",
    "To make the mediation example more concrete, we can translate the SBP-adjusted\n",
    "    logistic model into **predicted probabilities**.\n",
    "\n",
    "We:\n",
    "\n",
    "- Use the SBP-adjusted logistic model.\n",
    "- Fix SBP at a reference value (for example, the median SBP).\n",
    "- Vary daily salt intake (`salt_g_d`) across its observed range.\n",
    "- Plot the predicted probability of incident CVD.\n",
    "\n",
    "This visualises how the model predicts CVD risk to change with salt intake,\n",
    "**conditional on SBP being held constant**. It emphasises that we are now\n",
    "looking at the *direct effect of salt*, not the total effect including its\n",
    "    influence through SBP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediator_salt_pred_curve_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicted probability of incident CVD across salt intake (SBP fixed).\"\"\"\n",
    "\n",
    "# Choose a reference value for SBP (for example, the median)\n",
    "sbp_ref = df_salt[MEDIATOR_SBP].median()\n",
    "\n",
    "# Construct a grid of salt values over the central range\n",
    "salt_grid = np.linspace(\n",
    "    df_salt[EXPOSURE_SALT].quantile(0.05),\n",
    "    df_salt[EXPOSURE_SALT].quantile(0.95),\n",
    "    100\n",
    ")\n",
    "\n",
    "pred_df_salt = pd.DataFrame({\n",
    "    EXPOSURE_SALT: salt_grid,\n",
    "    MEDIATOR_SBP: sbp_ref,\n",
    "})\n",
    "\n",
    "pred_df_salt[\"p_cvd\"] = m_salt_adj.predict(pred_df_salt)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.plot(pred_df_salt[EXPOSURE_SALT], pred_df_salt[\"p_cvd\"], linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Salt intake (g/day)\")\n",
    "ax.set_ylabel(\"Predicted probability of incident CVD\")\n",
    "ax.set_title(\n",
    "    \"Adjusted logistic model: CVD_incident ~ salt_g_d + SBP\\n\"\n",
    "    f\"(SBP fixed at {sbp_ref:.1f} mmHg)\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediator_salt_pred_interpret_fb2nep_7",
   "metadata": {},
   "source": [
    "### Interpreting the predicted probability curve (salt, SBP fixed)\n",
    "\n",
    "In this synthetic dataset the predicted probability curve is typically **almost flat**,\n",
    "with at most a slight upward or downward slope as salt intake increases.\n",
    "\n",
    "This is expected because:\n",
    "\n",
    "- SBP carries most of the effect of salt on CVD in the data generator.\n",
    "- When SBP is fixed at a reference value, we remove the main pathway by which\n",
    "  salt influences CVD.\n",
    "- What remains is a small direct effect plus any residual correlation with\n",
    "  other variables.\n",
    "\n",
    "The key message is not the exact shape of the curve, but the **change of estimand**:\n",
    "\n",
    "- By conditioning on SBP, we are no longer looking at the total effect of salt\n",
    "  on CVD, but at the risk pattern **given equal SBP**.\n",
    "\n",
    "> Different models answer different causal questions. Prediction curves are useful\n",
    "> for visualising these differences, but they must always be interpreted in the\n",
    "> light of the assumed causal structure (here, salt → SBP → CVD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af343eca",
   "metadata": {},
   "source": [
    "## 6. Interaction and effect modification\n",
    "\n",
    "So far we have mainly treated covariates as **confounders**: variables that we\n",
    "adjust for to obtain a less biased estimate of the exposure–outcome association.\n",
    "\n",
    "Sometimes we are interested in whether the effect of an exposure is **different\n",
    "in different groups**. This is called **effect modification** (or interaction).\n",
    "\n",
    "Examples in nutritional epidemiology include:\n",
    "\n",
    "- Does the association between **physical activity** and CVD differ by **SES**?\n",
    "- Is the association between **salt intake** and CVD stronger in people with\n",
    "  **pre-existing hypertension**?\n",
    "- Does the association between **alcohol** and outcomes differ between **men and women**?\n",
    "\n",
    "In a regression model, we can represent effect modification using an\n",
    "**interaction term**. In a logistic regression model:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{P(Y=1)}{P(Y=0)}\\right)\n",
    "  = \\beta_0 + \\beta_1 X_{\\text{PA}} + \\beta_2 X_{\\text{SES}} + \\beta_3 (X_{\\text{PA}} \\times X_{\\text{SES}}),\n",
    "$$\n",
    "\n",
    "where, for example:\n",
    "\n",
    "- $ X_{\\text{PA}} $ is an indicator of **high physical activity** (`highPA`).\n",
    "- $ X_{\\text{SES}} $ is an indicator of **C2DE** vs **ABC1**.\n",
    "\n",
    "Then:\n",
    "\n",
    "- $ \\beta_1 $ describes the log-odds ratio for high vs non-high physical activity in\n",
    "  the **reference SES group** (for example, ABC1).\n",
    "- $ \\beta_1 + \\beta_3 $ describes the log-odds ratio for high vs non-high physical\n",
    "  activity in the **other SES group** (for example, C2DE).\n",
    "- $ \\beta_3 $ itself is the **difference** in log-odds ratios between SES groups.\n",
    "\n",
    "If $ \\beta_3 = 0 $, then the effect of physical activity on CVD is the same in both\n",
    "SES groups (on the **odds ratio** scale). If \\(\\beta_3 \\neq 0\\), there is\n",
    "multiplicative interaction between physical activity and SES.\n",
    "\n",
    "In practice we often:\n",
    "\n",
    "1. Fit a model **with** an interaction term.\n",
    "2. Derive and report the **group-specific odds ratios**.\n",
    "3. Test the interaction term (for example, using a Wald test).\n",
    "\n",
    "Below we extend the earlier physical activity–SES example by adding an\n",
    "interaction term and computing SES-specific odds ratios for high physical\n",
    "activity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Interaction between physical activity and SES (highPA × SES_class)\n",
    "#\n",
    "# We reuse the df_pa dataset defined earlier:\n",
    "# - OUTCOME_VAR = \"CVD_incident\"\n",
    "# - EXPOSURE_RAW = \"physical_activity\" (used to define highPA)\n",
    "# - CONF_VAR     = \"SES_class\" (ABC1 / C2DE)\n",
    "#\n",
    "# The model is:\n",
    "#     CVD_incident ~ highPA * C(SES_class)\n",
    "#\n",
    "# This allows the association between high physical activity and CVD to differ\n",
    "# between SES groups.\n",
    "\n",
    "\n",
    "# Safety check: ensure df_pa and required variables exist\n",
    "required_cols = {OUTCOME_VAR, \"highPA\", CONF_VAR}\n",
    "missing_cols = required_cols.difference(df_pa.columns)\n",
    "if missing_cols:\n",
    "    raise KeyError(\n",
    "        f\"The following required columns are missing from df_pa: {missing_cols}. \"\n",
    "        \"Please run the earlier confounding example cell first.\"\n",
    "    )\n",
    "\n",
    "# Fit logistic regression model with interaction term\n",
    "formula_int = f\"{OUTCOME_VAR} ~ highPA * C({CONF_VAR})\"\n",
    "m_int = smf.logit(formula_int, data=df_pa).fit(disp=False)\n",
    "\n",
    "print(\"Logistic model with interaction: \"\n",
    "      f\"{OUTCOME_VAR} ~ highPA * C({CONF_VAR})\\n\")\n",
    "\n",
    "# Extract coefficients and covariance matrix\n",
    "params = m_int.params\n",
    "cov = m_int.cov_params()\n",
    "\n",
    "# SES levels (we assume a binary SES variable here)\n",
    "ses_levels = sorted(df_pa[CONF_VAR].dropna().unique())\n",
    "if len(ses_levels) != 2:\n",
    "    raise ValueError(\n",
    "        f\"This example assumes exactly two {CONF_VAR} levels; found: {ses_levels}.\"\n",
    "    )\n",
    "\n",
    "ref_ses = ses_levels[0]   # reference level used by C(SES_class)\n",
    "other_ses = ses_levels[1]\n",
    "\n",
    "interaction_term = f\"highPA:C({CONF_VAR})[T.{other_ses}]\"\n",
    "if interaction_term not in params.index:\n",
    "    raise KeyError(\n",
    "        f\"Could not find interaction term '{interaction_term}' in model parameters.\\n\"\n",
    "        f\"Available parameters: {list(params.index)}\"\n",
    "    )\n",
    "\n",
    "rows = []\n",
    "\n",
    "# 1) Reference SES group (e.g. ABC1): uses the main highPA coefficient\n",
    "(\n",
    "    beta_ref,\n",
    "    ci_l_ref,\n",
    "    ci_u_ref,\n",
    "    p_ref,\n",
    "    OR_ref,\n",
    "    OR_l_ref,\n",
    "    OR_u_ref,\n",
    ") = or_from_linear_combination(\n",
    "    coeff_names=[\"highPA\"],\n",
    "    weights=[1.0],\n",
    "    params=params,\n",
    "    cov=cov,\n",
    ")\n",
    "\n",
    "rows.append(\n",
    "    {\n",
    "        \"SES_class\": ref_ses,\n",
    "        \"beta\": beta_ref,\n",
    "        \"ci_lower\": ci_l_ref,\n",
    "        \"ci_upper\": ci_u_ref,\n",
    "        \"p_value\": p_ref,\n",
    "        \"OR\": OR_ref,\n",
    "        \"OR_ci_lower\": OR_l_ref,\n",
    "        \"OR_ci_upper\": OR_u_ref,\n",
    "        \"note\": \"Reference SES group (no interaction term added)\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2) Other SES group (e.g. C2DE): main effect + interaction term\n",
    "(\n",
    "    beta_other,\n",
    "    ci_l_other,\n",
    "    ci_u_other,\n",
    "    p_other,\n",
    "    OR_other,\n",
    "    OR_l_other,\n",
    "    OR_u_other,\n",
    ") = or_from_linear_combination(\n",
    "    coeff_names=[\"highPA\", interaction_term],\n",
    "    weights=[1.0, 1.0],\n",
    "    params=params,\n",
    "    cov=cov,\n",
    ")\n",
    "\n",
    "rows.append(\n",
    "    {\n",
    "        \"SES_class\": other_ses,\n",
    "        \"beta\": beta_other,\n",
    "        \"ci_lower\": ci_l_other,\n",
    "        \"ci_upper\": ci_u_other,\n",
    "        \"p_value\": p_other,\n",
    "        \"OR\": OR_other,\n",
    "        \"OR_ci_lower\": OR_l_other,\n",
    "        \"OR_ci_upper\": OR_u_other,\n",
    "        \"note\": \"Main effect + interaction term\",\n",
    "    }\n",
    ")\n",
    "\n",
    "summary_int = pd.DataFrame(rows)\n",
    "\n",
    "# 3) The interaction term itself (difference in log-ORs between SES groups)\n",
    "beta_int = params[interaction_term]\n",
    "se_int = np.sqrt(cov.loc[interaction_term, interaction_term])\n",
    "z_crit = 1.96\n",
    "ci_l_int = beta_int - z_crit * se_int\n",
    "ci_u_int = beta_int + z_crit * se_int\n",
    "# p-value is less central here for teaching, so we skip computing it again\n",
    "\n",
    "interaction_summary = pd.Series(\n",
    "    {\n",
    "        \"beta\": beta_int,\n",
    "        \"ci_lower\": ci_l_int,\n",
    "        \"ci_upper\": ci_u_int,\n",
    "    },\n",
    "    name=\"highPA × SES_class\",\n",
    ")\n",
    "\n",
    "print(\"SES-specific odds ratios for high physical activity \"\n",
    "      \"(from interaction model):\\n\")\n",
    "display(summary_int.round(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40596ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nInteraction term (difference in log-ORs between SES groups):\\n\")\n",
    "display(interaction_summary.to_frame().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71524512",
   "metadata": {},
   "source": [
    "# Interpreting an Interaction Term in Logistic Regression  \n",
    "### (High Physical Activity × Socio-Economic Status)\n",
    "\n",
    "This note explains how to interpret an interaction term in a logistic regression\n",
    "model of the form:\n",
    "\n",
    "$$\n",
    "\\text{CVD\\_incident} \\sim \\text{highPA} \\times \\text{SES\\_class}.\n",
    "$$\n",
    "\n",
    "The goal is to give a **generic, reusable explanation** suitable for the\n",
    "workbook, without tying it to any specific numerical results.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What an interaction term represents\n",
    "\n",
    "An interaction term allows the effect of an exposure to differ across levels of\n",
    "another variable.\n",
    "\n",
    "In this model:\n",
    "\n",
    "- **highPA** captures the association between high physical activity and CVD  \n",
    "  **in the reference SES group** (e.g. ABC1).\n",
    "- **SES_class** captures the difference in baseline CVD risk between SES groups.\n",
    "- **highPA × SES_class** captures whether the association between physical\n",
    "  activity and CVD **differs between SES groups**.\n",
    "\n",
    "In other words, the interaction term answers:\n",
    "\n",
    "> *Is the association between physical activity and CVD the same in ABC1 and C2DE,  \n",
    "or does it differ?*\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How to read the interaction coefficient\n",
    "\n",
    "On the log-odds (logit) scale:\n",
    "\n",
    "- A **zero** interaction coefficient means the effect of high physical activity  \n",
    "  is the *same* in both SES groups.\n",
    "- A **positive** coefficient means the association is *stronger* (more protective\n",
    "  or more harmful, depending on direction) in the non-reference SES group.\n",
    "- A **negative** coefficient means the association is *weaker* in the\n",
    "  non-reference SES group.\n",
    "\n",
    "When exponentiated, the interaction coefficient becomes the **ratio of odds\n",
    "ratios**:\n",
    "\n",
    "$$\n",
    "\\exp(\\beta_{\\text{interaction}})  \n",
    "  = \\frac{\\text{OR(highPA in non-ref SES)}}{\\text{OR(highPA in ref SES)}}.\n",
    "$$\n",
    "\n",
    "Values:\n",
    "\n",
    "- **= 1** → no difference  \n",
    "- **> 1** → stronger association in the non-reference group  \n",
    "- **< 1** → weaker association in the non-reference group\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How the interaction affects group-specific effects\n",
    "\n",
    "The model estimates:\n",
    "\n",
    "- Effect of highPA in the **reference** SES group:  \n",
    "  $$\n",
    "  \\beta_{\\text{highPA}}.\n",
    "  $$\n",
    "\n",
    "- Effect of highPA in the **other** SES group:  \n",
    "  $$\n",
    "  \\beta_{\\text{highPA}} + \\beta_{\\text{interaction}}.\n",
    "  $$\n",
    "\n",
    "Thus the interaction term determines how much the highPA effect **changes**\n",
    "between SES strata.\n",
    "\n",
    "This is the key reason we need the interaction term: without it, the model\n",
    "assumes the effect is identical in all SES groups.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Confidence intervals and interpretation\n",
    "\n",
    "When interpreting an interaction:\n",
    "\n",
    "- A confidence interval that **includes zero** (on the log scale) or **includes 1**\n",
    "  (on the OR scale) implies that the data are **compatible with no interaction**.\n",
    "- Interaction terms typically require **large samples** to detect modest\n",
    "  differences.\n",
    "- Absence of statistical evidence for interaction does **not** prove the effects\n",
    "  are identical; it simply indicates we cannot conclude they differ.\n",
    "\n",
    "For teaching purposes:  \n",
    "*Interactions are often noisier and harder to estimate than main effects. This is\n",
    "normal and not a sign of a mistake.*\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Practical takeaway for applied epidemiology\n",
    "\n",
    "When students encounter an interaction term:\n",
    "\n",
    "1. Identify which group is the **reference** for SES.  \n",
    "2. Interpret the main effect of highPA as applying to that reference group.  \n",
    "3. Add the interaction term to obtain the effect in the other SES group.  \n",
    "4. Use the confidence interval of the interaction to judge whether a **meaningful\n",
    "   difference** between groups is supported by the data.  \n",
    "5. Avoid over-interpreting small, non-significant interactions.  \n",
    "\n",
    "In nutritional epidemiology it is common to *test* for effect modification but\n",
    "rare to *find* strong evidence unless the effect truly differs across groups or\n",
    "the study is very large.\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can also prepare a short “student-friendly” sidebar version for\n",
    "the workbook margins, or add an optional visual explaining interaction on the\n",
    "logit and OR scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "counterfactuals_fb2nep_7",
   "metadata": {},
   "source": [
    "## 7. Counterfactuals\n",
    "\n",
    "Modern causal inference often uses **counterfactual** or **potential outcome**\n",
    "language. For each individual we imagine:\n",
    "\n",
    "- $ Y(1) $: the outcome that would occur if the individual were exposed.\n",
    "- $ Y(0) $: the outcome that would occur if the same individual were not exposed.\n",
    "\n",
    "The **causal effect** for that individual is the (usually unobservable) difference\n",
    "$ Y(1) - Y(0) $. In practice we cannot observe both outcomes for the same\n",
    "person, so we rely on comparisons between groups, together with assumptions about\n",
    "confounding, measurement, and model specification.\n",
    "\n",
    "Adjustment strategies (for example, regression with appropriate covariates based on\n",
    "a sensible DAG) are used to make the exposed and unexposed groups more comparable,\n",
    "so that the difference in observed outcomes approximates the difference between\n",
    "counterfactual outcomes.\n",
    "\n",
    "Workbook 9 returns to these ideas and introduces more formal notation and methods\n",
    "for estimating causal effects under explicit assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection_exercises_fb2nep_7",
   "metadata": {},
   "source": [
    "## Reflection and exercises\n",
    "\n",
    "1. **Draw a DAG** for the association between red meat intake and incident cancer\n",
    "   in the FB2NEP cohort. Include at least age, sex, SES_class, IMD_quintile,\n",
    "   smoking_status, and family history. Identify plausible confounders,\n",
    "   colliders, and mediators.\n",
    "\n",
    "2. **Confounders in practice**: Choose a different exposure (for example,\n",
    "   `fruit_veg_g_d` or `salt_g_d`) and a relevant outcome. Propose at least two\n",
    "   variables as potential confounders based on subject-matter knowledge. Fit\n",
    "   crude and adjusted models and compare the estimates.\n",
    "\n",
    "3. **Energy adjustment**: Using `energy_kcal` and a nutrient of your choice\n",
    "   (for example, `fibre_g_d`), implement the nutrient density method and the\n",
    "   residual method. Compare the associations with BMI or another suitable\n",
    "   outcome for the raw, density-based, and residual-based exposures.\n",
    "\n",
    "4. **Collider bias**: Modify the collider simulation to use a different\n",
    "   collider (for example, an indicator of study participation) and show how\n",
    "   conditioning on participation can induce associations between variables\n",
    "   that are otherwise independent.\n",
    "\n",
    "5. **Mediators**: For a hypothetical causal chain in nutrition (for example,\n",
    "   `diet quality → BMI → blood pressure → CVD`), decide which variables you\n",
    "   would adjust for when estimating the total effect of diet quality on CVD,\n",
    "   and which you would *not* adjust for. Explain your reasoning.\n",
    "\n",
    "6. **Counterfactual thinking**: In your own words, describe what it would mean\n",
    "   to say that “reducing salt intake by 2 g/day would reduce average SBP by\n",
    "   5 mmHg” in terms of potential outcomes. What assumptions would be needed for\n",
    "   this statement to be interpreted causally in an observational study?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
