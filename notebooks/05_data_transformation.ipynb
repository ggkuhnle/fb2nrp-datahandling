{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_fb2nep_7",
   "metadata": {},
   "source": [
    "# FB2NEP Workbook 7 – Data Transformation and Preparation for Modelling\n",
    "\n",
    "Version 0.0.7\n",
    "\n",
    "In this workbook we explore how to prepare variables for statistical modelling.\n",
    "\n",
    "We consider:\n",
    "\n",
    "- Why we sometimes transform variables before modelling.\n",
    "- Log transformation, Box–Cox transformation and z-scoring.\n",
    "- Visual comparison of original and transformed variables.\n",
    "- Categorisation, quantiles and information loss.\n",
    "- Truncation of analytical values (for example, LOD / LLOQ / ULOQ).\n",
    "\n",
    "The focus is on understanding **why** transformations are used in epidemiological analyses,\n",
    "and how they affect model assumptions and interpretation.\n",
    "\n",
    "We begin with some background theory, using small simulated examples to illustrate:\n",
    "\n",
    "- What a normal distribution looks like.\n",
    "- Residuals and their distribution.\n",
    "- Heteroskedasticity (changing variance with the level of a predictor).\n",
    "- How one might decide whether to transform a variable or not.\n",
    "\n",
    "We then work with the FB2NEP synthetic cohort to apply these ideas in practice.\n",
    "\n",
    "Run the first code cell to set up the repository and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap_fb2nep_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FB2NEP bootstrap cell (works both locally and in Colab)\n",
    "#\n",
    "# What this cell does:\n",
    "# - Ensures that we are inside the fb2nep-epi repository.\n",
    "# - In Colab: clones the repository from GitHub if necessary.\n",
    "# - Loads and runs scripts/bootstrap.py.\n",
    "# - Makes the main dataset available as the variable `df`.\n",
    "#\n",
    "# Important:\n",
    "# - You may see messages printed below (for example from pip\n",
    "#   or from the bootstrap script). This is expected.\n",
    "# - You may also see WARNINGS (often in yellow). In most cases\n",
    "#   these are harmless and can be ignored for this module.\n",
    "# - The main thing to watch for is a red error traceback\n",
    "#   (for example FileNotFoundError, ModuleNotFoundError).\n",
    "#   If that happens, please re-run this cell first. If the\n",
    "#   error persists, ask for help.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Configuration: repository location and URL\n",
    "# ------------------------------------------------------------\n",
    "# REPO_URL: address of the GitHub repository.\n",
    "# REPO_DIR: folder name that will be created when cloning.\n",
    "REPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\n",
    "REPO_DIR = \"fb2nep-epi\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Ensure we are inside the fb2nep-epi repository\n",
    "# ------------------------------------------------------------\n",
    "# In local Jupyter, you may already be inside the repository,\n",
    "# for example in fb2nep-epi/notebooks.\n",
    "#\n",
    "# In Colab, the default working directory is /content, so\n",
    "# we need to clone the repository into /content/fb2nep-epi\n",
    "# and then change into that folder.\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "# Case A: we are already in the repository (scripts/bootstrap.py exists here)\n",
    "if (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n",
    "    repo_root = cwd\n",
    "\n",
    "# Case B: we are outside the repository (for example in Colab)\n",
    "else:\n",
    "    repo_root = cwd / REPO_DIR\n",
    "\n",
    "    # Clone the repository if it is not present yet\n",
    "    if not repo_root.is_dir():\n",
    "        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n",
    "    else:\n",
    "        print(f\"Using existing repository at {repo_root}\")\n",
    "\n",
    "    # Change the working directory to the repository root\n",
    "    os.chdir(repo_root)\n",
    "    repo_root = pathlib.Path.cwd()\n",
    "\n",
    "print(f\"Repository root set to: {repo_root}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load scripts/bootstrap.py as a module and call init()\n",
    "# ------------------------------------------------------------\n",
    "# The shared bootstrap script contains all logic to:\n",
    "# - Ensure that required Python packages are installed.\n",
    "# - Ensure that the synthetic dataset exists (and generate it\n",
    "#   if needed).\n",
    "# - Load the dataset into a pandas DataFrame.\n",
    "#\n",
    "# We load the script as a normal Python module (fb2nep_bootstrap)\n",
    "# and then call its init() function.\n",
    "bootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n",
    "\n",
    "if not bootstrap_path.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {bootstrap_path}. \"\n",
    "        \"Please check that the fb2nep-epi repository structure is intact.\"\n",
    "    )\n",
    "\n",
    "# Create a module specification from the file\n",
    "spec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\n",
    "bootstrap = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"fb2nep_bootstrap\"] = bootstrap\n",
    "\n",
    "# Execute the bootstrap script in the context of this module\n",
    "spec.loader.exec_module(bootstrap)\n",
    "\n",
    "# The init() function is defined in scripts/bootstrap.py.\n",
    "# It returns:\n",
    "# - df   : the main synthetic cohort as a pandas DataFrame.\n",
    "# - CTX  : a small context object with paths, flags and settings.\n",
    "df, CTX = bootstrap.init()\n",
    "\n",
    "# Optionally expose a few additional useful variables from the\n",
    "# bootstrap module (if they exist). These are not essential for\n",
    "# most analyses, but can be helpful for advanced use.\n",
    "for name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n",
    "    if hasattr(bootstrap, name):\n",
    "        globals()[name] = getattr(bootstrap, name)\n",
    "\n",
    "print(\"Bootstrap completed successfully.\")\n",
    "print(\"The main dataset is available as the variable `df`.\")\n",
    "print(\"The context object is available as `CTX`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "background_intro",
   "metadata": {},
   "source": [
    "## Background theory with simple examples\n",
    "\n",
    "Before using the FB2NEP dataset, we illustrate a few key ideas with small simulated examples:\n",
    "\n",
    "- What a **normal distribution** looks like.\n",
    "- What **residuals** are and why we care about their distribution.\n",
    "- What **heteroskedasticity** looks like in a residual plot.\n",
    "- How one might decide whether a transformation is needed.\n",
    "\n",
    "These examples are deliberately simple and are not based on real nutritional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_intro",
   "metadata": {},
   "source": [
    "We first import the Python packages required for this workbook:\n",
    "\n",
    "- `numpy` and `pandas` for data handling.\n",
    "- `matplotlib` for visualisation.\n",
    "- `scipy.stats` for skewness and Box–Cox transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducible simulated examples\n",
    "np.random.seed(11088)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal_example_intro",
   "metadata": {},
   "source": [
    "### Background 1 – What is a normal distribution?\n",
    "\n",
    "Many statistical methods assume that some quantity is approximately **normally distributed**.\n",
    "The normal distribution is the familiar “bell-shaped” curve:\n",
    "\n",
    "- Most values lie near the mean.\n",
    "- Values become less frequent as we move further away from the mean.\n",
    "- The distribution is symmetric around the mean.\n",
    "\n",
    "We now simulate 10,000 values from a normal distribution with mean 0 and standard deviation 1\n",
    "and plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal_example_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data from a standard normal distribution\n",
    "# mean = 0, standard deviation = 1\n",
    "\n",
    "x_norm = np.random.normal(loc=0.0, scale=1.0, size=10_000)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(x_norm, bins=30)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Simulated standard normal distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean:\", x_norm.mean())\n",
    "print(\"Standard deviation:\", x_norm.std())\n",
    "print(\"Skewness:\", stats.skew(x_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residuals_intro",
   "metadata": {},
   "source": [
    "### Background 2 – What are residuals?\n",
    "\n",
    "We start by looking at how well a simple linear model reproduces the data:\n",
    "\n",
    "- The **fitted value** is the model’s prediction for each observation.\n",
    "- Plotting **observed vs fitted** shows whether the model captures the *average* pattern.\n",
    "\n",
    "We then look at **residuals** (observed − fitted):\n",
    "\n",
    "- Residuals show what is left after the model has explained as much variation as it can.\n",
    "- Their distribution (for example, symmetry, spread) is what many methods make assumptions about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals_example_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: residuals approximately normal\n",
    "# ----------------------------------------\n",
    "# We simulate x from a normal distribution and y from a linear relation\n",
    "# plus normal (Gaussian) error.\n",
    "\n",
    "n = 500\n",
    "x1 = np.random.normal(loc=0.0, scale=1.0, size=n)\n",
    "epsilon1 = np.random.normal(loc=0.0, scale=1.0, size=n)  # normal residuals\n",
    "y1 = 2.0 + 1.5 * x1 + epsilon1\n",
    "\n",
    "# Fit a straight line y1 ~ x1 using numpy.polyfit\n",
    "slope1, intercept1 = np.polyfit(x1, y1, 1)\n",
    "fitted1 = intercept1 + slope1 * x1\n",
    "residuals1 = y1 - fitted1\n",
    "\n",
    "# Example 2: residuals clearly non-normal\n",
    "# ---------------------------------------\n",
    "# We use the same x, but generate skewed error terms (for example,\n",
    "# from an exponential distribution).\n",
    "\n",
    "epsilon2 = np.random.exponential(scale=1.0, size=n)  # skewed residuals\n",
    "y2 = 2.0 + 1.5 * x1 + epsilon2\n",
    "\n",
    "slope2, intercept2 = np.polyfit(x1, y2, 1)\n",
    "fitted2 = intercept2 + slope2 * x1\n",
    "residuals2 = y2 - fitted2\n",
    "\n",
    "# Plot observed vs fitted values for the two examples\n",
    "# ---------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "def add_identity_line(ax, x, y):\n",
    "    \"\"\"Add a y = x reference line over the range of fitted/observed values.\"\"\"\n",
    "    lo = min(x.min(), y.min())\n",
    "    hi = max(x.max(), y.max())\n",
    "    ax.plot([lo, hi], [lo, hi], color=\"black\", linewidth=1)\n",
    "\n",
    "# Example 1: approximately normal residuals\n",
    "axes[0].scatter(fitted1, y1, alpha=0.5)\n",
    "add_identity_line(axes[0], fitted1, y1)\n",
    "axes[0].set_title(\"Observed vs fitted – normal errors\")\n",
    "axes[0].set_xlabel(\"Fitted value\")\n",
    "axes[0].set_ylabel(\"Observed value\")\n",
    "\n",
    "# Example 2: skewed residuals\n",
    "axes[1].scatter(fitted2, y2, alpha=0.5)\n",
    "add_identity_line(axes[1], fitted2, y2)\n",
    "axes[1].set_title(\"Observed vs fitted – skewed errors\")\n",
    "axes[1].set_xlabel(\"Fitted value\")\n",
    "axes[1].set_ylabel(\"Observed value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8aba73",
   "metadata": {},
   "source": [
    "In both panels, the points lie roughly along the 45° line, so the *average* relation\n",
    "is captured well by the straight line model. The difference between the two examples\n",
    "will become clearer when we examine the residuals.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ec082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of residuals for both cases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].hist(residuals1, bins=30)\n",
    "axes[0].set_title(\"Residuals – approximately normal\")\n",
    "axes[0].set_xlabel(\"Residual\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(residuals2, bins=30)\n",
    "axes[1].set_title(\"Residuals – clearly skewed\")\n",
    "axes[1].set_xlabel(\"Residual\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Example 1 residual skewness:\", stats.skew(residuals1))\n",
    "print(\"Example 2 residual skewness:\", stats.skew(residuals2))\n",
    "\n",
    "# Residuals vs fitted values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].scatter(fitted1, residuals1, alpha=0.5)\n",
    "axes[0].axhline(0.0, color=\"black\", linewidth=1)\n",
    "axes[0].set_title(\"Residuals vs fitted – normal errors\")\n",
    "axes[0].set_xlabel(\"Fitted value\")\n",
    "axes[0].set_ylabel(\"Residual\")\n",
    "\n",
    "axes[1].scatter(fitted2, residuals2, alpha=0.5)\n",
    "axes[1].axhline(0.0, color=\"black\", linewidth=1)\n",
    "axes[1].set_title(\"Residuals vs fitted – skewed errors\")\n",
    "axes[1].set_xlabel(\"Fitted value\")\n",
    "axes[1].set_ylabel(\"Residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f1f5c",
   "metadata": {},
   "source": [
    "**Exercise – residuals and assumptions**\n",
    "\n",
    "- Which example looks closer to the ideal of symmetric, roughly constant residuals?\n",
    "- In which example would you be more confident about using methods that assume\n",
    "  normally distributed residuals with constant variance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hetero_intro",
   "metadata": {},
   "source": [
    "### Background 3 – Heteroskedasticity (changing variance)\n",
    "\n",
    "In many real data sets, the **spread** of residuals is not constant across the range of fitted values.\n",
    "If the residuals fan out as the predicted value increases, this is called **heteroskedasticity**.\n",
    "\n",
    "We now simulate a simple example where the variance of the error term increases with the level of x\n",
    "and show a residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hetero_example_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate heteroskedastic data\n",
    "# -----------------------------\n",
    "# We let the standard deviation of the error term increase with |x|.\n",
    "\n",
    "x_h = np.linspace(-3, 3, n)\n",
    "sigma_h = 0.5 + 0.5 * np.abs(x_h)  # larger variance for large |x|\n",
    "epsilon_h = np.random.normal(loc=0.0, scale=sigma_h)\n",
    "y_h = 1.0 + 2.0 * x_h + epsilon_h\n",
    "\n",
    "slope_h, intercept_h = np.polyfit(x_h, y_h, 1)\n",
    "fitted_h = intercept_h + slope_h * x_h\n",
    "residuals_h = y_h - fitted_h\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(fitted_h, residuals_h, alpha=0.5)\n",
    "plt.axhline(0.0, color=\"black\", linewidth=1)\n",
    "plt.xlabel(\"Fitted value\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Heteroskedasticity: residuals fan out with fitted value\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decide_transform_intro",
   "metadata": {},
   "source": [
    "### Background 4 – How to decide whether to transform\n",
    "\n",
    "In practice there is no automatic rule for when to transform a variable. Analysts usually combine:\n",
    "\n",
    "- **Exploratory plots of the variable itself** (histograms, density plots) to see skewness.\n",
    "- **Residual plots** from a model fitted on the original scale.\n",
    "- **Subject-matter knowledge**, for example whether multiplicative (percentage) effects are plausible.\n",
    "\n",
    "Some practical guidelines:\n",
    "\n",
    "- If residuals are approximately symmetric and the variance is roughly constant across fitted values,\n",
    "  a transformation is often unnecessary.\n",
    "- If the predictor is strongly right-skewed and residuals are clearly non-normal or heteroskedastic,\n",
    "  a log or Box–Cox transformation can help.\n",
    "- If predictors are on very different scales and you mainly care about comparing effect sizes,\n",
    "  z-scoring can be useful.\n",
    "- Transformations should also respect interpretability: a modest gain in “niceness” of residual plots\n",
    "  may not justify a scale that is hard to explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd922da8",
   "metadata": {},
   "source": [
    "---\n",
    "# Let's begin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_data_intro",
   "metadata": {},
   "source": [
    "## 1. Initial inspection of the FB2NEP data\n",
    "\n",
    "We now return to the FB2NEP synthetic cohort. Before transforming variables, we:\n",
    "\n",
    "- Check that the variables of interest exist.\n",
    "- Obtain a first impression of their ranges and types.\n",
    "\n",
    "In this workbook we will mainly use:\n",
    "\n",
    "- `red_meat_g_d` (or `energy_kcal` as fallback) as an example of a skewed exposure.\n",
    "- `BMI` (kg/m²) and `SBP` (systolic blood pressure, mmHg) as common continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Show the first rows (participants) of the dataset\n",
    "display(df.head())\n",
    "\n",
    "# Show data types of a subset of variables that we will use later.\n",
    "cols_of_interest = [\n",
    "    c for c in [\"red_meat_g_d\", \"energy_kcal\", \"BMI\", \"SBP\"] if c in df.columns\n",
    "]\n",
    "print(\"\\nColumns of interest:\", cols_of_interest)\n",
    "display(df[cols_of_interest].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "log_intro",
   "metadata": {},
   "source": [
    "## 2. Log transformation – nutrition data and log-normality\n",
    "\n",
    "Many dietary and biomarker variables are **right-skewed**: most participants have low or moderate\n",
    "values, with a long tail of high values. In nutrition, it is often more plausible to think in terms of\n",
    "**proportional** changes (for example, doubling intake) than in terms of fixed absolute changes.\n",
    "\n",
    "If a variable is approximately **log-normal** (that is, its logarithm is roughly normal), then:\n",
    "\n",
    "- The variable itself is right-skewed.\n",
    "- The log-transformed variable is closer to a normal distribution.\n",
    "- Effects can often be interpreted in terms of percentage or fold-changes.\n",
    "\n",
    "We now inspect the distribution of a skewed dietary variable and apply a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log_skewness_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a variable that is likely to be right-skewed.\n",
    "var = \"red_meat_g_d\" if \"red_meat_g_d\" in df.columns else \"energy_kcal\"\n",
    "print(f\"Using variable: {var}\")\n",
    "\n",
    "# Drop missing values for plotting and computing skewness.\n",
    "x = df[var].dropna()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "x.hist(bins=30)\n",
    "plt.xlabel(var)\n",
    "plt.ylabel(\"Number of participants\")\n",
    "plt.title(f\"Distribution of {var}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of non-missing values:\", x.shape[0])\n",
    "print(\"Mean:\", x.mean())\n",
    "print(\"Median:\", x.median())\n",
    "print(\"Skewness:\", stats.skew(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791bef7",
   "metadata": {},
   "source": [
    "Like most dietary data, meat intake is skewed - other variables are often even more skewed.\n",
    "\n",
    "What happens if we log-transform it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "log_transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helpers_tables import log_transform, plot_hist_pair\n",
    "\n",
    "# Use a small constant to avoid problems if there are zeros.\n",
    "log_const = 0.1\n",
    "log_var_name = \"log_\" + var\n",
    "df[log_var_name] = log_transform(df[var], constant=log_const)\n",
    "\n",
    "print(f\"Created column: {log_var_name} (log-transformed {var})\")\n",
    "\n",
    "# Compare skewness before and after log transformation.\n",
    "x_log = df[log_var_name].dropna()\n",
    "print(\"Skewness (original):\", stats.skew(x))\n",
    "print(\"Skewness (log-transformed):\", stats.skew(x_log))\n",
    "\n",
    "plot_hist_pair(\n",
    "    original=df[var],\n",
    "    transformed=df[log_var_name],\n",
    "    original_label=var,\n",
    "    transformed_label=f\"log({var} + {log_const})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "log_exercise",
   "metadata": {},
   "source": [
    "### Exercise – log transformation\n",
    "\n",
    "- Compare the histograms and skewness values before and after log transformation.\n",
    "- Does the log-transformed variable look more symmetric?\n",
    "- If you were to regress SBP on this exposure, would you consider using the log\n",
    "  scale rather than the original scale? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zscore_intro",
   "metadata": {},
   "source": [
    "## 3. Standardisation (z-scores) – making variables comparable\n",
    "\n",
    "Variables such as BMI and SBP are measured on different scales and in different\n",
    "units. When several such variables are included in a model, it can be useful to\n",
    "standardise them so that:\n",
    "\n",
    "- The mean is 0.\n",
    "- The standard deviation is 1.\n",
    "\n",
    "These standardised values are called **z-scores**. They are defined as:\n",
    "\n",
    "$$ z_i = \\frac{x_i - \\bar{x}}{s_x} $$\n",
    "\n",
    "where $\\bar{x}$ is the sample mean and $s_x$ is the sample standard deviation.\n",
    "\n",
    "In psychology and some areas of epidemiology it is common to report effects per\n",
    "1 standard deviation increase, because this makes effect sizes comparable across\n",
    "different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zscore_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helpers_tables import z_score\n",
    "\n",
    "for v in [\"BMI\", \"SBP\"]:\n",
    "    if v in df.columns:\n",
    "        z_name = \"z_\" + v\n",
    "        df[z_name] = z_score(df[v])\n",
    "        print(f\"\\nSummary of z-scored {v} (stored in {z_name}):\")\n",
    "        print(df[z_name].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zscore_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation: compare the distributions of BMI and z-scored BMI.\n",
    "if \"BMI\" in df.columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[\"BMI\"].dropna().hist(bins=30)\n",
    "    plt.xlabel(\"BMI (kg/m²)\")\n",
    "    plt.ylabel(\"Number of participants\")\n",
    "    plt.title(\"BMI (original scale)\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df[\"z_BMI\"].dropna().hist(bins=30)\n",
    "    plt.xlabel(\"z-BMI\")\n",
    "    plt.ylabel(\"Number of participants\")\n",
    "    plt.title(\"BMI (z-score)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zscore_exercise",
   "metadata": {},
   "source": [
    "### Exercise – interpretation of z-scores\n",
    "\n",
    "- Check the summary output for `z_BMI` and `z_SBP`.\n",
    "  - Are the means close to 0 and the standard deviations close to 1?\n",
    "- Imagine a regression model where the coefficient for `z_BMI` is 2.5 mmHg.\n",
    "  - How would you describe this effect in words (per 1 standard deviation increase\n",
    "    in BMI)?\n",
    "  - How does this compare to a model where BMI is used in kg/m² without\n",
    "    standardisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxcox_intro",
   "metadata": {},
   "source": [
    "## 4. Box–Cox transformations – a flexible family\n",
    "\n",
    "The log transformation is a special case of a more general family of power\n",
    "transformations called **Box–Cox transformations**. Box–Cox chooses an exponent\n",
    "(lambda, $\\lambda$) that aims to make the distribution more symmetric.\n",
    "\n",
    "For a variable $x$ that is strictly positive, the Box–Cox transformation is defined as\n",
    "\n",
    "$$\n",
    "x^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{x^{\\lambda} - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, \\\\[6pt]\n",
    "\\log(x), & \\text{if } \\lambda = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Key points:\n",
    "\n",
    "- For $\\lambda = 1$, the Box–Cox transform is essentially the original scale.\n",
    "- For $\\lambda = 0$, the transform corresponds to a log transform.\n",
    "- The method requires $x > 0$. In practice, one often adds a small constant\n",
    "  (for example, $x^{*} = x + c$) if some observations are zero or very small.\n",
    "\n",
    "We now apply a Box–Cox transformation to the same skewed dietary variable and\n",
    "compare its distribution with the original and log-transformed versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxcox_transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box–Cox transformation requires strictly positive values.\n",
    "# We therefore shift the data if necessary.\n",
    "\n",
    "x_bc = df[var].dropna()\n",
    "min_x = x_bc.min()\n",
    "\n",
    "shift = 0.0\n",
    "if min_x <= 0:\n",
    "    # Add a small positive amount so that the minimum becomes slightly > 0.\n",
    "    shift = -min_x + 1e-6\n",
    "\n",
    "x_bc_shifted = x_bc + shift\n",
    "\n",
    "# Apply Box–Cox. stats.boxcox returns transformed data and the estimated lambda.\n",
    "x_bc_transformed, lambda_bc = stats.boxcox(x_bc_shifted)\n",
    "\n",
    "# Store Box–Cox transformed values in the DataFrame, aligned with the original index.\n",
    "boxcox_var_name = \"boxcox_\" + var\n",
    "df[boxcox_var_name] = np.nan\n",
    "df.loc[x_bc.index, boxcox_var_name] = x_bc_transformed\n",
    "\n",
    "print(f\"Created column: {boxcox_var_name} (Box–Cox transformed {var})\")\n",
    "print(f\"Box–Cox lambda (λ): {lambda_bc:.3f}\")\n",
    "print(f\"Shift applied before Box–Cox: {shift}\")\n",
    "\n",
    "# Compare skewness for original, log and Box–Cox transformed data.\n",
    "x_boxcox = df[boxcox_var_name].dropna()\n",
    "print(\"Skewness (original):\", stats.skew(x))\n",
    "print(\"Skewness (log):\", stats.skew(x_log))\n",
    "print(\"Skewness (Box–Cox):\", stats.skew(x_boxcox))\n",
    "\n",
    "plot_hist_pair(\n",
    "    original=df[var],\n",
    "    transformed=df[boxcox_var_name],\n",
    "    original_label=var,\n",
    "    transformed_label=f\"Box–Cox({var})\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxcox_exercise",
   "metadata": {},
   "source": [
    "### Exercise – log versus Box–Cox\n",
    "\n",
    "- Compare the histograms and skewness values for the original, log-transformed and\n",
    "  Box–Cox transformed variables.\n",
    "- Which transformation appears to give the most symmetric distribution?\n",
    "- Which transformation would you prefer to use in a model, and why? Consider both\n",
    "  distributional properties and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorisation_intro",
   "metadata": {},
   "source": [
    "## 5. Categorisation and quantiles – useful but risky\n",
    "\n",
    "Epidemiological papers often categorise continuous variables, for example:\n",
    "\n",
    "- Clinical categories (for example, normal weight, overweight, obesity based on BMI).\n",
    "- Quantiles such as tertiles, quartiles or quintiles of intake.\n",
    "\n",
    "While categorisation can simplify presentation, it comes with several problems:\n",
    "\n",
    "- **Information loss:** individuals with very different values may end up in the\n",
    "  same category.\n",
    "- **Arbitrary cut-points:** results can change if cut-points are moved slightly.\n",
    "- **Reduced power:** categorisation usually reduces statistical power.\n",
    "\n",
    "Quantiles are particularly important in nutrition:\n",
    "\n",
    "- They are often used to describe the distribution (for example, median and interquartile range).\n",
    "- They are also used to form exposure groups (for example, quintiles of intake).\n",
    "\n",
    "However, the **extreme quantiles** (lowest and highest) can cover quite wide ranges of the underlying\n",
    "variable, especially when the distribution is skewed. We explore this using BMI and BMI quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorisation_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if {\"BMI\", \"SBP\"}.issubset(df.columns):\n",
    "    # Create BMI quintiles using pandas.qcut.\n",
    "    # qcut creates categories with (approximately) equal numbers of observations.\n",
    "    df[\"BMI_quintile\"] = pd.qcut(df[\"BMI\"], 5, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\"])\n",
    "\n",
    "    print(\"BMI quintiles (cut-points and ranges):\")\n",
    "    print(\n",
    "        df.groupby(\"BMI_quintile\")[\"BMI\"].describe()[[\"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n",
    "    )\n",
    "\n",
    "    # Scatter plot of SBP vs continuous BMI.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df[\"BMI\"], df[\"SBP\"], alpha=0.3)\n",
    "    plt.xlabel(\"BMI (kg/m²)\")\n",
    "    plt.ylabel(\"SBP (mmHg)\")\n",
    "    plt.title(\"SBP vs continuous BMI\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot of SBP by BMI quintile.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df.boxplot(column=\"SBP\", by=\"BMI_quintile\")\n",
    "    plt.xlabel(\"BMI quintile\")\n",
    "    plt.ylabel(\"SBP (mmHg)\")\n",
    "    plt.title(\"SBP by BMI quintile\")\n",
    "    plt.suptitle(\"\")  # Remove the default super-title.\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Compare mean SBP between quintiles.\n",
    "    print(\"\\nMean SBP by BMI quintile:\")\n",
    "    print(df.groupby(\"BMI_quintile\")[\"SBP\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "categorisation_exercise",
   "metadata": {},
   "source": [
    "### Exercise – categories and extreme quantiles\n",
    "\n",
    "1. Compare the scatter plot of SBP vs continuous BMI with the boxplot of SBP by\n",
    "   BMI quintiles.\n",
    "   - What information is visible in the scatter plot but lost when using quintiles?\n",
    "   - How much variation in SBP exists **within** each quintile?\n",
    "\n",
    "2. Look at the ranges of BMI within each quintile.\n",
    "   - Are the lowest and highest values in Q1 and Q5 quite far apart?\n",
    "   - How might this affect interpretation of “lowest” vs “highest” quintile?\n",
    "\n",
    "3. In which situations might categorisation still be useful or necessary\n",
    "   (for example, clinical decision thresholds)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "truncation_intro_main",
   "metadata": {},
   "source": [
    "## 6. Special case – truncation of analytical values\n",
    "\n",
    "Laboratory measurements often have a **limit of detection (LOD)**, a **lower limit of\n",
    "quantification (LLOQ)** and sometimes an **upper limit of quantification (ULOQ)**. Values\n",
    "below or above these limits are **truncated** by the measurement process.\n",
    "\n",
    "Common strategies for handling values below LOD or LLOQ include:\n",
    "\n",
    "- Treating them as missing and analysing only values above the limit.\n",
    "- Replacing them with a constant (for example, LOD/2 or LOD/√2).\n",
    "- Using the measured values as reported (if the instrument provides them), but recognising\n",
    "  that they are less reliable.\n",
    "\n",
    "We now simulate a small biomarker with a LOD and show how different approaches affect\n",
    "the distribution, particularly when applying a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "truncation_example_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a right-skewed biomarker (for example, a log-normal-like distribution)\n",
    "\n",
    "bio_raw = np.random.lognormal(mean=1.0, sigma=0.7, size=2000)\n",
    "\n",
    "# Assume a limit of detection (LOD)\n",
    "LOD = 1.0\n",
    "print(\"Assumed LOD:\", LOD)\n",
    "\n",
    "# Create three versions:\n",
    "# 1) Values below LOD treated as missing (NaN).\n",
    "# 2) Values below LOD replaced by LOD/2.\n",
    "# 3) Original measured values (no truncation), for comparison.\n",
    "\n",
    "bio_measured = bio_raw.copy()\n",
    "bio_na = bio_raw.copy()\n",
    "bio_const = bio_raw.copy()\n",
    "\n",
    "bio_na[bio_na < LOD] = np.nan\n",
    "bio_const[bio_const < LOD] = LOD / 2.0\n",
    "\n",
    "print(\"Proportion of values below LOD:\", np.mean(bio_raw < LOD))\n",
    "\n",
    "# Plot histograms for the three approaches on the original scale\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "axes[0].hist(bio_measured, bins=30)\n",
    "axes[0].axvline(LOD, color=\"red\", linestyle=\"--\")\n",
    "axes[0].set_title(\"Measured values (no truncation)\")\n",
    "axes[0].set_xlabel(\"Biomarker\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    \n",
    "axes[1].hist(bio_na[~np.isnan(bio_na)], bins=30)\n",
    "axes[1].axvline(LOD, color=\"red\", linestyle=\"--\")\n",
    "axes[1].set_title(\"Values < LOD as missing\")\n",
    "axes[1].set_xlabel(\"Biomarker\")\n",
    "\n",
    "axes[2].hist(bio_const, bins=30)\n",
    "axes[2].axvline(LOD, color=\"red\", linestyle=\"--\")\n",
    "axes[2].set_title(\"Values < LOD replaced by LOD/2\")\n",
    "axes[2].set_xlabel(\"Biomarker\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now apply a log transform (using a small constant to allow zeros if they appear)\n",
    "eps = 1e-6\n",
    "log_measured = np.log(bio_measured + eps)\n",
    "log_na = np.log(bio_na + eps)\n",
    "log_const = np.log(bio_const + eps)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "axes[0].hist(log_measured, bins=30)\n",
    "axes[0].set_title(\"log(measured)\")\n",
    "axes[0].set_xlabel(\"log(biomarker)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(log_na[~np.isnan(log_na)], bins=30)\n",
    "axes[1].set_title(\"log(values ≥ LOD)\")\n",
    "axes[1].set_xlabel(\"log(biomarker)\")\n",
    "\n",
    "axes[2].hist(log_const, bins=30)\n",
    "axes[2].set_title(\"log(values with LOD/2)\")\n",
    "axes[2].set_xlabel(\"log(biomarker)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "truncation_exercise",
   "metadata": {},
   "source": [
    "### Exercise – thinking about truncation\n",
    "\n",
    "- Compare the three original-scale histograms.\n",
    "  - How does treating values below LOD as missing change the shape of the distribution?\n",
    "  - How does replacing values below LOD by a constant (LOD/2) change it?\n",
    "- Compare the three log-scale histograms.\n",
    "  - Does constant substitution create an artificial spike in the distribution?\n",
    "  - How might this affect model fitting and interpretation?\n",
    "- In a real analysis, which approach would you choose, and what sensitivity analyses\n",
    "  might you consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation_intro",
   "metadata": {},
   "source": [
    "## 7. Summary, interpretation and practical rules\n",
    "\n",
    "When transformed variables are used in models, regression coefficients change their meaning:\n",
    "\n",
    "- **Log-transformed exposure (for example, log(intake))**  \n",
    "  - A one-unit increase in log(intake) corresponds to a multiplicative change in the original intake.\n",
    "  - Effects are often interpreted per *k*-fold increase (for example, per doubling), which can be\n",
    "    obtained by rescaling the log variable.\n",
    "\n",
    "- **Box–Cox transformed exposure**  \n",
    "  - Coefficients are expressed in a transformed scale that is not directly intuitive.\n",
    "  - Interpretation often focuses on comparing predicted outcomes at specific, back-transformed\n",
    "    values rather than on a generic “one-unit change”.\n",
    "\n",
    "- **z-scored variables (standardisation)**  \n",
    "  - Effects are per 1 standard deviation increase in the original variable.\n",
    "  - This facilitates comparison of effect sizes between variables but may be less intuitive for\n",
    "    clinical audiences.\n",
    "\n",
    "- **Categorised variables (for example, quantiles)**  \n",
    "  - Coefficients compare categories (for example, highest vs lowest quintile) and are easy to describe.\n",
    "  - However, they conceal variability within categories and depend on arbitrary cut-points.\n",
    "\n",
    "For reporting, it is often helpful to:\n",
    "\n",
    "- Present results both on the transformed scale (for modelling) and back-transformed or re-expressed\n",
    "  in the original units (for interpretation).\n",
    "- Clearly state which transformation was used, and, where relevant, which constants or parameters\n",
    "  (for example, \\(\\lambda\\) in Box–Cox, constants for LOD imputation) were applied.\n",
    "\n",
    "### Practical rules (summary)\n",
    "\n",
    "- Use **plots of the variable and residuals** to guide decisions; do not rely solely on formal\n",
    "  normality tests.\n",
    "- Consider a **log transform** for strongly right-skewed positive variables, especially where\n",
    "  proportional changes are meaningful.\n",
    "- Use **z-scores** when comparing effect sizes across variables on different scales.\n",
    "- Use **Box–Cox** when a simple log transform is clearly inadequate and interpretability can be\n",
    "  handled via back-transformation.\n",
    "- Use **categorisation** sparingly; when it is required (for example, clinical thresholds), be\n",
    "  explicit about cut-points and acknowledge information loss.\n",
    "- For **truncated analytical values**, state clearly how values below LOD / LLOQ were handled and\n",
    "  consider sensitivity analyses with alternative approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_table",
   "metadata": {},
   "source": [
    "### Summary table – transformations and their properties\n",
    "\n",
    "| Approach                      | Type                 | Reversible? | Typical purpose                                     | Advantages                                          | Limitations / cautions                                   | Interpretation of 1-unit change                    |\n",
    "|------------------------------|----------------------|------------|-----------------------------------------------------|-----------------------------------------------------|----------------------------------------------------------|----------------------------------------------------|\n",
    "| Log(x + c)                   | Monotonic transform  | Yes        | Reduce right-skewness; stabilise variance           | Simple; widely understood; often matches biology    | Requires x + c > 0; choice of c affects tail             | Multiplicative change in original x               |\n",
    "| Box–Cox(x; λ)                | Power transform      | Yes        | Reduce skewness; approximate normality              | Flexible; λ estimated from data                     | Less intuitive; sensitive to outliers and truncation      | Model-based; often via back-transforms            |\n",
    "| z-score = (x − mean)/sd      | Linear transform     | Yes        | Standardise scale; compare effect sizes             | Mean 0, sd 1; easy comparison across variables      | Depends on sample distribution; less intuitive clinically | Change per 1 sd in original x                     |\n",
    "| Categorisation (quantiles)   | Non-linear, discrete | No         | Simplify presentation; threshold-based decisions    | Easy to present and explain; suits decision rules   | Information loss; arbitrary cut-points; reduced power     | Category contrasts (e.g. highest vs lowest)       |\n",
    "| Truncation + constant imput. | Ad hoc modification  | Partly     | Handle values < LOD / LLOQ                          | Simple; preserves sample size                       | Artificial spikes; may bias low range                     | Depends on imputation rule                        |\n",
    "| Use measured values (LOD-flag)| Measurement choice  | Yes        | Retain numeric range from instrument                | Avoids artificial truncation                        | Low-range uncertainty; method-specific behaviour          | As for raw scale; note increased uncertainty      |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
