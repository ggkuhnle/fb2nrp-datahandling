{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32623dd",
   "metadata": {},
   "source": [
    "# FB2NEP Workbook 6 – Regression and Modelling: Foundations\n",
    "\n",
    "Version 0.0.5\n",
    "\n",
    "This workbook introduces the foundations of regression modelling in nutritional epidemiology.\n",
    "\n",
    "We will focus on:\n",
    "\n",
    "- Theoretical background to regression.\n",
    "- Linear, logistic, and Cox proportional hazards regression.\n",
    "- Quantile regression.\n",
    "- Model assumptions and diagnostics.\n",
    "- Non-linear models (polynomials and splines).\n",
    "- Interpretation of coefficients (β, OR, RR, HR).\n",
    "- Generating predictions from fitted models.\n",
    "\n",
    "All analyses use the synthetic *FB2NEP cohort*.\n",
    "\n",
    "Run the first code cell to configure the repository and load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FB2NEP bootstrap cell (works both locally and in Colab)\n",
    "#\n",
    "# What this cell does:\n",
    "# - Ensures that we are inside the fb2nep-epi repository.\n",
    "# - In Colab: clones the repository from GitHub if necessary.\n",
    "# - Loads and runs scripts/bootstrap.py.\n",
    "# - Makes the main dataset available as the variable `df`.\n",
    "#\n",
    "# Important:\n",
    "# - You may see messages printed below (for example from pip\n",
    "#   or from the bootstrap script). This is expected.\n",
    "# - You may also see WARNINGS (often in yellow). In most cases\n",
    "#   these are harmless and can be ignored for this module.\n",
    "# - The main thing to watch for is a red error traceback\n",
    "#   (for example FileNotFoundError, ModuleNotFoundError).\n",
    "#   If that happens, please re-run this cell first. If the\n",
    "#   error persists, ask for help.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Configuration: repository location and URL\n",
    "# ------------------------------------------------------------\n",
    "# REPO_URL: address of the GitHub repository.\n",
    "# REPO_DIR: folder name that will be created when cloning.\n",
    "REPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\n",
    "REPO_DIR = \"fb2nep-epi\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Ensure we are inside the fb2nep-epi repository\n",
    "# ------------------------------------------------------------\n",
    "# In local Jupyter, you may already be inside the repository,\n",
    "# for example in fb2nep-epi/notebooks.\n",
    "#\n",
    "# In Colab, the default working directory is /content, so\n",
    "# we need to clone the repository into /content/fb2nep-epi\n",
    "# and then change into that folder.\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "# Case A: we are already in the repository (scripts/bootstrap.py exists here)\n",
    "if (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n",
    "    repo_root = cwd\n",
    "\n",
    "# Case B: we are outside the repository (for example in Colab)\n",
    "else:\n",
    "    repo_root = cwd / REPO_DIR\n",
    "\n",
    "    # Clone the repository if it is not present yet\n",
    "    if not repo_root.is_dir():\n",
    "        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n",
    "    else:\n",
    "        print(f\"Using existing repository at {repo_root}\")\n",
    "\n",
    "    # Change the working directory to the repository root\n",
    "    os.chdir(repo_root)\n",
    "    repo_root = pathlib.Path.cwd()\n",
    "\n",
    "print(f\"Repository root set to: {repo_root}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load scripts/bootstrap.py as a module and call init()\n",
    "# ------------------------------------------------------------\n",
    "# The shared bootstrap script contains all logic to:\n",
    "# - Ensure that required Python packages are installed.\n",
    "# - Ensure that the synthetic dataset exists (and generate it\n",
    "#   if needed).\n",
    "# - Load the dataset into a pandas DataFrame.\n",
    "#\n",
    "# We load the script as a normal Python module (fb2nep_bootstrap)\n",
    "# and then call its init() function.\n",
    "bootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n",
    "\n",
    "if not bootstrap_path.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {bootstrap_path}. \"\n",
    "        \"Please check that the fb2nep-epi repository structure is intact.\"\n",
    "    )\n",
    "\n",
    "# Create a module specification from the file\n",
    "spec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\n",
    "bootstrap = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"fb2nep_bootstrap\"] = bootstrap\n",
    "\n",
    "# Execute the bootstrap script in the context of this module\n",
    "spec.loader.exec_module(bootstrap)\n",
    "\n",
    "# The init() function is defined in scripts/bootstrap.py.\n",
    "# It returns:\n",
    "# - df   : the main synthetic cohort as a pandas DataFrame.\n",
    "# - CTX  : a small context object with paths, flags and settings.\n",
    "df, CTX = bootstrap.init()\n",
    "\n",
    "# Optionally expose a few additional useful variables from the\n",
    "# bootstrap module (if they exist). These are not essential for\n",
    "# most analyses, but can be helpful for advanced use.\n",
    "for name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n",
    "    if hasattr(bootstrap, name):\n",
    "        globals()[name] = getattr(bootstrap, name)\n",
    "\n",
    "print(\"Bootstrap completed successfully.\")\n",
    "print(\"The main dataset is available as the variable `df`.\")\n",
    "print(\"The context object is available as `CTX`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c2bcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries for regression and survival analysis.\n",
    "\n",
    "This cell:\n",
    "\n",
    "- Imports core numerical and plotting libraries.\n",
    "- Imports regression tools (statsmodels, scipy).\n",
    "- Imports patsy for spline functions.\n",
    "- Ensures that the 'lifelines' package is available (local Jupyter and Colab).\n",
    "\"\"\"\n",
    "\n",
    "# Core data handling and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Linear and generalised linear models\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Statistical utilities (for example, Q–Q plots)\n",
    "from scipy import stats\n",
    "\n",
    "# Patsy for design matrices and spline terms.\n",
    "# 'cr' is the cubic regression spline used in restricted cubic spline examples.\n",
    "from patsy import dmatrix, dmatrices, cr\n",
    "\n",
    "# For neat display of tables in notebooks\n",
    "from IPython.display import display\n",
    "\n",
    "# Helper function to ensure 'lifelines' is installed.\n",
    "# This is defined in scripts/helpers_tables.py for the FB2NEP materials.\n",
    "from scripts.helpers_tables import ensure_lifelines\n",
    "\n",
    "# Ensure that 'lifelines' is available, installing it if needed.\n",
    "lifelines = ensure_lifelines()\n",
    "\n",
    "# Import survival analysis tools from lifelines:\n",
    "# - KaplanMeierFitter for Kaplan–Meier curves\n",
    "# - CoxPHFitter for Cox proportional hazards models\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inspect the first few rows and the variable types.\n",
    "\n",
    "This provides a quick overview of the FB2NEP cohort and its variables.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "display(df.head())\n",
    "display(df.dtypes.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af831752",
   "metadata": {},
   "source": [
    "## 1. What regression is\n",
    "\n",
    "Regression modelling is a central tool in epidemiology. In its most basic form, regression estimates the **expected value** of an outcome variable given one or more predictors:\n",
    "\n",
    "$$\n",
    "E(Y \\mid X_1, X_2, \\ldots, X_p).\n",
    "$$\n",
    "\n",
    "The regression model describes a **systematic component** (the part explained by the predictors) and a **random component** (the unexplained variability, or error term).\n",
    "\n",
    "In this workbook we will:\n",
    "\n",
    "- Start with simple regression models.\n",
    "- Extend to different outcome types.\n",
    "- Introduce models for non-linear relationships and for different parts of the outcome distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb63901",
   "metadata": {},
   "source": [
    "### 1.1 Prediction versus inference\n",
    "\n",
    "Regression can be used for different purposes:\n",
    "\n",
    "- **Prediction**: obtain accurate predictions $\\hat{Y}$ for new individuals.\n",
    "- **Inference**: estimate and interpret the parameters (for example, β, OR, HR) and their uncertainty.\n",
    "\n",
    "In nutritional epidemiology we are often interested primarily in **inference**:\n",
    "\n",
    "- How much higher is blood pressure, on average, in individuals with high sodium intake?\n",
    "- What is the hazard ratio for cardiovascular disease per 5 kg/m² higher BMI?\n",
    "\n",
    "Prediction is also important, for example when developing risk scores, but the focus in this workbook is on understanding **parameters** and **assumptions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73eaa7c",
   "metadata": {},
   "source": [
    "Simple visual example: BMI and age.\n",
    "\n",
    "Here we:\n",
    "\n",
    "- Create a scatter plot of BMI against age.\n",
    "- Add rudimentary formatting to make the figure readable.\n",
    "\n",
    "We do *not* fit a model yet; this is purely descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae40a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(df[\"age\"], df[\"BMI\"], alpha=0.3, edgecolor=\"none\")\n",
    "\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_title(\"Scatter plot of BMI against age (FB2NEP cohort)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cba438",
   "metadata": {},
   "source": [
    "### Interpreting the scatter plot\n",
    "\n",
    "This plot shows each participant’s body mass index (BMI) against age.\n",
    "\n",
    "When inspecting such a plot, it is useful to consider:\n",
    "\n",
    "- **Overall pattern**: does BMI tend to increase, decrease, or stay roughly constant with age?\n",
    "- **Linearity**: does a straight line seem reasonable, or is there evidence of curvature?\n",
    "- **Spread**: is the variability of BMI similar across ages, or does it increase or decrease?\n",
    "- **Outliers or subgroups**: are there points or clusters that look unusual?\n",
    "\n",
    "In later sections we will use regression models to describe this relationship more formally. For now, focus on whether a *simple linear model* appears plausible as a first approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad325f39",
   "metadata": {},
   "source": [
    "## 2. Types of regression models\n",
    "\n",
    "In this section we introduce three commonly used regression models in epidemiology:\n",
    "\n",
    "- **Linear regression** for continuous outcomes.\n",
    "- **Logistic regression** for binary outcomes.\n",
    "- **Cox proportional hazards regression** for time-to-event outcomes.\n",
    "\n",
    "The underlying idea is similar in all three cases: we model how the **expected outcome** (mean, probability, hazard) changes with predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0e206",
   "metadata": {},
   "source": [
    "### 2.1 Linear regression\n",
    "\n",
    "Linear regression models a continuous outcome as a linear function of predictors:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon.\n",
    "$$\n",
    "\n",
    "- $ Y $ is a continuous variable (for example, BMI or systolic blood pressure).\n",
    "- $ X_1, X_2, \\ldots, X_p $ are predictors (for example, age, sex, smoking status).\n",
    "- $ \\varepsilon $ is a random error term.\n",
    "\n",
    "The key quantity is the **conditional mean**:\n",
    "\n",
    "$$\n",
    "E(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n",
    "$$\n",
    "\n",
    "The coefficient $ \\beta_j $ describes the expected difference in Y associated with a one-unit difference in \\( X_j \\), holding the other predictors constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacbf609",
   "metadata": {},
   "source": [
    "Linear regression example: BMI on age (and sex).\n",
    "\n",
    "We will:\n",
    "\n",
    "- Fit a simple linear regression model.\n",
    "- Inspect the summary output.\n",
    "- Overlay the fitted line on a scatter plot.\n",
    "\n",
    "Assumptions and diagnostics will be discussed later; for now the aim is to see the basic mechanics.\n",
    "\n",
    "> For this example we assume the following variables exist:\n",
    "> - 'bmi': continuous outcome\n",
    "> - 'age': continuous predictor\n",
    "> - 'sex': binary or categorical (for example 'Male', 'Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757879a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "# Fit an ordinary least squares (OLS) model using a formula interface.\n",
    "model_lin = smf.ols(\"BMI ~ age + C(sex)\", data=df)\n",
    "result_lin = model_lin.fit()\n",
    "\n",
    "# Display a standard model summary.\n",
    "result_lin.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2873de",
   "metadata": {},
   "source": [
    "### Interpreting the linear regression output\n",
    "\n",
    "The model estimates the association between age, sex, and BMI. The key points are:\n",
    "\n",
    "- **Age** is positively associated with BMI.  \n",
    "  - The coefficient for age is **0.071**.  \n",
    "  - *For each additional year of age, the mean BMI is estimated to be 0.071 kg/m² higher, on average, holding sex constant.*  \n",
    "  - The 95 % confidence interval (0.065 to 0.077) indicates a precise estimate.\n",
    "\n",
    "- **Sex** shows no clear association with BMI in this model.  \n",
    "  - The coefficient for `C(sex)[T.M]` is **0.055** with a P-value of 0.335.  \n",
    "  - *Mean BMI does not differ materially between men and women in this dataset.*\n",
    "\n",
    "- **Intercept** (≈ 23.0) represents the estimated mean BMI for the reference group (women) at age 0.  \n",
    "  This has no substantive interpretation but is necessary for the model.\n",
    "\n",
    "- **R² = 0.023**: age and sex explain about **2.3 %** of the variability in BMI.  \n",
    "  This is typical in epidemiological cohorts where many factors influence BMI.\n",
    "\n",
    "- **Model significance**  \n",
    "  - The F-statistic is large with P < 0.001, indicating that the model explains more variation than a null model with no predictors.  \n",
    "  - The small R² emphasises that statistical significance does not necessarily imply a strong association.\n",
    "\n",
    "- **Residual diagnostics**  \n",
    "  - Skewness ≈ 0 and kurtosis ≈ 2.94 suggest residuals close to normality.  \n",
    "  - Durbin–Watson ≈ 2.0 indicates no meaningful autocorrelation (expected for cross-sectional data).  \n",
    "  - The Omnibus and Jarque–Bera tests are statistically significant, but with large samples even very small deviations from normality yield significance.\n",
    "\n",
    "**Summary:**  \n",
    "Age is strongly and positively associated with BMI in the FB2NEP cohort, but age and sex together explain only a modest proportion of the variability in BMI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221b1fa",
   "metadata": {},
   "source": [
    "### Plot the fitted regression line for BMI ~ age.\n",
    "\n",
    "For visual simplicity we will:\n",
    "\n",
    "- Restrict to one sex (for example, 'Female').\n",
    "- Fit a simple model with age as the only predictor in this subgroup.\n",
    "- Overlay the fitted line on the scatter plot.\n",
    "\n",
    "This is purely for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4565c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subset to one sex (adjust the label if your dataset uses different coding).\n",
    "df_female = df[df[\"sex\"] == \"F\"].copy()\n",
    "\n",
    "model_lin_f = smf.ols(\"BMI ~ age\", data=df_female)\n",
    "result_lin_f = model_lin_f.fit()\n",
    "\n",
    "# Create a grid of ages spanning the observed range.\n",
    "age_grid = np.linspace(df_female[\"age\"].min(), df_female[\"age\"].max(), 100)\n",
    "pred_df = pd.DataFrame({\"age\": age_grid})\n",
    "pred_df[\"bmi_hat\"] = result_lin_f.predict(pred_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(df_female[\"age\"], df_female[\"BMI\"], alpha=0.3, edgecolor=\"none\", label=\"Observed BMI\")\n",
    "ax.plot(pred_df[\"age\"], pred_df[\"bmi_hat\"], linewidth=2, label=\"Fitted line\")\n",
    "\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_title(\"Linear regression: BMI ~ age (example subset)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b5322",
   "metadata": {},
   "source": [
    "### 2.2 Logistic regression\n",
    "\n",
    "Logistic regression is used when the outcome is **binary**, for example the presence or absence of hypertension.\n",
    "\n",
    "Let $ Y \\in \\{0, 1\\} $ with $ Y = 1 $ indicating that the event (for example, hypertension) is present. The logistic model specifies the **log odds** of the event as a linear function of predictors:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p,\n",
    "$$\n",
    "\n",
    "where $ p = P(Y = 1 \\mid X_1, \\ldots, X_p) $.\n",
    "\n",
    "If we exponentiate a coefficient $ \\beta_j $, we obtain an **odds ratio**:\n",
    "\n",
    "$$\n",
    "\\exp(\\beta_j)\n",
    "$$\n",
    "\n",
    "which describes the multiplicative change in the odds of the outcome associated with a one-unit increase in $ X_j $, holding other predictors constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac948a2d",
   "metadata": {},
   "source": [
    "#### Understanding the logit transformation\n",
    "\n",
    "Probabilities are restricted to the interval (0, 1). A linear model of the form\n",
    "\n",
    "$$\n",
    "p = \\beta_0 + \\beta_1 X\n",
    "$$\n",
    "\n",
    "is unsuitable because it can produce values outside this range.\n",
    "\n",
    "To use linear predictors sensibly, we transform the probability using the **logit**:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right).\n",
    "$$\n",
    "\n",
    "The logit has two key properties:\n",
    "\n",
    "- It maps probabilities $ p \\in (0, 1) $ to the whole real line $ (-\\infty, +\\infty) $.  \n",
    "- It is strictly increasing: higher probability corresponds to higher logit.\n",
    "\n",
    "By modelling the *logit* of the probability as a linear function of predictors, logistic regression ensures that all predicted values are valid probabilities when we convert back using the inverse-logit:\n",
    "\n",
    "$$\n",
    "p = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)}, \\qquad \\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n",
    "$$\n",
    "\n",
    "This connection between the probability \\( p \\), the logit, and the linear predictor is the basis of logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualising the logit transformation.\n",
    "\n",
    "We:\n",
    "\n",
    "- Create a grid of probabilities between 0 and 1.\n",
    "- Compute logit(p) = log(p / (1 - p)) for each value.\n",
    "- Plot logit(p) against p.\n",
    "\n",
    "This illustrates how the logit maps probabilities in (0, 1) to the whole real line.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Avoid exactly 0 and 1 to prevent division by zero in log(p / (1 - p))\n",
    "p = np.linspace(0.001, 0.999, 200)\n",
    "logit_p = np.log(p / (1 - p))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.plot(p, logit_p)\n",
    "\n",
    "ax.set_xlabel(\"Probability p\")\n",
    "ax.set_ylabel(\"logit(p) = log(p / (1 - p))\")\n",
    "ax.set_title(\"The logit transformation\")\n",
    "\n",
    "# Add a horizontal line at 0 to highlight p = 0.5 (where odds = 1)\n",
    "ax.axhline(0, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9912884",
   "metadata": {},
   "source": [
    "#### Interpreting the logit plot\n",
    "\n",
    "- As \\( p \\to 0 \\), logit(p) goes to **−∞**.  \n",
    "- As \\( p \\to 1 \\), logit(p) goes to **+∞**.  \n",
    "- At \\( p = 0.5 \\), logit(p) = 0 (odds = 1:1).\n",
    "\n",
    "This explains why modelling the *logit* as a linear function of predictors still gives\n",
    "valid probabilities once we apply the inverse-logit transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a604b",
   "metadata": {},
   "source": [
    "Logistic regression example: hypertension on BMI and age.\n",
    "\n",
    "We assume there is a binary outcome variable 'hypertension' coded 0/1.\n",
    "\n",
    "The model:\n",
    "\n",
    "    logit(P(hypertension = 1)) = β0 + β1 * BMI + β2 * age + β3 * sex\n",
    "\n",
    "We fit the model and inspect the estimated odds ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f63fd1",
   "metadata": {},
   "source": [
    "#### Creating a binary hypertension variable\n",
    "\n",
    "Logistic regression requires a binary outcome. In this workbook we define a variable\n",
    "`hypertension` as:\n",
    "\n",
    "- 1 if the participant has systolic blood pressure ≥ 140 mmHg, or  \n",
    "- diastolic blood pressure ≥ 90 mmHg, or  \n",
    "- is on antihypertensive medication,\n",
    "\n",
    "and 0 otherwise.\n",
    "\n",
    "This is one of several possible definitions; different studies may use slightly different\n",
    "cut-offs. The important point here is to obtain a clearly defined 0/1 variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a binary hypertension variable for logistic regression.\n",
    "\n",
    "Definition (example):\n",
    "- Hypertension = 1 if sbp ≥ 140 mmHg or dbp ≥ 90 mmHg or on blood pressure medication.\n",
    "- Hypertension = 0 otherwise.\n",
    "\n",
    "Adjust the variable names and thresholds if your dataset uses different coding.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Start with threshold-based hypertension\n",
    "hypertension = (df[\"SBP\"] >= 140) \n",
    "\n",
    "df[\"hypertension\"] = hypertension.astype(int)\n",
    "\n",
    "df[\"hypertension\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9418998",
   "metadata": {},
   "source": [
    "Logistic regression example: hypertension on BMI and age.\n",
    "\n",
    "We assume there is a binary outcome variable 'hypertension' coded 0/1.\n",
    "\n",
    "The model:\n",
    "\n",
    "    logit(P(hypertension = 1)) = β0 + β1 * BMI + β2 * age + β3 * sex\n",
    "\n",
    "We fit the model and inspect the estimated odds ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfa256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# Fit logistic regression using the formula interface.\n",
    "model_log = smf.logit(\"hypertension ~ BMI + age + C(sex)\", data=df)\n",
    "result_log = model_log.fit()\n",
    "\n",
    "# Display the model summary.\n",
    "display(result_log.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f07457",
   "metadata": {},
   "source": [
    "### Interpreting the logistic regression output\n",
    "\n",
    "The table above shows the results of a logistic regression model in which hypertension\n",
    "(0/1) is regressed on sex, BMI, and age. Logistic regression models the **log odds**\n",
    "of the outcome, so the coefficients in the summary are expressed on the **logit scale**.\n",
    "\n",
    "Key elements of the output are:\n",
    "\n",
    "- **Coefficients (coef):**  \n",
    "  These represent changes in the *log odds* of hypertension per unit change in the\n",
    "  predictor, holding the other variables constant. They are not directly intuitive and\n",
    "  are normally exponentiated to obtain more interpretable effect measures.\n",
    "\n",
    "- **Standard errors and z-values:**  \n",
    "  Used to assess whether the coefficients differ meaningfully from zero on the log-odds\n",
    "  scale. With large samples, even small effects can achieve small P-values.\n",
    "\n",
    "- **P-values:**  \n",
    "  Indicate evidence against the null hypothesis that a coefficient is zero on the log-odds\n",
    "  scale. Here, BMI and age show very small P-values, suggesting clear associations.\n",
    "\n",
    "- **Confidence intervals ([0.025, 0.975]):**  \n",
    "  These are 95 % confidence intervals for the log-odds coefficients.\n",
    "\n",
    "- **Pseudo-R² (0.024):**  \n",
    "  Indicates that the model explains about 2.4 % of the variation in the *log odds*.\n",
    "  This is typical for epidemiological data, where many factors contribute to hypertension.\n",
    "\n",
    "- **LL-Null and LL:**  \n",
    "  The log-likelihood values for the intercept-only model and for the full model\n",
    "  respectively. The likelihood ratio test (LLR P-value) shows that the model provides\n",
    "  a significantly better fit than an intercept-only model.\n",
    "\n",
    "Because the coefficients are expressed on the log-odds scale, they are not directly\n",
    "interpretable in terms of risk or probability. In the next step we convert them into\n",
    "**odds ratios**, which provide a more intuitive multiplicative measure of association.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b444b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract odds ratios and 95 % confidence intervals.\n",
    "params = result_log.params\n",
    "conf = result_log.conf_int()\n",
    "or_table = pd.DataFrame({\n",
    "    \"OR\": np.exp(params),\n",
    "    \"CI_lower\": np.exp(conf[0]),\n",
    "    \"CI_upper\": np.exp(conf[1]),\n",
    "})\n",
    "\n",
    "or_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee75c5",
   "metadata": {},
   "source": [
    "### From log-odds coefficients to odds ratios\n",
    "\n",
    "Logistic regression estimates effects on the **log odds** scale:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n",
    "$$\n",
    "\n",
    "A coefficient $\\beta_j$ therefore represents the expected change in the **log odds**\n",
    "of the outcome per one-unit change in $X_j$.\n",
    "\n",
    "To obtain an interpretable effect measure we **exponentiate** the coefficient:\n",
    "\n",
    "$$\n",
    "\\exp(\\beta_j).\n",
    "$$\n",
    "\n",
    "This converts the change in log odds into a **multiplicative change in the odds** of\n",
    "the outcome:\n",
    "\n",
    "- $\\exp(\\beta_j) = 1$ → no association.  \n",
    "- $\\exp(\\beta_j) > 1$ → higher odds of the outcome.  \n",
    "- $\\exp(\\beta_j) < 1$ → lower odds of the outcome.\n",
    "\n",
    "These exponentiated values are known as **odds ratios (ORs)**.\n",
    "\n",
    "For example, if the coefficient for BMI is 0.059, then:\n",
    "\n",
    "$$\n",
    "\\exp(0.059) \\approx 1.06,\n",
    "$$\n",
    "\n",
    "meaning that each additional unit of BMI is associated with approximately **6 % higher\n",
    "odds** of hypertension, holding other variables constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0730e",
   "metadata": {},
   "source": [
    "### Predicted probabilities from the logistic model\n",
    "\n",
    "We can use the fitted logistic regression model to obtain **predicted probabilities**\n",
    "of hypertension for specific combinations of predictors.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "1. Plot the predicted probability of hypertension across BMI **with a 95 % confidence band**\n",
    "   for a reference profile (for example, age 60 years, sex = F).\n",
    "2. Plot predicted probabilities across BMI for **several different ages** to illustrate\n",
    "   how the age effect shifts the whole curve.\n",
    "\n",
    "In both cases, age and sex are treated as fixed at chosen values, so any changes in the\n",
    "curve reflect only the association with BMI (and age, where varied explicitly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicted probability of hypertension across BMI with 95 % confidence band.\n",
    "\n",
    "We:\n",
    "\n",
    "- Fix age and sex at reference values.\n",
    "- Vary BMI from the 5th to the 95th percentile.\n",
    "- Use the fitted logistic model to compute:\n",
    "  - the linear predictor (η),\n",
    "  - its standard error,\n",
    "  - a 95 % confidence interval for η,\n",
    "  - and then transform these to probabilities using the inverse logit.\n",
    "\"\"\"\n",
    "\n",
    "import patsy\n",
    "\n",
    "# Reference profile\n",
    "age_ref = 60\n",
    "sex_ref = \"F\"  # adjust if coding is different\n",
    "\n",
    "# BMI grid\n",
    "bmi_grid = np.linspace(\n",
    "    df[\"BMI\"].quantile(0.05),\n",
    "    df[\"BMI\"].quantile(0.95),\n",
    "    100\n",
    ")\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"BMI\": bmi_grid,\n",
    "    \"age\": age_ref,\n",
    "    \"sex\": sex_ref,\n",
    "})\n",
    "\n",
    "# Build the design matrix for the new data using the same formula structure\n",
    "formula = \"hypertension ~ BMI + age + C(sex)\"\n",
    "design_info = result_log.model.data.design_info\n",
    "X_new = patsy.build_design_matrices([design_info], pred_df)[0]\n",
    "\n",
    "# Parameter estimates and covariance matrix\n",
    "beta_hat = result_log.params.values\n",
    "cov_beta = result_log.cov_params().values\n",
    "\n",
    "# Linear predictor η = Xβ\n",
    "eta_hat = X_new @ beta_hat\n",
    "\n",
    "# Standard error of η: sqrt(diag(X cov_beta X^T))\n",
    "var_eta = np.einsum(\"ij,jk,ik->i\", X_new, cov_beta, X_new)\n",
    "se_eta = np.sqrt(var_eta)\n",
    "\n",
    "# 95 % CI on the η (log-odds) scale\n",
    "z = 1.96\n",
    "eta_lower = eta_hat - z * se_eta\n",
    "eta_upper = eta_hat + z * se_eta\n",
    "\n",
    "# Inverse logit to get probabilities and CIs\n",
    "def inv_logit(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "p_hat = inv_logit(eta_hat)\n",
    "p_lower = inv_logit(eta_lower)\n",
    "p_upper = inv_logit(eta_upper)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.plot(bmi_grid, p_hat, linewidth=2, label=\"Predicted probability\")\n",
    "ax.fill_between(bmi_grid, p_lower, p_upper, alpha=0.2, label=\"95 % CI\")\n",
    "\n",
    "ax.set_xlabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_ylabel(\"Predicted probability of hypertension\")\n",
    "ax.set_title(f\"Hypertension vs BMI (age = {age_ref}, sex = {sex_ref})\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90be396",
   "metadata": {},
   "source": [
    "> You can change age and sex in the function to get different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicted probability of hypertension across BMI for different ages.\n",
    "\n",
    "We:\n",
    "\n",
    "- Choose a small set of ages (for example, 40, 60, 80 years).\n",
    "- For each age, vary BMI over the same grid.\n",
    "- Compute predicted probabilities from the logistic model.\n",
    "- Plot the curves on the same graph.\n",
    "\n",
    "This illustrates how age shifts the entire BMI–hypertension curve.\n",
    "\"\"\"\n",
    "\n",
    "ages_to_show = [40, 60, 80]  # adjust as desired\n",
    "sex_ref = \"F\"                # keep sex fixed\n",
    "\n",
    "bmi_grid = np.linspace(\n",
    "    df[\"BMI\"].quantile(0.05),\n",
    "    df[\"BMI\"].quantile(0.95),\n",
    "    100\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for a in ages_to_show:\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"BMI\": bmi_grid,\n",
    "        \"age\": a,\n",
    "        \"sex\": sex_ref,\n",
    "    })\n",
    "    # Use the fitted logistic model directly for probabilities\n",
    "    p_hyp = result_log.predict(pred_df)\n",
    "    ax.plot(bmi_grid, p_hyp, linewidth=2, label=f\"Age {a} years\")\n",
    "\n",
    "ax.set_xlabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_ylabel(\"Predicted probability of hypertension\")\n",
    "ax.set_title(f\"Hypertension vs BMI for different ages (sex = {sex_ref})\")\n",
    "ax.legend(title=\"Profile\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3874128",
   "metadata": {},
   "source": [
    "### Risk ratios (RR) and their relation to odds ratios (OR)\n",
    "\n",
    "In prospective cohort studies the most common effect measure is the **risk ratio (RR)**:\n",
    "\n",
    "$$\n",
    "RR = \\frac{P(Y = 1 \\mid X = 1)}{P(Y = 1 \\mid X = 0)}.\n",
    "$$\n",
    "\n",
    "The RR compares **probabilities**, and is directly interpretable as a multiplicative\n",
    "change in risk.\n",
    "\n",
    "Logistic regression, however, models the **odds**:\n",
    "\n",
    "$$\n",
    "\\text{odds} = \\frac{p}{1-p},\n",
    "\\qquad\n",
    "OR = \\frac{\\text{odds}_1}{\\text{odds}_0}.\n",
    "$$\n",
    "\n",
    "The OR and RR differ because the odds and the probability are not the same.\n",
    "\n",
    "Two important points:\n",
    "\n",
    "1. **When the outcome is rare** (for example, prevalence below about 10 %),  \n",
    "   $$\n",
    "   OR \\approx RR.\n",
    "   $$\n",
    "   In this setting, logistic regression produces estimates that can be interpreted\n",
    "   approximately as risk ratios.\n",
    "\n",
    "2. **When the outcome is common**, the OR can be noticeably larger than the RR\n",
    "   (sometimes substantially so). This is not an error: it reflects the mathematical\n",
    "   behaviour of the odds.\n",
    "\n",
    "In prospective cohort studies the RR is often preferable because it is easier to\n",
    "interpret—“a 30 % higher risk” is more intuitive than “a 30 % higher odds”.\n",
    "Logistic regression cannot estimate RRs directly because of its logit link function,\n",
    "but alternative models exist:\n",
    "\n",
    "- **log-binomial regression** (models log risk; can have convergence issues);\n",
    "- **Poisson regression with robust variance** (commonly used and usually stable).\n",
    "\n",
    "In this workbook we focus on logistic regression, but the distinction between OR and RR\n",
    "is important when interpreting results, especially when the outcome is common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac5107",
   "metadata": {},
   "source": [
    "### Visualising the difference between odds ratios and risk ratios\n",
    "\n",
    "To see how odds ratios (OR) and risk ratios (RR) differ in practice, we can look at\n",
    "what happens for different **baseline risks**.\n",
    "\n",
    "For a given baseline risk $ $p_0 $ in the unexposed group and a chosen odds ratio (OR),\n",
    "we can compute:\n",
    "\n",
    "- the risk $ p_1 $ in the exposed group implied by that OR, and  \n",
    "- the corresponding risk ratio $ RR = p_1 / p_0 $.\n",
    "\n",
    "If the outcome is rare (small $p_0 $), OR and RR are very similar.  \n",
    "When the outcome is common (large $p_0 $), OR and RR diverge.\n",
    "\n",
    "The following plot shows RR as a function of OR for three different baseline risks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot RR vs OR for different baseline risks.\n",
    "\n",
    "We:\n",
    "\n",
    "- Choose three baseline risks p0 (e.g. 5 %, 20 %, 50 %).\n",
    "- For a range of odds ratios (OR), compute the implied risk ratio (RR).\n",
    "- Plot RR against OR, with one curve per baseline risk.\n",
    "\n",
    "This illustrates that OR ≈ RR when the outcome is rare, but OR increasingly\n",
    "overstates the RR when the outcome is common.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Baseline risks in the unexposed group\n",
    "p0_values = [0.05, 0.20, 0.50]\n",
    "\n",
    "# Range of odds ratios to consider\n",
    "or_grid = np.linspace(1.0, 5.0, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for p0 in p0_values:\n",
    "    # Odds in unexposed group\n",
    "    odds0 = p0 / (1 - p0)\n",
    "\n",
    "    # For each OR, compute odds1, p1, and RR\n",
    "    odds1 = or_grid * odds0\n",
    "    p1 = odds1 / (1 + odds1)\n",
    "    rr = p1 / p0\n",
    "\n",
    "    ax.plot(or_grid, rr, linewidth=2, label=f\"Baseline risk p0 = {p0:.2f}\")\n",
    "\n",
    "ax.plot([1, 5], [1, 5], linestyle=\"--\", linewidth=1, label=\"Line RR = OR\")\n",
    "\n",
    "ax.set_xlabel(\"Odds ratio (OR)\")\n",
    "ax.set_ylabel(\"Risk ratio (RR)\")\n",
    "ax.set_title(\"Relationship between OR and RR for different baseline risks\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca51e8",
   "metadata": {},
   "source": [
    "#### Interpreting the OR–RR plot\n",
    "\n",
    "The dashed line shows where OR and RR would be equal.\n",
    "\n",
    "- For **rare outcomes** (baseline risk \\( p_0 = 0.05 \\)), the curve lies very close to\n",
    "  the line RR = OR. In this situation the odds ratio approximates the risk ratio well.\n",
    "\n",
    "- For **more common outcomes** (for example, \\( p_0 = 0.20 \\) or \\( p_0 = 0.50 \\)),\n",
    "  the curves lie increasingly above the line RR = OR. For the same OR, the RR is\n",
    "  noticeably smaller.\n",
    "\n",
    "This explains why, in prospective studies with common outcomes, odds ratios can give\n",
    "the impression of larger effects than risk ratios, even when both are correctly\n",
    "calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4330c0",
   "metadata": {},
   "source": [
    "### 2.3 Cox proportional hazards regression\n",
    "\n",
    "In many epidemiological studies we are interested in **time-to-event** outcomes,\n",
    "for example time to incident cardiovascular disease. Cox proportional hazards regression\n",
    "models the **hazard**—the instantaneous event rate at time \\( t \\)—as:\n",
    "\n",
    "$\n",
    "h(t \\mid X_1, \\ldots, X_p) =\n",
    "    h_0(t)\\,\\exp(\\beta_1 X_1 + \\cdots + \\beta_p X_p),\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ h_0(t) $ is the **baseline hazard**, left unspecified, and  \n",
    "- $ \\exp(\\beta_j) $ are **hazard ratios** comparing individuals who differ by\n",
    "  one unit in $ X_j $, holding other variables constant.\n",
    "\n",
    "A hazard ratio $ \\exp(\\beta_j) > 1 $ indicates a higher instantaneous rate of the\n",
    "event associated with higher $ X_j $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd24440",
   "metadata": {},
   "source": [
    "#### The proportional hazards assumption\n",
    "\n",
    "The key assumption of the Cox model is that the hazard ratios are **constant over time**.\n",
    "Formally, for two individuals with covariates $ X $ and $ X' $:\n",
    "\n",
    "$$\n",
    "\\frac{h(t \\mid X)}{h(t \\mid X')} =\n",
    "    \\exp\\big( \\beta (X - X') \\big),\n",
    "$$\n",
    "\n",
    "and this ratio does **not** depend on $ t $.  \n",
    "\n",
    "This means that:\n",
    "\n",
    "- the covariates shift the hazard **multiplicatively**,  \n",
    "- the effect size is the same at all follow-up times, and  \n",
    "- the shapes of the survival curves may differ, but **their ratio on the log-hazard scale is parallel**.\n",
    "\n",
    "Violation of this assumption (for example, if the effect of age or smoking weakens over time)\n",
    "requires either time-varying coefficients or an alternative modelling approach.\n",
    "\n",
    "In later sections we examine diagnostic tools, such as **Schoenfeld residuals**, to assess\n",
    "the validity of the proportional hazards assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1713df10",
   "metadata": {},
   "source": [
    "### Understanding censoring in time-to-event data\n",
    "\n",
    "Time-to-event outcomes are rarely observed for all participants. Some individuals:\n",
    "\n",
    "- have not yet experienced the event by the end of follow-up,\n",
    "- are lost to follow-up,\n",
    "- withdraw from the study, or\n",
    "- die from another cause before the event of interest occurs.\n",
    "\n",
    "In these situations we do **not** know the true event time, only that it is **later than**\n",
    "the last time at which the participant was observed. This is called **right-censoring**.\n",
    "\n",
    "Formally, for each participant we record:\n",
    "\n",
    "- the observed time $ T = \\min(T^*, C) $, where  \n",
    "  $ T^* $ is the true event time and $ C $ is the censoring time,\n",
    "- an indicator $ \\delta = 1 $ if the event occurred and $ \\delta = 0 $ if the observation was censored.\n",
    "\n",
    "Censoring is acceptable for Cox regression provided it is **non-informative**, meaning that\n",
    "the mechanism determining censoring is unrelated to the participant’s underlying risk of\n",
    "the event. For example, end-of-study censoring is typically non-informative, whereas\n",
    "dropping out due to worsening illness may not be.\n",
    "\n",
    "Censoring is a central feature of survival analysis and must be handled explicitly in\n",
    "statistical models such as the Cox proportional hazards model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c2662",
   "metadata": {},
   "source": [
    "### Visualising censoring at the individual level\n",
    "\n",
    "Censoring is often easier to understand if we look at individual follow-up histories.\n",
    "\n",
    "In the plot below each horizontal line represents a **single participant**:\n",
    "\n",
    "- The left end of the line is the time when the participant **enters** the risk set.  \n",
    "- The right end of the line is the time when observation **stops** (either because the\n",
    "  event occurs or because follow-up ends for another reason).\n",
    "\n",
    "We distinguish:\n",
    "\n",
    "- **Event**: the line ends with an event marker.  \n",
    "- **Right censoring**: the line ends with a censoring marker (follow-up stops before\n",
    "  the event is observed).  \n",
    "- **Left censoring** (or more commonly in cohorts, **left truncation / delayed entry**):\n",
    "  the line starts after time 0, indicating that the participant only becomes observable\n",
    "  from that time onwards.\n",
    "\n",
    "The example is based on artificial data and is intended purely to illustrate different\n",
    "types of observation patterns that arise in time-to-event analyses.\n",
    "\n",
    "![Censoring](../_assets/censoring.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121625a9",
   "metadata": {},
   "source": [
    "### Prepare time-to-event variables for Cox regression\n",
    "\n",
    "In order to analyse the data, we need to know the *time-to-event*. Our dataset has event-dates, so they need to be converted:\n",
    "\n",
    "We have:\n",
    "\n",
    "- 'baseline_date': date of baseline assessment.\n",
    "- 'CVD_date': date of incident cardiovascular disease (NaT if no event).\n",
    "- 'CVD_incident': 1 if incident CVD occurred during follow-up, 0 otherwise.\n",
    "\n",
    "We construct:\n",
    "\n",
    "- 'event_cvd'  : event indicator for Cox (1 = event, 0 = censored).\n",
    "- 'time_cvd'   : follow-up time in years from baseline to event or censoring.\n",
    "\n",
    "For simplicity, we use a common censoring date equal to the latest of all observed\n",
    "CVD events or baseline dates. In a real analysis this would usually be the study\n",
    "end date.\n",
    "\n",
    "> The common censoring date is usually the **end of follow-up**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79927861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure date variables are in datetime format\n",
    "df[\"baseline_date\"] = pd.to_datetime(df[\"baseline_date\"])\n",
    "df[\"CVD_date\"] = pd.to_datetime(df[\"CVD_date\"])\n",
    "\n",
    "# Event indicator for Cox model\n",
    "df[\"event_cvd\"] = df[\"CVD_incident\"].astype(int)\n",
    "\n",
    "# Choose a global censoring date (here: latest date observed in the cohort)\n",
    "global_censor_date = pd.concat(\n",
    "    [df[\"CVD_date\"].dropna(), df[\"baseline_date\"].dropna()]\n",
    ").max()\n",
    "\n",
    "print(\"Global censoring date:\", global_censor_date.date())\n",
    "\n",
    "# Time to event for those with CVD; NaN if no event\n",
    "event_times = (df[\"CVD_date\"] - df[\"baseline_date\"]).dt.days / 365.25\n",
    "\n",
    "# Time to censoring for those without CVD\n",
    "censor_times = (global_censor_date - df[\"baseline_date\"]).dt.days / 365.25\n",
    "\n",
    "# Combine into a single follow-up time variable\n",
    "df[\"time_cvd\"] = np.where(df[\"event_cvd\"] == 1, event_times, censor_times)\n",
    "\n",
    "# Quick sanity check\n",
    "print(df[[\"time_cvd\", \"event_cvd\"]].describe(include=\"all\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b78cc9",
   "metadata": {},
   "source": [
    "### Kaplan–Meier curves and censoring\n",
    "\n",
    "In time-to-event analysis we often describe the data using a **Kaplan–Meier survival curve**.\n",
    "For each time \\( t \\), the Kaplan–Meier estimator \\( \\hat{S}(t) \\) gives the estimated\n",
    "probability of *remaining free of the event* up to time \\( t \\).\n",
    "\n",
    "Using the constructed variables:\n",
    "\n",
    "- `time_cvd` — follow-up time (in years) to CVD or censoring,  \n",
    "- `event_cvd` — event indicator (1 = CVD event, 0 = censored),\n",
    "\n",
    "we can estimate the survival function for incident cardiovascular disease.\n",
    "\n",
    "Graphically:\n",
    "\n",
    "- The **stepwise curve** shows the estimated survival probability over time.  \n",
    "- **Downward steps** occur whenever CVD events take place.  \n",
    "- **Censoring marks** (small vertical ticks) indicate participants who were censored at\n",
    "  that point in time (for example, end of follow-up or loss to follow-up).\n",
    "\n",
    "The Kaplan–Meier method is **non-parametric**: it does not assume any particular shape for\n",
    "the hazard or survival curve. It does, however, rely on the assumption of\n",
    "**non-informative censoring** — that the mechanism causing censoring is unrelated to the\n",
    "participant’s underlying risk of CVD.\n",
    "\n",
    "In the next step we compute and plot the Kaplan–Meier curve for the FB2NEP cohort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset (drop missing values just in case)\n",
    "df_surv = df[[\"time_cvd\", \"event_cvd\"]].dropna().copy()\n",
    "\n",
    "# Basic summary for the students\n",
    "print(\"Number of observations:\", len(df_surv))\n",
    "print(\"Event count:\", int(df_surv[\"event_cvd\"].sum()))\n",
    "print(\"Censored:\", int((1 - df_surv[\"event_cvd\"]).sum()))\n",
    "\n",
    "# Fit KM curve\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(\n",
    "    durations=df_surv[\"time_cvd\"],\n",
    "    event_observed=df_surv[\"event_cvd\"],\n",
    "    label=\"CVD-free survival\"\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "kmf.plot_survival_function(ax=ax, ci_show=True)\n",
    "\n",
    "ax.set_xlabel(\"Follow-up time (years)\")\n",
    "ax.set_ylabel(\"Estimated survival probability $\\\\hat{S}(t)$\")\n",
    "ax.set_title(\"Kaplan–Meier curve: time to incident CVD\")\n",
    "\n",
    "# Keep 0–1 range clear\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df088724",
   "metadata": {},
   "source": [
    "### From Kaplan–Meier curves to Cox regression\n",
    "\n",
    "The Kaplan–Meier curve provides a **descriptive**, non-parametric summary of time to\n",
    "incident CVD. It shows how the probability of remaining event free changes over time,\n",
    "but it has two important limitations:\n",
    "\n",
    "- It usually compares only **one factor at a time** (for example, survival by sex or\n",
    "  by exposure group).\n",
    "- It does **not** adjust for other covariates (such as age, BMI, smoking, or blood\n",
    "  pressure) that may confound the association.\n",
    "\n",
    "In practice, epidemiological studies almost always need to:\n",
    "\n",
    "- estimate the association between several predictors and time to event **simultaneously**,  \n",
    "- obtain **adjusted effect estimates** (for example, “hazard ratio per 5 kg/m² higher BMI,\n",
    "  adjusted for age and sex”), and  \n",
    "- test whether these associations are compatible with the **proportional hazards**\n",
    "  assumption.\n",
    "\n",
    "The **Cox proportional hazards model** addresses these needs by modelling the hazard as\n",
    "\n",
    "$$\n",
    "h(t \\mid X) = h_0(t)\\,\\exp(\\beta_1 X_1 + \\cdots + \\beta_p X_p),\n",
    "$$\n",
    "\n",
    "where $ h_0(t) $ is an unspecified baseline hazard and the exponentiated coefficients\n",
    "$ \\exp(\\beta_j) $ are **hazard ratios**. In the next step we fit such a model to the\n",
    "FB2NEP cohort using age, BMI and sex as predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cox regression example: time to CVD event.\n",
    "\n",
    "We assume the dataset contains:\n",
    "\n",
    "- 'time_cvd': follow-up time (for example, in years).\n",
    "- 'event_cvd': event indicator (1 if event occurred, 0 if censored).\n",
    "- 'age', 'sex', 'bmi' as predictors.\n",
    "\n",
    "We use the `lifelines` package for Cox regression.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Select relevant columns and drop missing values.\n",
    "cols = [\"time_cvd\", \"event_cvd\", \"age\", \"BMI\", \"sex\"]\n",
    "df_cox = df[cols].dropna().copy()\n",
    "\n",
    "# Lifelines expects categorical variables to be encoded appropriately.\n",
    "# Here we create a simple indicator for sex == 'Female' as an example.\n",
    "df_cox[\"sex_female\"] = (df_cox[\"sex\"] == \"F\").astype(int)\n",
    "\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_cox[[\"time_cvd\", \"event_cvd\", \"age\", \"BMI\", \"sex_female\"]],\n",
    "        duration_col=\"time_cvd\",\n",
    "        event_col=\"event_cvd\")\n",
    "\n",
    "summary_df = cph.summary\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7a256",
   "metadata": {},
   "source": [
    "### Interpreting the Cox regression output\n",
    "\n",
    "The Cox model summary shows, for each predictor:\n",
    "\n",
    "- the estimated log hazard ratio (coef),\n",
    "- the hazard ratio itself (`exp(coef)`),\n",
    "- standard errors and confidence intervals,\n",
    "- tests of whether the coefficient differs from zero.\n",
    "\n",
    "For example, a hazard ratio of 1.10 for BMI (per 1 kg/m²) would mean\n",
    "a 10 % higher instantaneous rate of CVD per unit increase in BMI,\n",
    "assuming the proportional hazards assumption holds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfae2a6",
   "metadata": {},
   "source": [
    "### Survival curves for specific risk profiles\n",
    "\n",
    "The Kaplan–Meier estimator provides a descriptive, unadjusted view of the overall\n",
    "survival experience in the cohort. However, it does not allow us to compare\n",
    "**adjusted** survival patterns for individuals who differ in specific covariates.\n",
    "\n",
    "Once a Cox proportional hazards model has been fitted, we can obtain **model-based\n",
    "survival curves** for hypothetical individuals. These curves show:\n",
    "\n",
    "- how the predicted survival probability changes over time **for a given set of\n",
    "  covariates**,  \n",
    "- how two individuals with different risk factor profiles (for example, higher vs\n",
    "  lower BMI) are expected to differ, **holding all other variables constant**, and  \n",
    "- how the Cox model translates hazard ratios into differences in survival over time.\n",
    "\n",
    "These curves are not non-parametric estimates; they are **conditional survival functions**\n",
    "derived from the fitted Cox model:\n",
    "\n",
    "$$\n",
    "\\hat{S}(t \\mid X) = \\hat{S}_0(t)^{\\exp(\\beta X)},\n",
    "$$\n",
    "\n",
    "where $ \\hat{S}_0(t) $ is the estimated baseline survival curve and  \n",
    "$ \\exp(\\beta X) $ is the relative hazard for the specified covariate pattern.\n",
    "\n",
    "In the next step we compute survival curves for two contrasting profiles (for example,\n",
    "a lower-BMI and a higher-BMI individual) to illustrate how covariate differences influence\n",
    "predicted survival.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c36f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define two example profiles.\n",
    "profile_low = {\n",
    "    \"age\": 60,\n",
    "    \"BMI\": 24,\n",
    "    \"sex_female\": 1,\n",
    "}\n",
    "\n",
    "profile_high = {\n",
    "    \"age\": 60,\n",
    "    \"BMI\": 32,\n",
    "    \"sex_female\": 1,\n",
    "}\n",
    "\n",
    "profiles = pd.DataFrame([profile_low, profile_high])\n",
    "profiles.index = [\"BMI 24\", \"BMI 32\"]\n",
    "\n",
    "surv = cph.predict_survival_function(profiles)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for label in surv.columns:\n",
    "    ax.plot(surv.index, surv[label], label=label)\n",
    "\n",
    "ax.set_xlabel(\"Follow-up time\")\n",
    "ax.set_ylabel(\"Estimated survival probability\")\n",
    "ax.set_title(\"Cox model: example survival curves\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8c198",
   "metadata": {},
   "source": [
    "## 3. Using quantile-based categories in regression\n",
    "\n",
    "A common approach in nutritional epidemiology is to convert a continuous exposure\n",
    "(for example, flavanol intake, dietary fibre, plasma biomarkers) into **quantile-based\n",
    "categories**, such as tertiles, quartiles, or quintiles.\n",
    "\n",
    "This creates ordered categories with approximately equal numbers of participants in each\n",
    "group. Analysts then use these categories as predictors in regression models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd57bb",
   "metadata": {},
   "source": [
    "### 3.1 Creating quantile categories\n",
    "\n",
    "Let $ X $ be the continuous exposure of interest. We divide its distribution into\n",
    "$ K $ groups (for example, $ K = 5 $ for quintiles). Each participant is assigned\n",
    "a category $ Q \\in \\{1, \\ldots, K\\} $.\n",
    "\n",
    "In analysis this categorical variable enters the regression through **dummy variables**,\n",
    "treating the lowest group (usually Q1) as the **reference category**.\n",
    "\n",
    "For example, in a linear model:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_2 I(Q = 2) + \\beta_3 I(Q = 3) +\n",
    "      \\beta_4 I(Q = 4) + \\beta_5 I(Q = 5)\n",
    "      + \\gamma_1 Z_1 + \\cdots + \\gamma_p Z_p,\n",
    "$$\n",
    "\n",
    "where $ Z_1, \\ldots, Z_p $ are adjustment variables.\n",
    "\n",
    "The coefficients $ \\beta_2, \\ldots, \\beta_5 $ represent **differences in the mean**\n",
    "outcome in each quantile compared with the reference quantile Q1.\n",
    "\n",
    "The same structure applies in logistic regression (as odds ratios) or Cox regression\n",
    "(as hazard ratios).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2dceb",
   "metadata": {},
   "source": [
    "### 3.2 Test for trend across quantiles\n",
    "\n",
    "Because quantile groups have a natural order (Q1 < Q2 < Q3 < Q4 < Q5), we can formally\n",
    "test whether the outcome increases or decreases **across** the quantiles.\n",
    "\n",
    "The usual approach is:\n",
    "\n",
    "1. Assign each quantile group an **integer score** (1, 2, 3, 4, 5).  \n",
    "2. Fit a regression model including this score as a **continuous variable**.  \n",
    "\n",
    "For example:\n",
    "\n",
    "$$\n",
    "Y = \\alpha_0 + \\alpha_1 \\, \\text{quantile\\_score} + \\gamma^\\top Z,\n",
    "$$\n",
    "\n",
    "or in a logistic model:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p}{1 - p}\\right)\n",
    "  = \\alpha_0 + \\alpha_1 \\, \\text{quantile\\_score} + \\gamma^\\top Z.\n",
    "$$\n",
    "\n",
    "The coefficient $ \\alpha_1 $ provides a **test for linear trend** across the\n",
    "quantile categories (“P for trend”).\n",
    "\n",
    "This trend model does **not** replace the categorical model; both are usually presented:\n",
    "\n",
    "- the categorical model shows **non-linear patterns** across quantiles;  \n",
    "- the trend model tests whether there is an overall monotonic association.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bbaac",
   "metadata": {},
   "source": [
    "### 3.3 Why quantiles are used\n",
    "\n",
    "Quantile-based approaches are often used because they:\n",
    "\n",
    "- reduce the impact of extreme values,  \n",
    "- allow for simple categorical comparisons (“Q5 vs Q1”),  \n",
    "- can highlight non-linear relationships,  \n",
    "- match the conventions of major epidemiological cohorts (EPIC, NHANES, UK Biobank).\n",
    "\n",
    "However, quantile categorisation also loses some information compared with modelling\n",
    "the exposure as a continuous variable.\n",
    "\n",
    "In later workbooks we will compare:\n",
    "\n",
    "- continuous models,  \n",
    "- categorised models, and  \n",
    "- spline-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b4ac7",
   "metadata": {},
   "source": [
    "## 3.4 Example: continuous vs quintile-based exposure\n",
    "\n",
    "In this example we compare two approaches for modelling the association between\n",
    "fruit and vegetable intake (`fruit_veg_g_d`, grams per day) and BMI:\n",
    "\n",
    "1. A **continuous model**: BMI regressed directly on `fruit_veg_g_d`.  \n",
    "2. A **categorical model**: BMI regressed on **quintiles** of `fruit_veg_g_d`\n",
    "   (Q1 to Q5), using Q1 as the reference group.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Create quintiles of `fruit_veg_g_d` using `pandas.qcut`.  \n",
    "- Fit both models using ordinary least squares.  \n",
    "- Compare the estimated associations.  \n",
    "- Visualise the relationship using a scatter plot with a fitted regression line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create quintiles of fruit and vegetable intake.\n",
    "\n",
    "We:\n",
    "\n",
    "- Restrict to participants with non-missing BMI and fruit_veg_g_d.\n",
    "- Use pandas.qcut to divide fruit_veg_g_d into 5 equally sized groups (quintiles).\n",
    "- Create:\n",
    "  - an integer quintile code (1–5),\n",
    "  - a categorical label (\"Q1\"–\"Q5\") for use in regression output.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Subset to relevant variables and drop missing values\n",
    "df_q = df[[\"BMI\", \"fruit_veg_g_d\"]].dropna().copy()\n",
    "\n",
    "# Construct quintiles: qcut tries to place approximately equal numbers in each group\n",
    "df_q[\"fv_quintile\"] = pd.qcut(\n",
    "    df_q[\"fruit_veg_g_d\"],\n",
    "    q=5,\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "# Also create a categorical version with labels \"Q1\"–\"Q5\" for clearer output\n",
    "df_q[\"fv_quintile_cat\"] = pd.qcut(\n",
    "    df_q[\"fruit_veg_g_d\"],\n",
    "    q=5,\n",
    "    labels=[\"Q1 (lowest)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (highest)\"]\n",
    ")\n",
    "\n",
    "df_q[[\"fruit_veg_g_d\", \"fv_quintile\", \"fv_quintile_cat\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0816e",
   "metadata": {},
   "source": [
    "After we have created quintiles, we can now conduct regression analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bea78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare continuous and quintile-based linear regression models.\n",
    "\n",
    "Model 1 (continuous):\n",
    "    BMI ~ fruit_veg_g_d\n",
    "\n",
    "Model 2 (quintiles, reference = lowest quintile):\n",
    "    BMI ~ C(fv_quintile_cat)\n",
    "\n",
    "We display the full summaries and then a compact table for the quintile model.\n",
    "\"\"\"\n",
    "\n",
    "# Continuous model\n",
    "model_cont = smf.ols(\"BMI ~ fruit_veg_g_d\", data=df_q)\n",
    "result_cont = model_cont.fit()\n",
    "\n",
    "print(\"Continuous model: BMI ~ fruit_veg_g_d\")\n",
    "display(result_cont.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698b3df",
   "metadata": {},
   "source": [
    "#### Interpreting the continuous regression model\n",
    "\n",
    "This model examines the association between fruit and vegetable intake\n",
    "(`fruit_veg_g_d`, grams per day) and BMI as a *continuous* relationship.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- The coefficient for `fruit_veg_g_d` is **very small** (0.0005) and not statistically\n",
    "  significant (P = 0.121).  \n",
    "  This means that, within the FB2NEP cohort, higher fruit and vegetable intake is  \n",
    "  *not* strongly associated with differences in BMI when modelled linearly.\n",
    "\n",
    "- The confidence interval (–0.000, 0.001) includes zero, which is consistent with\n",
    "  the non-significant result.\n",
    "\n",
    "- The \\( R^2 \\) is effectively **zero**, indicating that fruit and vegetable intake\n",
    "  explains almost none of the variability in BMI.\n",
    "\n",
    "- The intercept (27.02 kg/m²) represents the **expected BMI** when intake is zero,\n",
    "  but is mainly a baseline reference point rather than a meaningful real-world value.\n",
    "\n",
    "This illustrates a common situation in nutritional epidemiology:  \n",
    "a weak or flat association when using a simple linear model on the continuous exposure.\n",
    "In the next section we explore an alternative approach using **quintiles**, which can\n",
    "sometimes reveal non-linear patterns that a straight-line model does not capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quintile model (fv_quintile_cat as a categorical predictor, Q1 is the reference)\n",
    "model_quint = smf.ols(\"BMI ~ C(fv_quintile_cat)\", data=df_q)\n",
    "result_quint = model_quint.fit()\n",
    "\n",
    "print(\"\\nQuintile model: BMI ~ C(fv_quintile_cat)\")\n",
    "display(result_quint.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a compact coefficient table for the quintile model\n",
    "coef_quint = result_quint.params\n",
    "ci_quint = result_quint.conf_int()\n",
    "table_quint = pd.DataFrame({\n",
    "    \"coef\": coef_quint,\n",
    "    \"CI_lower\": ci_quint[0],\n",
    "    \"CI_upper\": ci_quint[1],\n",
    "})\n",
    "\n",
    "print(\"\\nQuintile model: estimated differences in mean BMI vs reference (Q1)\")\n",
    "table_quint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347b0f9",
   "metadata": {},
   "source": [
    "#### Interpreting the quintile-based model\n",
    "\n",
    "Here we compare mean BMI across quintiles of fruit and vegetable intake, with the\n",
    "lowest intake group (Q1) as the reference category.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- All estimated differences (Q2–Q5 vs Q1) are **small** (around 0.1–0.17 BMI units).  \n",
    "  Such differences are unlikely to be meaningful in practice.\n",
    "\n",
    "- All confidence intervals **cross zero**, indicating **no clear evidence** that mean BMI\n",
    "  differs between fruit/vegetable intake groups.\n",
    "\n",
    "- The pattern across quintiles does not show a monotonic increase or decrease:\n",
    "  estimated BMI is slightly higher in Q2–Q5, but the values are similar and imprecise.\n",
    "\n",
    "- The intercept (27.0 kg/m²) is the estimated mean BMI in the reference group (Q1).\n",
    "\n",
    "Taken together, this quintile model gives the same substantive conclusion as the\n",
    "continuous model: in the FB2NEP cohort, fruit and vegetable intake is **not strongly\n",
    "or systematically associated** with BMI. Categorising the exposure does not reveal\n",
    "hidden non-linear patterns in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scatter plot of BMI vs fruit and vegetable intake, with quintiles.\n",
    "\n",
    "We:\n",
    "\n",
    "- Plot individual participants (BMI vs fruit_veg_g_d).\n",
    "- Overlay the fitted regression line from the continuous model.\n",
    "- Add vertical lines at the quintile cut-points of fruit_veg_g_d.\n",
    "- Add one point per quintile showing the mean BMI and mean intake.\n",
    "\n",
    "This links the continuous and quintile-based views of the same association.\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Scatter plot of observed data\n",
    "ax.scatter(\n",
    "    df_q[\"fruit_veg_g_d\"],\n",
    "    df_q[\"BMI\"],\n",
    "    alpha=0.3,\n",
    "    edgecolor=\"none\",\n",
    "    label=\"Observed BMI\"\n",
    ")\n",
    "\n",
    "# Fitted line from the continuous model\n",
    "fv_grid = np.linspace(\n",
    "    df_q[\"fruit_veg_g_d\"].min(),\n",
    "    df_q[\"fruit_veg_g_d\"].max(),\n",
    "    100\n",
    ")\n",
    "pred_df = pd.DataFrame({\"fruit_veg_g_d\": fv_grid})\n",
    "pred_df[\"BMI_hat\"] = result_cont.predict(pred_df)\n",
    "\n",
    "ax.plot(\n",
    "    pred_df[\"fruit_veg_g_d\"],\n",
    "    pred_df[\"BMI_hat\"],\n",
    "    linewidth=2,\n",
    "    label=\"Fitted line (continuous model)\"\n",
    ")\n",
    "\n",
    "# Quintile boundaries (excluding min/max)\n",
    "quintile_probs = [0.2, 0.4, 0.6, 0.8]\n",
    "quintile_cuts = df_q[\"fruit_veg_g_d\"].quantile(quintile_probs)\n",
    "\n",
    "for q_val in quintile_cuts:\n",
    "    ax.axvline(q_val, linestyle=\"--\", linewidth=1)\n",
    "    \n",
    "# Add text labels for quintile regions (optional, simple version)\n",
    "# for i, (p, v) in enumerate(quintile_cuts.items(), start=1):\n",
    "#     ax.text(v, ax.get_ylim()[1], f\"Q{i+1}\", ha=\"center\", va=\"top\")\n",
    "\n",
    "# Mean BMI and mean intake within each quintile\n",
    "quintile_means = (\n",
    "    df_q\n",
    "    .groupby(\"fv_quintile\")\n",
    "    .agg(\n",
    "        mean_fv=(\"fruit_veg_g_d\", \"mean\"),\n",
    "        mean_BMI=(\"BMI\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    quintile_means[\"mean_fv\"],\n",
    "    quintile_means[\"mean_BMI\"],\n",
    "    s=60,\n",
    "    marker=\"o\",\n",
    "    label=\"Quintile means\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Fruit and vegetable intake (g/day)\")\n",
    "ax.set_ylabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_title(\"BMI vs fruit and vegetable intake\\nwith quintiles and fitted line\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15621372",
   "metadata": {},
   "source": [
    "### 3.1 Strengths and limitations of quantile regression\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "- Provides a more complete description of the conditional distribution of Y.\n",
    "- Robust to outliers (especially when modelling the median).\n",
    "- Naturally accommodates heteroscedasticity (non-constant variance).\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Interpretation can be less intuitive than mean regression.\n",
    "- Confidence intervals and hypothesis tests are more complex.\n",
    "- More demanding computationally (although not an issue for this workbook).\n",
    "\n",
    "In nutritional epidemiology quantile regression can be particularly useful when:\n",
    "\n",
    "- The upper tail of a distribution is of special interest (for example, high sodium intake).\n",
    "- The outcome distribution is strongly skewed (for example, some biomarkers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3face152",
   "metadata": {},
   "source": [
    "## 4. Assumptions of regression models\n",
    "\n",
    "All models are simplifications of reality. To interpret results sensibly we need to be aware of their assumptions.\n",
    "\n",
    "Here we briefly review key assumptions for:\n",
    "\n",
    "- Linear regression.\n",
    "- Logistic regression.\n",
    "- Cox proportional hazards regression.\n",
    "\n",
    "Diagnostics and practical illustrations follow in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02930333",
   "metadata": {},
   "source": [
    "### 4.1 Linearity\n",
    "\n",
    "In a standard linear regression model we assume that the relationship between each continuous predictor and the outcome is **linear** (after any transformations we choose).\n",
    "\n",
    "If the true relationship is markedly non-linear, then:\n",
    "\n",
    "- The model may fit poorly.\n",
    "- Estimates of effect may be biased.\n",
    "- Residual plots may show systematic patterns.\n",
    "\n",
    "Later in this workbook we will introduce non-linear models (polynomials and splines) that relax this assumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225a919",
   "metadata": {},
   "source": [
    "### 4.2 Independence\n",
    "\n",
    "We usually assume that the residuals (errors) are **independent** between individuals.\n",
    "\n",
    "Violations of independence can occur when:\n",
    "\n",
    "- The same individual contributes multiple observations (for example, repeated measures).\n",
    "- Observations are clustered (for example, participants from the same household or clinic).\n",
    "\n",
    "More advanced methods, such as mixed models or cluster-robust standard errors, are used in those situations. Here we make the simplifying assumption of independence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c84a6",
   "metadata": {},
   "source": [
    "### 4.3 Homoscedasticity\n",
    "\n",
    "**Homoscedasticity** means that the variance of the residuals is constant across levels of the predictors.\n",
    "\n",
    "If residual variance increases or decreases with fitted values (heteroscedasticity), then:\n",
    "\n",
    "- Estimates of standard errors may be biased.\n",
    "- Confidence intervals and P-values may be unreliable.\n",
    "\n",
    "Residual-versus-fitted plots can be used to detect such patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa269417",
   "metadata": {},
   "source": [
    "### 4.4 Normality of residuals\n",
    "\n",
    "For linear regression, we often assume that the residuals are approximately **normally distributed**.\n",
    "\n",
    "- This assumption is not necessary for obtaining unbiased estimates of the mean.\n",
    "- It matters mainly for **inference** (confidence intervals and tests) in small samples.\n",
    "\n",
    "Normality can be explored with **Q–Q plots**, which compare the distribution of residuals with a theoretical normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f594460",
   "metadata": {},
   "source": [
    "### 4.5 Multicollinearity\n",
    "\n",
    "**Multicollinearity** arises when predictors are strongly correlated with one another.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "- Coefficients may be unstable.\n",
    "- Standard errors become large.\n",
    "- It can be difficult to disentangle separate effects.\n",
    "\n",
    "The **variance inflation factor (VIF)** is a commonly used diagnostic: large VIF values indicate problematic collinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216e4a3",
   "metadata": {},
   "source": [
    "### 4.6 Separation in logistic regression\n",
    "\n",
    "In logistic regression, **separation** occurs when a predictor (or combination of predictors) perfectly predicts the outcome (for example, all smokers have disease, all non-smokers are healthy).\n",
    "\n",
    "Consequences:\n",
    "\n",
    "- Maximum likelihood estimates may not exist or may be extremely large.\n",
    "- Standard logistic regression fails.\n",
    "\n",
    "In practice one may:\n",
    "\n",
    "- Collapse categories.\n",
    "- Use penalised logistic regression.\n",
    "- Rethink the model structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff44e6",
   "metadata": {},
   "source": [
    "### 4.7 Proportional hazards in Cox regression\n",
    "\n",
    "The Cox model assumes that hazard ratios are **constant over time** (proportional hazards). In other words:\n",
    "\n",
    "$$\n",
    "\\frac{h(t \\mid X = 1)}{h(t \\mid X = 0)} = \\text{constant in } t.\n",
    "$$\n",
    "\n",
    "Violations of this assumption can be detected using:\n",
    "\n",
    "- Plots of log(-log(survival)) curves.\n",
    "- Schoenfeld residuals and associated tests.\n",
    "\n",
    "If proportional hazards does not hold, options include:\n",
    "\n",
    "- Stratified Cox models.\n",
    "- Time-varying coefficients.\n",
    "- Alternative modelling approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706e120",
   "metadata": {},
   "source": [
    "## 5. Model diagnostics\n",
    "\n",
    "We now illustrate a few standard diagnostic tools for regression models.\n",
    "\n",
    "The aim is not to be exhaustive, but to provide a first hands-on experience with:\n",
    "\n",
    "- Residual plots.\n",
    "- Q–Q plots.\n",
    "- Influence diagnostics.\n",
    "- Goodness-of-fit metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81334e2b",
   "metadata": {},
   "source": [
    "### 5.1 Residual plots\n",
    "\n",
    "A **residual** is the difference between the observed outcome and the value predicted\n",
    "by the model:\n",
    "\n",
    "\\[\n",
    "e_i = y_i - \\hat{y}_i.\n",
    "\\]\n",
    "\n",
    "Residual plots help assess whether the assumptions of the linear regression model are\n",
    "reasonable. Two aspects are particularly important:\n",
    "\n",
    "- **Linearity**: the mean of the residuals should be close to zero across the range of\n",
    "  fitted values. A curved pattern suggests that the relationship between predictors and\n",
    "  outcome may not be adequately captured by a simple linear model.\n",
    "\n",
    "- **Homoscedasticity** (constant variance): the spread of residuals should be roughly\n",
    "  constant. A “funnel shape” (residuals widening or narrowing with fitted values)\n",
    "  suggests heteroscedasticity, which can affect standard errors and inference.\n",
    "\n",
    "A residual-versus-fitted plot is therefore a quick visual check of whether the linear\n",
    "model provides a reasonable description of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Residuals vs fitted values for a linear regression model.\n",
    "\n",
    "We:\n",
    "\n",
    "- Extract fitted values and residuals from an existing OLS model (`result_lin`).\n",
    "- Create a scatter plot of residuals vs fitted values.\n",
    "- Add a horizontal reference line at 0.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- If the linear model is appropriate and the variance is roughly constant,\n",
    "  the residuals should be scattered randomly around 0 with no clear pattern.\n",
    "- Curvature suggests non-linearity.\n",
    "- A “funnel” shape suggests heteroscedasticity (non-constant variance).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Replace `result_lin` with the name of your fitted OLS model if different.\n",
    "fitted = result_lin.fittedvalues\n",
    "residuals = result_lin.resid\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(fitted, residuals, alpha=0.3, edgecolor=\"none\")\n",
    "\n",
    "ax.axhline(0, linewidth=1)  # reference line at 0\n",
    "\n",
    "ax.set_xlabel(\"Fitted values\")\n",
    "ax.set_ylabel(\"Residuals\")\n",
    "ax.set_title(\"Residuals vs fitted values\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396669a9",
   "metadata": {},
   "source": [
    "### 5.2 Q–Q plots (normal probability plots)\n",
    "\n",
    "Linear regression also assumes that the **residuals are approximately normally distributed**.\n",
    "This is not required for the model to produce unbiased estimates, but it is relevant for\n",
    "the validity of confidence intervals and \\( p \\)-values.\n",
    "\n",
    "A **Q–Q plot** compares the distribution of the model residuals with a theoretical\n",
    "normal distribution:\n",
    "\n",
    "- Points lying close to the diagonal line indicate that the residuals are consistent with\n",
    "  normality.  \n",
    "- Systematic deviations (for example, curves in the tails) suggest heavier or lighter\n",
    "  tails than expected under a normal distribution.  \n",
    "- Severe deviations may indicate that transformations or alternative modelling\n",
    "  approaches could be helpful.\n",
    "\n",
    "Residual plots and Q–Q plots together give an initial, practical assessment of whether\n",
    "the assumptions underlying ordinary least squares regression are acceptable for the\n",
    "analysis at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normal Q–Q plot of residuals (using scipy).\n",
    "\n",
    "We:\n",
    "\n",
    "- Take the residuals from the linear model (`result_lin`).\n",
    "- Use scipy.stats.probplot to compare them with a theoretical normal distribution.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Points close to the diagonal line indicate residuals that are approximately normal.\n",
    "- Systematic deviations (especially in the tails) suggest departures from normality.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Residuals as a 1D NumPy array\n",
    "residuals = np.asarray(result_lin.resid)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "ax.set_title(\"Normal Q–Q plot of residuals\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a9965",
   "metadata": {},
   "source": [
    "#### Interpreting the Q–Q plot\n",
    "\n",
    "The Q–Q plot compares the ordered residuals from the linear model with the\n",
    "quantiles of a theoretical normal distribution. If the residuals were exactly\n",
    "normally distributed, all points would lie close to the diagonal reference line.\n",
    "\n",
    "In this plot we observe:\n",
    "\n",
    "- **Good overall alignment** with the diagonal:  \n",
    "  most points fall close to the straight line, indicating that the residuals are\n",
    "  broadly consistent with a normal distribution.\n",
    "\n",
    "- **Departures in the lower tail** (left side):  \n",
    "  the most negative residuals lie below the reference line, suggesting that the\n",
    "  lower tail of the residual distribution is slightly heavier (more extreme values)\n",
    "  than expected under a normal distribution.\n",
    "\n",
    "- **Mild curvature at both extremes**:  \n",
    "  slight deviations at the top end indicate similar behaviour in the upper tail.\n",
    "  These deviations are not dramatic, but they indicate that the residuals are not\n",
    "  perfectly normal.\n",
    "\n",
    "Overall, the pattern is typical for observational data with a moderately skewed\n",
    "exposure distribution: the residuals are close enough to normal for standard\n",
    "inference to be reasonable, but the tails show mild departures that are worth\n",
    "noting.\n",
    "\n",
    "In combination with the residual–versus–fitted plot, this Q–Q plot suggests that:\n",
    "\n",
    "- the linear model is **adequate** for illustrating the regression concepts,  \n",
    "- residual normality is not perfect but not severely violated, and  \n",
    "- any refinements (for example, transformations or robust regression) would be\n",
    "minor and not essential for introductory analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e40da8",
   "metadata": {},
   "source": [
    "## 6. Non-linear models\n",
    "\n",
    "The term \"linear regression\" refers to linearity in the **parameters** (β), not necessarily in the predictors themselves.\n",
    "\n",
    "Many epidemiological relationships are **non-linear**. For example:\n",
    "\n",
    "- Body mass index and mortality risk.\n",
    "- Age and blood pressure.\n",
    "- Sodium intake and blood pressure.\n",
    "\n",
    "To capture such patterns we can:\n",
    "\n",
    "- Add **polynomial terms** (for example, age²).\n",
    "- Use **splines**, which fit smooth curves made of polynomial segments.\n",
    "\n",
    "In this section we briefly introduce both approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7416718",
   "metadata": {},
   "source": [
    "### 6.1 Polynomial regression\n",
    "\n",
    "A simple extension of linear regression is to add powers of a predictor, for example:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\varepsilon.\n",
    "$$\n",
    "\n",
    "This is still a linear model in the parameters $ \\beta_0, \\beta_1, \\beta_2 $, but represents a **curved** relationship between X and Y.\n",
    "\n",
    "Caution is required:\n",
    "\n",
    "- High-order polynomials can behave very erratically at the boundaries of the data.\n",
    "- Interpretation of individual coefficients is difficult; the focus should be on the **overall shape** of the fitted curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Polynomial regression example: BMI on age and age².\n",
    "\n",
    "We:\n",
    "\n",
    "- Create a squared age term.\n",
    "- Fit a simple linear model: BMI ~ age.\n",
    "- Fit a polynomial model: BMI ~ age + age².\n",
    "- Plot both fitted curves together with the observed data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "df_poly = df[[\"age\", \"BMI\"]].dropna().copy()\n",
    "df_poly[\"age2\"] = df_poly[\"age\"] ** 2\n",
    "\n",
    "# Simple linear model\n",
    "model_lin = smf.ols(\"BMI ~ age\", data=df_poly).fit()\n",
    "\n",
    "# Polynomial model with age and age²\n",
    "model_poly = smf.ols(\"BMI ~ age + age2\", data=df_poly).fit()\n",
    "\n",
    "# Grid of ages for predictions\n",
    "age_grid = np.linspace(df_poly[\"age\"].min(), df_poly[\"age\"].max(), 100)\n",
    "pred_frame = pd.DataFrame({\n",
    "    \"age\": age_grid,\n",
    "    \"age2\": age_grid ** 2,\n",
    "})\n",
    "\n",
    "# Predictions from both models\n",
    "pred_lin = model_lin.predict(pred_frame[[\"age\"]])\n",
    "pred_poly = model_poly.predict(pred_frame)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(\n",
    "    df_poly[\"age\"],\n",
    "    df_poly[\"BMI\"],\n",
    "    alpha=0.2,\n",
    "    edgecolor=\"none\",\n",
    "    label=\"Observed BMI\"\n",
    ")\n",
    "\n",
    "ax.plot(age_grid, pred_lin, linewidth=2, label=\"Linear model\")\n",
    "ax.plot(age_grid, pred_poly, linewidth=2, linestyle=\"--\", label=\"Polynomial (age + age²)\")\n",
    "\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_title(\"Polynomial regression: BMI ~ age + age²\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c9622",
   "metadata": {},
   "source": [
    "### 6.2 Splines\n",
    "\n",
    "**Splines** provide a more flexible and stable approach to modelling non-linear relationships.\n",
    "\n",
    "Idea:\n",
    "\n",
    "- The range of X is divided into intervals by \"knots\".\n",
    "- Within each interval we fit low-degree polynomials.\n",
    "- The pieces are joined smoothly at the knots.\n",
    "\n",
    "A widely used choice in epidemiology is the **restricted cubic spline**, which behaves linearly beyond the outer knots and smoothly between knots.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Flexible yet stable.\n",
    "- Interpretation focuses on the **shape** of the curve.\n",
    "- Works well in large cohorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Restricted cubic spline example: BMI on age.\n",
    "\n",
    "We:\n",
    "\n",
    "- Use a restricted cubic spline (4 df) for age in a linear model.\n",
    "- Fit the model using the formula interface.\n",
    "- Plot the fitted spline curve together with the observed data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Subset with complete data\n",
    "df_spline = df[[\"age\", \"BMI\"]].dropna().copy()\n",
    "\n",
    "# Fit OLS model with a restricted cubic spline for age (4 df)\n",
    "model_spline = smf.ols(\"BMI ~ cr(age, df=4)\", data=df_spline).fit()\n",
    "\n",
    "# Prediction grid for age\n",
    "age_grid = np.linspace(df_spline[\"age\"].min(), df_spline[\"age\"].max(), 100)\n",
    "grid = pd.DataFrame({\"age\": age_grid})\n",
    "\n",
    "# Predicted BMI from the spline model\n",
    "pred_spline = model_spline.predict(grid)\n",
    "\n",
    "# Plot observed data and spline fit\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(\n",
    "    df_spline[\"age\"],\n",
    "    df_spline[\"BMI\"],\n",
    "    alpha=0.2,\n",
    "    edgecolor=\"none\",\n",
    "    label=\"Observed BMI\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    age_grid,\n",
    "    pred_spline,\n",
    "    linewidth=2,\n",
    "    label=\"Spline fit (df = 4)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Body mass index (kg/m²)\")\n",
    "ax.set_title(\"Restricted cubic spline: BMI ~ age\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d782e2b",
   "metadata": {},
   "source": [
    "### 6.3 Comparing models\n",
    "\n",
    "To decide whether a non-linear term is useful we can compare models using:\n",
    "\n",
    "- Visual inspection of fitted curves.\n",
    "- Goodness-of-fit measures such as the Akaike information criterion (AIC).\n",
    "- Likelihood ratio tests (for nested models).\n",
    "\n",
    "For example, we can compare:\n",
    "\n",
    "- A simple linear model (BMI ~ age).\n",
    "- A polynomial model (BMI ~ age + age²).\n",
    "- A spline model (BMI ~ spline(age)).\n",
    "\n",
    "Lower AIC values indicate better trade-off between fit and complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ac8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compare linear, polynomial, and spline models using AIC.\n",
    "\n",
    "This is a simple numeric comparison; interpretation still relies on graphs and subject-matter knowledge.\n",
    "\"\"\"\n",
    "\n",
    "aic_results = pd.DataFrame({\n",
    "    \"model\": [\"Linear\", \"Polynomial (age + age²)\", \"Spline (df = 4)\"],\n",
    "    \"AIC\": [model_lin.aic, model_poly.aic, model_spline.aic],\n",
    "})\n",
    "\n",
    "aic_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc1579",
   "metadata": {},
   "source": [
    "## 7. Interpreting effect estimates\n",
    "\n",
    "Different regression models produce different types of effect estimates. It is important to be clear about their meaning.\n",
    "\n",
    "- **β (beta) coefficients** in linear regression: expected difference in the mean outcome per unit change in the predictor.\n",
    "- **Odds ratios (OR)** in logistic regression: multiplicative change in the odds of the outcome.\n",
    "- **Risk ratios (RR)**: multiplicative change in risk (probability); not directly estimated in standard logistic models.\n",
    "- **Hazard ratios (HR)** in Cox regression: multiplicative change in the instantaneous hazard.\n",
    "\n",
    "In non-linear models (polynomials, splines, quantile regression) the interpretation usually focuses on the **shape of the curve** rather than individual coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract and summarise effect estimates from the fitted models.\n",
    "\n",
    "We:\n",
    "\n",
    "- Summarise β estimates from the linear model.\n",
    "- Present odds ratios from the logistic model.\n",
    "- Present hazard ratios from the Cox model.\n",
    "\n",
    "This illustrates how different models report different effect measures.\n",
    "\"\"\"\n",
    "\n",
    "# Linear regression coefficients (bmi ~ age + C(sex)).\n",
    "beta_lin = result_lin.params.to_frame(name=\"estimate\")\n",
    "beta_lin[\"model\"] = \"Linear (BMI)\"\n",
    "\n",
    "# Logistic regression odds ratios (hypertension ~ bmi + age + C(sex)).\n",
    "params_log = result_log.params\n",
    "conf_log = result_log.conf_int()\n",
    "or_table = pd.DataFrame({\n",
    "    \"estimate\": np.exp(params_log),\n",
    "    \"CI_lower\": np.exp(conf_log[0]),\n",
    "    \"CI_upper\": np.exp(conf_log[1]),\n",
    "})\n",
    "or_table[\"model\"] = \"Logistic (hypertension)\"\n",
    "\n",
    "# Cox model hazard ratios.\n",
    "cox_summary = cph.summary[[\"coef\", \"exp(coef)\", \"exp(coef) lower 95%\", \"exp(coef) upper 95%\"]].copy()\n",
    "cox_summary.rename(columns={\n",
    "    \"coef\": \"coef\",\n",
    "    \"exp(coef)\": \"HR\",\n",
    "    \"exp(coef) lower 95%\": \"CI_lower\",\n",
    "    \"exp(coef) upper 95%\": \"CI_upper\",\n",
    "}, inplace=True)\n",
    "cox_summary[\"model\"] = \"Cox (time to CVD)\"\n",
    "\n",
    "display(beta_lin)\n",
    "display(or_table)\n",
    "display(cox_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25703405",
   "metadata": {},
   "source": [
    "## 8. Estimation and inference (brief overview)\n",
    "\n",
    "Most regression models in this workbook are estimated using **maximum likelihood** (or, in the case of ordinary least squares, a closely related approach).\n",
    "\n",
    "The key ideas are:\n",
    "\n",
    "- Parameters are estimated by finding values that make the observed data \"most likely\" under the assumed model.\n",
    "- Standard errors quantify the typical variation of estimates across hypothetical repeated samples.\n",
    "- **Wald tests** and **likelihood ratio tests** are used to assess whether coefficients differ from zero.\n",
    "- **Confidence intervals** indicate a range of parameter values that are compatible with the observed data and the model assumptions.\n",
    "\n",
    "A full treatment of the underlying theory is beyond the scope of FB2NEP, but it is important to know that:\n",
    "\n",
    "- Estimates are subject to sampling variability.\n",
    "- P-values and confidence intervals rely on model assumptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Manual computation of a confidence interval for a linear regression coefficient.\n",
    "\n",
    "We illustrate the basic idea using the coefficient for 'age' in the linear model.\n",
    "\n",
    "The 95 % confidence interval is:\n",
    "\n",
    "    estimate ± 1.96 * standard_error\n",
    "\n",
    "under a normal approximation.\n",
    "\"\"\"\n",
    "\n",
    "# Extract estimate and standard error for 'age'.\n",
    "age_est = result_lin.params[\"age\"]\n",
    "age_se = result_lin.bse[\"age\"]\n",
    "\n",
    "ci_lower = age_est - 1.96 * age_se\n",
    "ci_upper = age_est + 1.96 * age_se\n",
    "\n",
    "print(\"Coefficient for age (linear model):\", f\"{age_est:.3f}\")\n",
    "print(\"Standard error:\", f\"{age_se:.3f}\")\n",
    "print(\"Approximate 95 % CI:\", f\"[{ci_lower:.3f}, {ci_upper:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac3741",
   "metadata": {},
   "source": [
    "## 9. Predictions from fitted models\n",
    "\n",
    "One of the most practical uses of regression models is to obtain **predicted values** for specified combinations of predictors.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Predicted mean BMI at age 65 years in women.\n",
    "- Predicted probability of hypertension at age 65 years for different BMI values.\n",
    "- Predicted survival curves for different risk profiles.\n",
    "\n",
    "In all cases it is important to remember:\n",
    "\n",
    "- Predictions depend on the **assumed model** and its **fitted parameters**.\n",
    "- Uncertainty in predictions can be quantified (for example, by confidence intervals or prediction intervals).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prediction from a linear model: BMI at age 65.\n",
    "\n",
    "We:\n",
    "\n",
    "- Create a small DataFrame with the desired predictor values.\n",
    "- Use the `predict` method of the fitted model.\n",
    "\n",
    "For simplicity we focus on a single sex.\n",
    "\"\"\"\n",
    "\n",
    "# Example: predict BMI at age 65 for women.\n",
    "new_data = pd.DataFrame({\n",
    "    \"age\": [65],\n",
    "    \"sex\": [\"F\"],\n",
    "})\n",
    "\n",
    "pred_bmi = result_lin.predict(new_data)\n",
    "\n",
    "print(\"Predicted mean BMI at age 65 (Female):\", float(pred_bmi.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06e645",
   "metadata": {},
   "source": [
    "## 10. Summary and further reading\n",
    "\n",
    "In this workbook you have:\n",
    "\n",
    "- Reviewed the basic idea of regression as modelling conditional expectations.\n",
    "- Fitted and interpreted linear, logistic, and Cox proportional hazards models.\n",
    "- Seen how quantile regression extends the idea to conditional quantiles.\n",
    "- Examined key model assumptions and basic diagnostics.\n",
    "- Introduced non-linear models using polynomial terms and splines.\n",
    "- Obtained predictions from fitted models.\n",
    "\n",
    "These tools are building blocks for more advanced topics in nutritional epidemiology:\n",
    "\n",
    "- Confounding and adjustment.\n",
    "- Causal diagrams (DAGs).\n",
    "- Mediation analysis.\n",
    "- Missing data and more complex model structures.\n",
    "\n",
    "These topics are developed further in **Workbook 7**.\n",
    "\n",
    "**Suggested further reading:**\n",
    "\n",
    "- Kleinbaum, D. G., and Klein, M. *Logistic Regression: A Self-Learning Text.*\n",
    "- Harrell, F. E. *Regression Modelling Strategies.*\n",
    "- Rothman, K. J., Greenland, S., and Lash, T. L. *Modern Epidemiology.*\n",
    "- Koenker, R. *Quantile Regression.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
