{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro_data_handling",
      "metadata": {},
      "source": [
        "# Data Handling and Basic Analysis (FB2NRP)\n",
        "*Version 0.0.3*\n",
        "\n",
        "This workbook introduces the foundations of **data handling and basic analysis**, using a small **synthetic RCT dataset** that mimics some nutrition trials:\n",
        "\n",
        "- Blood pressure change after different amounts of coffee  \n",
        "- Blood glucose after different cereals  \n",
        "- Appetite VAS after different test foods  \n",
        "\n",
        "The data are **simulated** and include **age** and **sex**.  \n",
        "The dataset is made available as a pandas DataFrame called `df` by the **bootstrap cell above**.\n",
        "\n",
        "The workbook is designed to work both as:\n",
        "\n",
        "- a **lecture resource** (with explanations and examples), and  \n",
        "- a **practical notebook** (with commented Python code cells).\n",
        "\n",
        "By the end of the workbook you should be able to:\n",
        "\n",
        "- Distinguish between **categorical**, **ordinal**, and **continuous** variables  \n",
        "- Explore a dataset with `df.info()` and `df.describe()`  \n",
        "- Compute and report **mean**, **SD**, **median**, **IQR** for continuous data  \n",
        "- Understand common **distributions** (normal, log-normal, *t*, Poisson) at a light-touch level  \n",
        "- Explore **distributions** and use Q–Q plots to assess normality  \n",
        "- Create **contingency tables** for categorical data  \n",
        "- Describe data appropriately for publication (e.g. a simple **Table 1**)  \n",
        "- Understand the basics of **NHST** (H0 vs H1), **p-values**, and **95% CIs**  \n",
        "- See why p = 0.05 is not a magical threshold (we will use **α = 0.0314**)  \n",
        "- Compare two and more groups using **parametric** and **non-parametric** tests  \n",
        "- Remember that **statistics are tools, not an oracle**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flow_of_analysis",
      "metadata": {},
      "source": [
        "## 0. From raw data to analysis: the basic flow\n",
        "\n",
        "In almost every quantitative study, the **flow of analysis** is:\n",
        "\n",
        "1. **View the data**  \n",
        "   - Load the dataset into your software (here: pandas DataFrame `df`).  \n",
        "   - Look at a few rows (`df.head()`).  \n",
        "   - Check variable names, data types, and obvious issues (`df.info()`).\n",
        "\n",
        "2. **Clean the data**  \n",
        "   - Handle missing values (decide when to impute, when to drop).  \n",
        "   - Detect obviously impossible values (e.g. age = −5, VAS > 100).  \n",
        "   - Fix coding problems (e.g. \"Male\" vs \"M\" vs \"m\").\n",
        "\n",
        "3. **Standardise the data**  \n",
        "   - Ensure variables use **consistent units** (e.g. all blood pressure in mmHg, all glucose in mmol/L).  \n",
        "   - Recode categories in a consistent way (e.g. `F`/`M`, or 0/1 with clear labels).\n",
        "\n",
        "4. **Analyse the data**  \n",
        "   - Start with **descriptive statistics** (means, medians, counts, percentages).  \n",
        "   - Present a clear **Table 1** of baseline characteristics.  \n",
        "   - Move to **statistical inference** (p-values, confidence intervals, models) only once you understand the data.\n",
        "\n",
        "In this workbook we follow the same structure: first **understand and describe**, then **compare and infer**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bootstrap_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FB2NRP bootstrap cell (works both locally and in Colab)\n",
        "#\n",
        "# What this cell does:\n",
        "# - Locally: expects you to open the notebook from *inside*\n",
        "#   the fb2nrp-datahandling repository (e.g. repo/notebooks).\n",
        "#   It walks up the directory tree to find scripts/bootstrap.py.\n",
        "# - In Colab: if the repo is not found, it clones it from GitHub\n",
        "#   into /content/fb2nrp-datahandling.\n",
        "# - Loads and runs scripts/bootstrap.py.\n",
        "# - Generates a synthetic dataset and makes it available as `df`.\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import subprocess\n",
        "import importlib.util\n",
        "\n",
        "REPO_URL = \"https://github.com/ggkuhnle/fb2nrp-datahandling.git\"\n",
        "REPO_DIR = \"fb2nrp-datahandling\"\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    \"\"\"Return True if running inside Google Colab.\"\"\"\n",
        "    try:\n",
        "        import google.colab  # type: ignore  # noqa: F401\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Make sure the process cwd is valid\n",
        "try:\n",
        "    cwd = pathlib.Path.cwd()\n",
        "except FileNotFoundError:\n",
        "    raise RuntimeError(\n",
        "        \"Current working directory no longer exists.\\n\"\n",
        "        \"Please restart the kernel from inside the fb2nrp-datahandling repository \"\n",
        "        \"(e.g. open the notebook from repo/notebooks and try again).\"\n",
        "    )\n",
        "\n",
        "# Try to find the repo root by walking up the directory tree\n",
        "repo_root = None\n",
        "for parent in [cwd] + list(cwd.parents):\n",
        "    if (parent / \"scripts\" / \"bootstrap.py\").is_file():\n",
        "        repo_root = parent\n",
        "        break\n",
        "\n",
        "if repo_root is not None:\n",
        "    # We are somewhere inside an existing clone (local or Colab)\n",
        "    os.chdir(repo_root)\n",
        "    repo_root = pathlib.Path.cwd()\n",
        "    print(f\"Repository root detected at: {repo_root}\")\n",
        "else:\n",
        "    # Repo not found by walking up\n",
        "    if in_colab():\n",
        "        # In Colab: clone into /content/fb2nrp-datahandling\n",
        "        base_dir = pathlib.Path(\"/content\")\n",
        "        os.chdir(base_dir)\n",
        "        repo_root = base_dir / REPO_DIR\n",
        "        if not repo_root.is_dir():\n",
        "            print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n",
        "            subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n",
        "        else:\n",
        "            print(f\"Using existing repository at {repo_root}\")\n",
        "        os.chdir(repo_root)\n",
        "        repo_root = pathlib.Path.cwd()\n",
        "        print(f\"Repository root set to: {repo_root}\")\n",
        "    else:\n",
        "        # Local but not inside the repo: fail with a clear message\n",
        "        raise RuntimeError(\n",
        "            \"Could not find fb2nrp-datahandling repository root.\\n\"\n",
        "            \"Please make sure you open this notebook from inside the \"\n",
        "            \"`fb2nrp-datahandling` repository (e.g. repo/notebooks) and \"\n",
        "            \"then re-run this cell.\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Load scripts/bootstrap.py as a module and call init()\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "bootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n",
        "\n",
        "if not bootstrap_path.is_file():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find {bootstrap_path}. \"\n",
        "        \"Please check that the fb2nrp-datahandling repository structure is intact.\"\n",
        "    )\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"fb2nrp_bootstrap\", bootstrap_path)\n",
        "bootstrap = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"fb2nrp_bootstrap\"] = bootstrap\n",
        "spec.loader.exec_module(bootstrap)\n",
        "\n",
        "# CTX will contain paths and settings defined in bootstrap.py\n",
        "CTX = bootstrap.init()\n",
        "\n",
        "for name in [\"REPO_NAME\", \"REPO_URL\"]:\n",
        "    if hasattr(bootstrap, name):\n",
        "        globals()[name] = getattr(bootstrap, name)\n",
        "\n",
        "print(\"Bootstrap completed successfully.\")\n",
        "print(\"The context object is available as `CTX`.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Setup: scientific Python libraries and plotting style\n",
        "#\n",
        "# Assumes the bootstrap cell above has already created:\n",
        "#   - CTX : context object with paths and settings\n",
        "# ============================================================\n",
        "\n",
        "# Data handling and numerical computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Statistical tests and distributions\n",
        "import scipy.stats as st\n",
        "\n",
        "# Display options (optional but helpful)\n",
        "pd.set_option(\"display.max_rows\", 20)\n",
        "pd.set_option(\"display.max_columns\", 20)\n",
        "\n",
        "# Plot style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
        "\n",
        "print(\"Libraries loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simulate_data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Generate the synthetic dataset for this workbook\n",
        "# ============================================================\n",
        "\n",
        "# The helper function simulate_practical_data() returns a\n",
        "# small DataFrame that mimics some nutrition RCT practicals.\n",
        "from scripts.helpers import simulate_practical_data, VARIABLES\n",
        "\n",
        "# Use a fixed seed for reproducibility\n",
        "df = simulate_practical_data(seed=11088)\n",
        "\n",
        "print(f\"Dataset loaded with {len(df)} rows and {df.shape[1]} columns.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "variables_types_text",
      "metadata": {},
      "source": [
        "## 1. Study variables and data types\n",
        "\n",
        "Before running any analysis, we need to understand **what kind of variables** we have.  \n",
        "Different types require different summaries and different statistical tests.\n",
        "\n",
        "### 1.1 Overview of variable types\n",
        "\n",
        "| Variable type | What it means | Examples | How to summarise | Appropriate analyses |\n",
        "|--------------|---------------|----------|------------------|----------------------|\n",
        "| **Categorical (nominal)** | Distinct labels with **no natural order**. Values are names only. | Sex (F/M), coffee arm (low/medium/high), cereal arm, favourite animal (hippo optional). | Counts and percentages. | Chi-squared tests, Fisher’s exact test, logistic/multinomial regression. |\n",
        "| **Ordinal** | Categories **with a natural order**, but **unequal spacing** between levels. | Likert scale 1–5, hunger rating (low/medium/high), symptom severity. | Counts/percentages; sometimes median (IQR) of coded scores with justification. | Mann–Whitney test, Kruskal–Wallis, ordinal logistic regression. |\n",
        "| **Continuous (or approx. continuous)** | Numerical values where **differences and averages are meaningful**. Often many possible values. | Age (years), BP change (mmHg), glucose, VAS (0–100, often treated as continuous). | Mean ± SD (if symmetric), or median (IQR) if skewed. | t-tests, ANOVA, correlation, linear regression; non-parametric alternatives if needed. |\n",
        "\n",
        "A few reminders:\n",
        "\n",
        "- Coding categorical data as numbers **does not** turn them into continuous variables.  \n",
        "- Ordinal scales can sometimes be treated as continuous **only** if many levels and behaved distributions make it reasonable.  \n",
        "- VAS scores (0–100) occupy a grey zone: technically ordinal, often acceptable to treat as continuous in nutrition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "variables_in_dataset",
      "metadata": {},
      "source": [
        "### 1.2 Variables in our synthetic dataset\n",
        "\n",
        "Our synthetic dataset `df` contains (one row per participant):\n",
        "\n",
        "| Variable | Type | Description |\n",
        "|----------|------|-------------|\n",
        "| `sex` | Categorical (nominal) | Participant sex (F/M) |\n",
        "| `age` | Continuous | Age in years |\n",
        "| `coffee_arm` | Categorical (nominal) | Intervention: low / medium / high coffee |\n",
        "| `cereal_arm` | Categorical (nominal) | Cereal: bran / cornflakes / muesli |\n",
        "| `food_arm` | Categorical (nominal) | Test food: apple / biscuit / yoghurt |\n",
        "| `bp_change` | Continuous | Change in blood pressure (mmHg) |\n",
        "| `glucose` | Continuous | Postprandial blood glucose (arbitrary units) |\n",
        "| `appetite_vas` | Continuous / ordinal | VAS 0–100; treated here as approx. continuous |\n",
        "\n",
        "For completeness, we can also display the helper metadata `VARIABLES` that describes each column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "variables_table",
      "metadata": {},
      "outputs": [],
      "source": [
        "VARIABLES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "first_look_text",
      "metadata": {},
      "source": [
        "## 2. First look at the dataset\n",
        "\n",
        "We start with a **quick overview** of `df`:\n",
        "\n",
        "- `df.head()` shows the first few rows (useful to spot obvious coding issues).  \n",
        "- `df.info()` summarises variables, data types, and missing values.\n",
        "\n",
        "This is the **\"view\"** step of the analysis flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "head_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First few rows of the dataset\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "info_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall structure of the DataFrame (types, missingness)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "missing_values_text",
      "metadata": {},
      "source": [
        "### 2.1 Missing values and impossible values\n",
        "\n",
        "We should also check for **missing values** and obviously **impossible values** (e.g. negative age, VAS > 100, glucose = 0 in a living participant).\n",
        "\n",
        "Our simulator does not generate missing or impossible values, but in real data these checks are essential and sometimes the longest part of the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "missing_values_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count of missing values per variable\n",
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continuous_intro",
      "metadata": {},
      "source": [
        "## 3. Describing continuous variables\n",
        "\n",
        "For continuous variables (age, BP change, glucose, VAS) we want to describe:\n",
        "\n",
        "1. **Where the values tend to lie** (central tendency).  \n",
        "2. **How much they vary** (dispersion).  \n",
        "3. **What the distribution looks like** (shape).\n",
        "\n",
        "In this section we first look at **distributions**, then define **central tendency and dispersion**, and finally compute appropriate **summary statistics**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "distributions_overview",
      "metadata": {},
      "source": [
        "### 3.1 Distributions and how to look at them\n",
        "\n",
        "Many statistical methods assume that variables follow certain **distributions**.  \n",
        "For this workbook, four are particularly useful:\n",
        "\n",
        "- **Normal distribution** (bell-shaped, symmetric).  \n",
        "- **Log-normal distribution** (skewed; log of the variable is normal).  \n",
        "- **t-distribution** (like normal, but with heavier tails; used in t-tests).  \n",
        "- **Poisson distribution** (for **counts**, especially of rare events).\n",
        "\n",
        "We do not need the formulas; we just need to recognise their shapes and know when they are plausible models.\n",
        "\n",
        "We usually look at distributions in two ways:\n",
        "\n",
        "- **Histograms/density plots**: show the shape of the data.  \n",
        "- **Q–Q plots (Quantile–Quantile plots)**: compare the quantiles of the data to those of a reference distribution (often normal).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "normal_lognormal_text",
      "metadata": {},
      "source": [
        "#### 3.1.1 Normal and log-normal distributions\n",
        "\n",
        "- A variable is **approximately normal** when its histogram is symmetric and bell-shaped.  \n",
        "  - Example: adult height, measurement error, often blood pressure in reasonably homogeneous groups.\n",
        "- A variable is **log-normal** when its **logarithm** is approximately normal.  \n",
        "  - Example: many biomarkers and concentrations, where values are strictly positive and skewed to the right.\n",
        "\n",
        "Below we simulate data from a normal distribution to illustrate the shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "normal_example_plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated example: normal distribution\n",
        "rng = np.random.default_rng(11088)\n",
        "normal_sample = rng.normal(loc=0, scale=1, size=2000)\n",
        "\n",
        "sns.histplot(normal_sample, kde=True)\n",
        "plt.title(\"Simulated normal distribution (mean = 0, SD = 1)\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lognormal_text",
      "metadata": {},
      "source": [
        "Below we simulate data from a log-normal distribution. Notice the **right-skewed** shape: many observations near the lower end, with a long tail of higher values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lognormal_example_plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulated example: log-normal distribution\n",
        "lognormal_sample = rng.lognormal(mean=0, sigma=0.6, size=2000)\n",
        "\n",
        "sns.histplot(lognormal_sample, kde=True)\n",
        "plt.title(\"Simulated log-normal distribution\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t_dist_text",
      "metadata": {},
      "source": [
        "#### 3.1.2 t-distribution\n",
        "\n",
        "The **t-distribution** appears when we:\n",
        "\n",
        "- estimate means from **small samples**, and  \n",
        "- use the **sample SD** instead of the true population SD.\n",
        "\n",
        "It looks similar to the normal distribution but has **heavier tails**, especially with **small degrees of freedom (df)**.  \n",
        "This matters for **t-tests** and confidence intervals based on small samples.\n",
        "\n",
        "Below we plot t-distributions with different degrees of freedom and compare them to the standard normal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t_distribution_plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# t-distributions vs standard normal\n",
        "x = np.linspace(-4, 4, 400)\n",
        "pdf_normal = st.norm.pdf(x, loc=0, scale=1)\n",
        "pdf_t3 = st.t.pdf(x, df=3)\n",
        "pdf_t10 = st.t.pdf(x, df=10)\n",
        "pdf_t30 = st.t.pdf(x, df=30)\n",
        "\n",
        "plt.plot(x, pdf_normal, label=\"Normal\")\n",
        "plt.plot(x, pdf_t3, linestyle=\"--\", label=\"t, df=3\")\n",
        "plt.plot(x, pdf_t10, linestyle=\":\", label=\"t, df=10\")\n",
        "plt.plot(x, pdf_t30, linestyle=\"-.\", label=\"t, df=30\")\n",
        "plt.title(\"Normal vs t-distributions\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poisson_text",
      "metadata": {},
      "source": [
        "#### 3.1.3 Poisson distribution\n",
        "\n",
        "The **Poisson distribution** is a model for **counts** of events in a fixed time or space, especially when events are:\n",
        "\n",
        "- **independent**, and  \n",
        "- individually **rare**.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Number of adverse events per participant in a trial.  \n",
        "- Number of emergency admissions per day in a small hospital.  \n",
        "- Number of typing errors per page in a report (for some of us).\n",
        "\n",
        "It has a single parameter **λ (lambda)**, which is both the **mean** and the **variance** of the distribution.\n",
        "\n",
        "Below we show the probabilities for a Poisson distribution with λ = 2.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "poisson_distribution_plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Poisson distribution example (lambda = 2.5)\n",
        "lam = 2.5\n",
        "k_values = np.arange(0, 11)  # 0 to 10 events\n",
        "pmf = st.poisson.pmf(k_values, mu=lam)\n",
        "\n",
        "plt.stem(k_values, pmf, use_line_collection=True)\n",
        "plt.title(\"Poisson(λ = 2.5) distribution\")\n",
        "plt.xlabel(\"Number of events (k)\")\n",
        "plt.ylabel(\"Probability P(X = k)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "our_data_dist_text",
      "metadata": {},
      "source": [
        "#### 3.1.4 Looking at our data: BP change\n",
        "\n",
        "Now we return to our dataset and look at the distribution of **blood pressure change** (`bp_change`).\n",
        "\n",
        "We use:\n",
        "\n",
        "- a **histogram** with a smooth density estimate, and  \n",
        "- a **Q–Q plot** against the normal distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hist_bp_change",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogram and density for blood pressure change\n",
        "sns.histplot(df[\"bp_change\"], kde=True)\n",
        "plt.title(\"Distribution of BP change\")\n",
        "plt.xlabel(\"BP change (mmHg)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qqplot_explanation",
      "metadata": {},
      "source": [
        "A **Q–Q plot** compares the quantiles of our data to those of a perfect normal distribution.\n",
        "\n",
        "- If the points lie roughly on a straight line, the data are not wildly inconsistent with normality.  \n",
        "- Systematic curves (S-shape, heavy tails) suggest deviations such as skewness or outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qqplot_bp_change",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q–Q plot to assess normality of BP change\n",
        "st.probplot(df[\"bp_change\"], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q–Q plot of BP change\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "central_tendency_dispersion",
      "metadata": {},
      "source": [
        "### 3.2 Central tendency and dispersion\n",
        "\n",
        "Once we have a sense of the **shape** of a distribution, we can talk about:\n",
        "\n",
        "- **Central tendency** – where the values tend to lie.  \n",
        "- **Dispersion (spread)** – how much they vary around the centre.\n",
        "\n",
        "Common choices:\n",
        "\n",
        "- **Mean** (average) and **standard deviation (SD)**  \n",
        "  - Most useful when the distribution is not too skewed.  \n",
        "- **Median** and **interquartile range (IQR)**  \n",
        "  - More robust to skewed distributions and outliers.\n",
        "\n",
        "Choice of summary should be guided by the **distributional shape**, not by habit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mean_sd_median_iqr",
      "metadata": {},
      "source": [
        "### 3.3 Mean, SD, median, and IQR\n",
        "\n",
        "Definitions:\n",
        "\n",
        "- **Mean**: add up all observations and divide by the number of observations.  \n",
        "- **Standard deviation (SD)**: describes how far, on average, observations are from the mean.  \n",
        "- **Median**: the middle value when the data are ordered (50% below, 50% above).  \n",
        "- **Interquartile range (IQR)**: difference between the 75th percentile (Q3) and 25th percentile (Q1).\n",
        "\n",
        "Rules of thumb:\n",
        "\n",
        "- If the distribution is **roughly symmetric** → report *mean ± SD*.  \n",
        "- If the distribution is **clearly skewed** → report *median (IQR)*.\n",
        "\n",
        "In practice, many papers report both, at least in supplementary material.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sample_population_sem",
      "metadata": {},
      "source": [
        "### 3.4 Sample vs population and the standard error of the mean (SEM)\n",
        "\n",
        "In practice we almost never observe the **entire population**. We observe a **sample** and use it to say something about the population.\n",
        "\n",
        "- **Population mean (μ)**: the true average in the entire population (usually unknown).  \n",
        "- **Sample mean (x̄)**: the average in our sample.\n",
        "\n",
        "If we repeatedly took new samples of the same size and calculated the mean each time, those sample means would vary.\n",
        "\n",
        "- The **standard deviation (SD)** describes variability **between individuals**.  \n",
        "- The **standard error of the mean (SEM)** describes variability **between sample means**.\n",
        "\n",
        "For a sample of size *n*, and sample SD = *s*, a common estimate is:\n",
        "\n",
        "$$\\text{SEM} \\approx \\frac{s}{\\sqrt{n}}.$$\n",
        "\n",
        "SEM is mainly used when constructing **confidence intervals** and performing **hypothesis tests**, not for describing raw data in a Table 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "describe_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic summary statistics for numeric variables\n",
        "# (mean, SD, min, max, quartiles)\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "descriptive_cont_text",
      "metadata": {},
      "source": [
        "### 3.5 Descriptive statistics for key continuous outcomes\n",
        "\n",
        "Let us compute both mean/SD and median/IQR for the three main continuous outcomes:\n",
        "\n",
        "- `bp_change`  \n",
        "- `glucose`  \n",
        "- `appetite_vas`\n",
        "\n",
        "This table is close to what you might include in a **results section** or a **Table 1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descriptive_cont_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "cont_vars = [\"bp_change\", \"glucose\", \"appetite_vas\"]\n",
        "rows = []\n",
        "\n",
        "for var in cont_vars:\n",
        "    series = df[var].dropna()\n",
        "    mean = series.mean()\n",
        "    sd = series.std()\n",
        "    median = series.median()\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    rows.append({\n",
        "        \"variable\": var,\n",
        "        \"mean\": mean,\n",
        "        \"sd\": sd,\n",
        "        \"median\": median,\n",
        "        \"q1\": q1,\n",
        "        \"q3\": q3,\n",
        "        \"iqr\": iqr\n",
        "    })\n",
        "\n",
        "summary_cont = pd.DataFrame(rows)\n",
        "summary_cont\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "categorical_descriptive_text",
      "metadata": {},
      "source": [
        "## 4. Categorical variables: nominal vs ordinal\n",
        "\n",
        "For **categorical variables** we usually report **counts and percentages**.\n",
        "\n",
        "Two subclasses are important:\n",
        "\n",
        "- **Nominal** (unordered)  \n",
        "  - Examples: sex (F/M), cereal arm, country, favourite snack.  \n",
        "  - The labels have no inherent ranking.\n",
        "\n",
        "- **Ordinal** (ordered)  \n",
        "  - Examples: Likert scales, symptom severity (mild/moderate/severe).  \n",
        "  - There is a natural order, but the distance between categories is not known.\n",
        "\n",
        "### 4.1 Why not just convert categories into numbers and treat them as continuous?\n",
        "\n",
        "It is tempting to code categories as numbers (e.g. *apple* = 1, *biscuit* = 2, *yoghurt* = 3) and then compute a mean.\n",
        "\n",
        "This is **usually a bad idea** because:\n",
        "\n",
        "- The numerical codes are **arbitrary labels**, not real quantities.  \n",
        "- The differences between codes (2 − 1 vs 3 − 2) have **no scientific meaning**.  \n",
        "- Treating them as continuous in a t-test or regression can give misleading results.\n",
        "\n",
        "Instead, we summarise categorical variables using **counts**, **percentages**, and **contingency tables**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categorical_value_counts",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of participants by coffee arm (counts)\n",
        "df[\"coffee_arm\"].value_counts().to_frame(name=\"count\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "categorical_crosstab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contingency table: sex by coffee arm\n",
        "pd.crosstab(df[\"sex\"], df[\"coffee_arm\"], margins=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "table1_intro",
      "metadata": {},
      "source": [
        "## 5. Data description as \"Table 1\"\n",
        "\n",
        "Most clinical and nutrition papers include a **Table 1** that describes the **baseline characteristics** of the study sample.\n",
        "\n",
        "Typical elements:\n",
        "\n",
        "- A column for **\"All participants\"**.  \n",
        "- Additional columns for **treatment arms** (or exposure groups).  \n",
        "- Rows for key variables: age, sex, BMI, main outcomes, etc.  \n",
        "- Continuous variables shown as *mean ± SD* or *median (IQR)*.  \n",
        "- Categorical variables shown as *n (%)*.\n",
        "\n",
        "Below we build a **very simple Table 1** describing age and sex by coffee arm.  \n",
        "This is for illustration only – in real work you would usually format the table more nicely (e.g. for LaTeX or Word).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "table1_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Table 1: age and sex by coffee arm\n",
        "\n",
        "group_var = \"coffee_arm\"\n",
        "continuous_vars = [\"age\"]\n",
        "categorical_vars = [\"sex\"]\n",
        "\n",
        "arms = df[group_var].unique()\n",
        "arms.sort()\n",
        "\n",
        "table1_rows = []\n",
        "\n",
        "# Continuous variables: report mean ± SD\n",
        "for var in continuous_vars:\n",
        "    row = {\"variable\": var, \"type\": \"continuous\"}\n",
        "    for arm in arms:\n",
        "        sub = df[df[group_var] == arm][var].dropna()\n",
        "        m = sub.mean()\n",
        "        s = sub.std()\n",
        "        row[arm] = f\"{m:.1f} ± {s:.1f}\"\n",
        "    table1_rows.append(row)\n",
        "\n",
        "# Categorical variables: report n (%)\n",
        "for var in categorical_vars:\n",
        "    levels = df[var].dropna().unique()\n",
        "    levels.sort()\n",
        "    for level in levels:\n",
        "        row = {\n",
        "            \"variable\": f\"{var} = {level}\",\n",
        "            \"type\": \"categorical\"\n",
        "        }\n",
        "        for arm in arms:\n",
        "            sub = df[df[group_var] == arm]\n",
        "            n = (sub[var] == level).sum()\n",
        "            total = len(sub)\n",
        "            perc = 100 * n / total if total > 0 else np.nan\n",
        "            row[arm] = f\"{n} ({perc:.1f}%)\"\n",
        "        table1_rows.append(row)\n",
        "\n",
        "table1 = pd.DataFrame(table1_rows)\n",
        "table1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "publication_description_text",
      "metadata": {},
      "source": [
        "### 5.1 Example text for methods/results\n",
        "\n",
        "Using a table like the one above, you might write in a paper:\n",
        "\n",
        "- *\"Participants had a mean age of 22.1 ± 3.1 years.\"*  \n",
        "- *\"Overall, 40% of participants were male (n = 72/180).\"*  \n",
        "- *\"Baseline blood pressure did not differ meaningfully between coffee arms.\"*\n",
        "\n",
        "The exact wording depends on the study, but the principle is always:\n",
        "\n",
        "- Describe **who** was studied.  \n",
        "- Use **appropriate summaries** for each variable type.  \n",
        "- Make it possible for the reader to judge how well the sample represents the population of interest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nhst_intro_text",
      "metadata": {},
      "source": [
        "## 6. Statistical inference and NHST\n",
        "\n",
        "So far we have described the **sample**. Statistical inference is about what we can reasonably say about the **underlying population**.\n",
        "\n",
        "In classical **null hypothesis significance testing (NHST)** we:\n",
        "\n",
        "1. Formulate a **null hypothesis (H0)**, usually \"no difference\" or \"no effect\".  \n",
        "2. Formulate an **alternative hypothesis (H1)**, e.g. \"there is a difference\".  \n",
        "3. Choose a test statistic (e.g. a t-statistic) and compute it from the data.  \n",
        "4. Compute a **p-value**: the probability (under H0) of observing a result *at least as extreme* as the one we saw.  \n",
        "5. Compare the p-value to a threshold **α** (alpha) to decide whether the result is *compatible* with H0.\n",
        "\n",
        "Example in this workbook:\n",
        "\n",
        "- H0: mean BP change is the same in **low** and **high** coffee arms.  \n",
        "- H1: mean BP change is different in the two arms.\n",
        "\n",
        "We never prove H0 or H1; we simply assess how **compatible** the data are with H0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvalue_text",
      "metadata": {},
      "source": [
        "### 6.1 p-values and why 0.05 is not magical\n",
        "\n",
        "- A **p-value** is *not* the probability that H0 is true.  \n",
        "- It is the probability of the observed data (or more extreme) *if H0 were true*.\n",
        "\n",
        "Common misunderstandings:\n",
        "\n",
        "- p = 0.04 does **not** mean there is a 96% chance that the effect is real.  \n",
        "- p = 0.06 does **not** mean \"no effect\".\n",
        "\n",
        "The widely used threshold **α = 0.05** is **just a convention**:\n",
        "\n",
        "- 0.049 and 0.051 are essentially the same in terms of evidence.  \n",
        "- Treating them as \"significant\" vs \"non-significant\" can be misleading.  \n",
        "- In reality, we should look at **effect size**, **uncertainty**, and **context**.\n",
        "\n",
        "In this workbook we deliberately use an unusual threshold **α = 0.0314** to emphasise that the choice of α is arbitrary and should be justified, not blindly copied.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ci_explanation_text",
      "metadata": {},
      "source": [
        "### 6.2 Confidence intervals (CIs)\n",
        "\n",
        "A **95% confidence interval (CI)** for a parameter (e.g. difference in means) is constructed such that, in repeated samples, **95% of such intervals would contain the true parameter**.\n",
        "\n",
        "In practice:\n",
        "\n",
        "- If a 95% CI for a difference **excludes 0**, the corresponding two-sided test at α = 0.05 is \"statistically significant\".  \n",
        "- The CI gives information about **precision** (width of the interval) and **effect size** (where the interval lies).  \n",
        "- CIs are usually more informative than a bare p-value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "example_ci_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: difference in mean BP change between low and high coffee arms\n",
        "\n",
        "bp_low = df[df[\"coffee_arm\"] == \"low\"][\"bp_change\"].dropna()\n",
        "bp_high = df[df[\"coffee_arm\"] == \"high\"][\"bp_change\"].dropna()\n",
        "\n",
        "mean_low = bp_low.mean()\n",
        "mean_high = bp_high.mean()\n",
        "diff = mean_high - mean_low\n",
        "\n",
        "# Standard error for difference in means (Welch t-test style)\n",
        "se_diff = np.sqrt(bp_low.var(ddof=1)/len(bp_low) + bp_high.var(ddof=1)/len(bp_high))\n",
        "\n",
        "# 95% CI using normal approximation (for teaching; in practice use statsmodels)\n",
        "z = 1.96\n",
        "ci_lower = diff - z * se_diff\n",
        "ci_upper = diff + z * se_diff\n",
        "\n",
        "print(f\"Mean BP change (low coffee):  {mean_low:6.2f} mmHg\")\n",
        "print(f\"Mean BP change (high coffee): {mean_high:6.2f} mmHg\")\n",
        "print(f\"Difference (high - low):      {diff:6.2f} mmHg\")\n",
        "print(f\"Approx. 95% CI: [{ci_lower:6.2f}, {ci_upper:6.2f}] mmHg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alpha_text",
      "metadata": {},
      "source": [
        "### 6.3 Simulating p-values under the null\n",
        "\n",
        "To see what p-values look like when **there is no true effect**, we can simulate many small RCTs where both groups come from the same distribution.\n",
        "\n",
        "If H0 is true and we repeat the experiment many times:\n",
        "\n",
        "- p-values are roughly **uniformly distributed** between 0 and 1.  \n",
        "- The proportion of p-values below α is **approximately α** (e.g. about 5% below 0.05).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pvalue_sim_plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate 10 000 null experiments (no true difference between groups)\n",
        "rng = np.random.default_rng(11088)\n",
        "p_values = []\n",
        "\n",
        "n_per_group = 30\n",
        "n_sim = 10000\n",
        "\n",
        "for _ in range(n_sim):\n",
        "    x = rng.normal(0, 1, n_per_group)\n",
        "    y = rng.normal(0, 1, n_per_group)\n",
        "    _, p = st.ttest_ind(x, y, equal_var=False)\n",
        "    p_values.append(p)\n",
        "\n",
        "alpha_1 = 0.05\n",
        "alpha_2 = 0.0314\n",
        "\n",
        "sns.histplot(p_values, bins=30)\n",
        "plt.axvline(alpha_1, linestyle=\"--\", label=\"0.05\")\n",
        "plt.axvline(alpha_2, linestyle=\":\", label=\"0.0314\")\n",
        "plt.title(\"Distribution of p-values when there is NO true effect\")\n",
        "plt.xlabel(\"p-value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pvalue_sim_props",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Proportion of p-values below each alpha threshold\n",
        "p_values_array = np.array(p_values)\n",
        "prop_005 = np.mean(p_values_array < alpha_1)\n",
        "prop_0314 = np.mean(p_values_array < alpha_2)\n",
        "\n",
        "print(f\"Proportion of p-values < 0.05:   {prop_005:5.3f}\")\n",
        "print(f\"Proportion of p-values < 0.0314: {prop_0314:5.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvalue_reflection_text",
      "metadata": {},
      "source": [
        "**Reflection**\n",
        "\n",
        "- Roughly what proportion of p-values fall below 0.05 when H0 is true?  \n",
        "- What happens when we tighten the threshold to 0.0314?  \n",
        "- What does this tell you about treating p = 0.049 and p = 0.051 as fundamentally different?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "two_groups_text",
      "metadata": {},
      "source": [
        "## 7. Basic applications: parametric vs non-parametric tests\n",
        "\n",
        "A **parametric test** makes assumptions about the distribution of the data (e.g. normality, similar variances).  \n",
        "A **non-parametric test** usually works on **ranks** and makes fewer distributional assumptions.\n",
        "\n",
        "We now compare **BP change** between two coffee arms (e.g. low vs high):\n",
        "\n",
        "- **Parametric test**: independent-samples t-test (Welch).  \n",
        "- **Non-parametric test**: Mann–Whitney U test (Wilcoxon rank-sum).\n",
        "\n",
        "We will use **α = 0.0314** as our decision threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "two_groups_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select two arms for comparison: low vs high coffee\n",
        "bp_low = df[df[\"coffee_arm\"] == \"low\"][\"bp_change\"].dropna()\n",
        "bp_high = df[df[\"coffee_arm\"] == \"high\"][\"bp_change\"].dropna()\n",
        "\n",
        "# Independent-samples t-test (Welch, unequal variances)\n",
        "t_stat, p_t = st.ttest_ind(bp_low, bp_high, equal_var=False)\n",
        "\n",
        "# Mann–Whitney U test (non-parametric)\n",
        "u_stat, p_u = st.mannwhitneyu(bp_low, bp_high, alternative=\"two-sided\")\n",
        "\n",
        "alpha = 0.0314\n",
        "\n",
        "print(f\"t-test:       t = {t_stat:6.3f}, p = {p_t:6.4f}\")\n",
        "print(f\"Mann–Whitney: U = {u_stat:6.1f}, p = {p_u:6.4f}\")\n",
        "print(f\"Using alpha = {alpha}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "two_groups_questions_text",
      "metadata": {},
      "source": [
        "**Questions**\n",
        "\n",
        "- Do the conclusions from the t-test and Mann–Whitney test agree at α = 0.0314?  \n",
        "- Would your conclusion change if you (arbitrarily) switched to α = 0.05?  \n",
        "- Looking back at the distributions, does a parametric or non-parametric test seem more appropriate?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multi_groups_text",
      "metadata": {},
      "source": [
        "## 8. Comparing more than two groups\n",
        "\n",
        "Now consider **blood glucose** across the three cereal arms:\n",
        "\n",
        "- bran  \n",
        "- cornflakes  \n",
        "- muesli\n",
        "\n",
        "We can use:\n",
        "\n",
        "- **One-way ANOVA** (parametric): compares mean values across groups.  \n",
        "- **Kruskal–Wallis test** (non-parametric): compares distributions using ranks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multi_groups_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "groups_glucose = [group[\"glucose\"].values for _, group in df.groupby(\"cereal_arm\")]\n",
        "\n",
        "# One-way ANOVA\n",
        "f_stat, p_anova = st.f_oneway(*groups_glucose)\n",
        "\n",
        "# Kruskal–Wallis test\n",
        "h_stat, p_kw = st.kruskal(*groups_glucose)\n",
        "\n",
        "print(\"One-way ANOVA:\")\n",
        "print(f\"  F = {f_stat:6.3f}, p = {p_anova:6.4f}\")\n",
        "print(\"Kruskal–Wallis:\")\n",
        "print(f\"  H = {h_stat:6.3f}, p = {p_kw:6.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multi_groups_boxplot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualise glucose values by cereal arm\n",
        "sns.boxplot(data=df, x=\"cereal_arm\", y=\"glucose\")\n",
        "plt.title(\"Blood glucose by cereal arm\")\n",
        "plt.xlabel(\"Cereal arm\")\n",
        "plt.ylabel(\"Glucose (arbitrary units)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "categorical_comparison_text",
      "metadata": {},
      "source": [
        "## 9. Comparing categories: chi-squared tests and a warning\n",
        "\n",
        "Sometimes we want to know whether **two categorical variables are associated**, for example:\n",
        "\n",
        "- Is the proportion of participants with **high appetite** different across **test foods**?\n",
        "\n",
        "For this we can use a **chi-squared test of independence** on a contingency table.\n",
        "\n",
        "⚠️ **Important reminder:**  \n",
        "- **Categorical** and **ordinal** data should **not** be analysed as if they were continuous without careful justification.  \n",
        "- For example, treating a 5-point Likert scale as if it were a continuous variable and running a t-test can be misleading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chi_squared_table",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple categorical outcome: high vs not-high appetite\n",
        "high_cutoff = 70\n",
        "df[\"appetite_high\"] = (df[\"appetite_vas\"] >= high_cutoff).astype(int)\n",
        "\n",
        "# Contingency table: appetite_high by food_arm\n",
        "table = pd.crosstab(df[\"appetite_high\"], df[\"food_arm\"])\n",
        "table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chi_squared_test",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chi-squared test of independence\n",
        "chi2, p_chi, dof, expected = st.chi2_contingency(table)\n",
        "\n",
        "print(f\"Chi-squared test: chi2 = {chi2:6.3f}, df = {dof}, p = {p_chi:6.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chi_squared_reflection_text",
      "metadata": {},
      "source": [
        "**Questions**\n",
        "\n",
        "- Does the chi-squared test suggest a difference in the proportion of high appetite across test foods (using α = 0.0314)?  \n",
        "- How would you **report** this result in words?  \n",
        "- Why is a chi-squared test more appropriate here than a t-test on the raw VAS scores split into two categories?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tools_not_oracle_text",
      "metadata": {},
      "source": [
        "## 10. Statistics: tools, not an oracle\n",
        "\n",
        "Finally, a reminder:\n",
        "\n",
        "- Statistical methods help us **summarise uncertainty** and **quantify evidence**.  \n",
        "- They do **not** replace judgement about study design, data quality, or plausibility.  \n",
        "- A \"significant\" p-value does not guarantee truth, and a \"non-significant\" result does not prove there is no effect.\n",
        "\n",
        "When using these tools in real research, always consider:\n",
        "\n",
        "- Are the data appropriate for the method?  \n",
        "- Are the assumptions at least approximately met?  \n",
        "- Do the results make sense in the context of other evidence?  \n",
        "- If a friendly statistician or methodologist (or a small hippo) looked over your analysis, would they recognise the decisions you took and why?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
