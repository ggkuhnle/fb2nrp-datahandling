[
  {
    "objectID": "notebooks/data_handling.html",
    "href": "notebooks/data_handling.html",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "",
    "text": "Version 0.0.3\nThis workbook introduces the foundations of data handling and basic analysis, using a small synthetic RCT dataset that mimics some nutrition trials:\nThe data are simulated and include age and sex.\nThe dataset is made available as a pandas DataFrame called df by the bootstrap cell above.\nThe workbook is designed to work both as:\nBy the end of the workbook you should be able to:"
  },
  {
    "objectID": "notebooks/data_handling.html#from-raw-data-to-analysis-the-basic-flow",
    "href": "notebooks/data_handling.html#from-raw-data-to-analysis-the-basic-flow",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "1 0. From raw data to analysis: the basic flow",
    "text": "1 0. From raw data to analysis: the basic flow\nIn almost every quantitative study, the flow of analysis is:\n\nView the data\n\nLoad the dataset into your software (here: pandas DataFrame df).\n\nLook at a few rows (df.head()).\n\nCheck variable names, data types, and obvious issues (df.info()).\n\nClean the data\n\nHandle missing values (decide when to impute, when to drop).\n\nDetect obviously impossible values (e.g. age = −5, VAS &gt; 100).\n\nFix coding problems (e.g. “Male” vs “M” vs “m”).\n\nStandardise the data\n\nEnsure variables use consistent units (e.g. all blood pressure in mmHg, all glucose in mmol/L).\n\nRecode categories in a consistent way (e.g. F/M, or 0/1 with clear labels).\n\nAnalyse the data\n\nStart with descriptive statistics (means, medians, counts, percentages).\n\nPresent a clear Table 1 of baseline characteristics.\n\nMove to statistical inference (p-values, confidence intervals, models) only once you understand the data.\n\n\nIn this workbook we follow the same structure: first understand and describe, then compare and infer.\n\n# ============================================================\n# FB2NRP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Locally: expects you to open the notebook from *inside*\n#   the fb2nrp-datahandling repository (e.g. repo/notebooks).\n#   It walks up the directory tree to find scripts/bootstrap.py.\n# - In Colab: if the repo is not found, it clones it from GitHub\n#   into /content/fb2nrp-datahandling.\n# - Loads and runs scripts/bootstrap.py.\n# - Generates a synthetic dataset and makes it available as `df`.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\nREPO_URL = \"https://github.com/ggkuhnle/fb2nrp-datahandling.git\"\nREPO_DIR = \"fb2nrp-datahandling\"\n\ndef in_colab() -&gt; bool:\n    \"\"\"Return True if running inside Google Colab.\"\"\"\n    try:\n        import google.colab  # type: ignore  # noqa: F401\n        return True\n    except ImportError:\n        return False\n\n# Make sure the process cwd is valid\ntry:\n    cwd = pathlib.Path.cwd()\nexcept FileNotFoundError:\n    raise RuntimeError(\n        \"Current working directory no longer exists.\\n\"\n        \"Please restart the kernel from inside the fb2nrp-datahandling repository \"\n        \"(e.g. open the notebook from repo/notebooks and try again).\"\n    )\n\n# Try to find the repo root by walking up the directory tree\nrepo_root = None\nfor parent in [cwd] + list(cwd.parents):\n    if (parent / \"scripts\" / \"bootstrap.py\").is_file():\n        repo_root = parent\n        break\n\nif repo_root is not None:\n    # We are somewhere inside an existing clone (local or Colab)\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n    print(f\"Repository root detected at: {repo_root}\")\nelse:\n    # Repo not found by walking up\n    if in_colab():\n        # In Colab: clone into /content/fb2nrp-datahandling\n        base_dir = pathlib.Path(\"/content\")\n        os.chdir(base_dir)\n        repo_root = base_dir / REPO_DIR\n        if not repo_root.is_dir():\n            print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n            subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n        else:\n            print(f\"Using existing repository at {repo_root}\")\n        os.chdir(repo_root)\n        repo_root = pathlib.Path.cwd()\n        print(f\"Repository root set to: {repo_root}\")\n    else:\n        # Local but not inside the repo: fail with a clear message\n        raise RuntimeError(\n            \"Could not find fb2nrp-datahandling repository root.\\n\"\n            \"Please make sure you open this notebook from inside the \"\n            \"`fb2nrp-datahandling` repository (e.g. repo/notebooks) and \"\n            \"then re-run this cell.\"\n        )\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nrp-datahandling repository structure is intact.\"\n    )\n\nspec = importlib.util.spec_from_file_location(\"fb2nrp_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nrp_bootstrap\"] = bootstrap\nspec.loader.exec_module(bootstrap)\n\n# CTX will contain paths and settings defined in bootstrap.py\nCTX = bootstrap.init()\n\nfor name in [\"REPO_NAME\", \"REPO_URL\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The context object is available as `CTX`.\")\n\n\n# ============================================================\n# Setup: scientific Python libraries and plotting style\n#\n# Assumes the bootstrap cell above has already created:\n#   - CTX : context object with paths and settings\n# ============================================================\n\n# Data handling and numerical computing\nimport numpy as np\nimport pandas as pd\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical tests and distributions\nimport scipy.stats as st\n\n# Display options (optional but helpful)\npd.set_option(\"display.max_rows\", 20)\npd.set_option(\"display.max_columns\", 20)\n\n# Plot style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nprint(\"Libraries loaded.\")\n\n\n# ============================================================\n# Generate the synthetic dataset for this workbook\n# ============================================================\n\n# The helper function simulate_practical_data() returns a\n# small DataFrame that mimics some nutrition RCT practicals.\nfrom scripts.helpers import simulate_practical_data, VARIABLES\n\n# Use a fixed seed for reproducibility\ndf = simulate_practical_data(seed=11088)\n\nprint(f\"Dataset loaded with {len(df)} rows and {df.shape[1]} columns.\")"
  },
  {
    "objectID": "notebooks/data_handling.html#study-variables-and-data-types",
    "href": "notebooks/data_handling.html#study-variables-and-data-types",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "2 1. Study variables and data types",
    "text": "2 1. Study variables and data types\nBefore running any analysis, we need to understand what kind of variables we have.\nDifferent types require different summaries and different statistical tests.\n\n2.1 1.1 Overview of variable types\n\n\n\n\n\n\n\n\n\n\nVariable type\nWhat it means\nExamples\nHow to summarise\nAppropriate analyses\n\n\n\n\nCategorical (nominal)\nDistinct labels with no natural order. Values are names only.\nSex (F/M), coffee arm (low/medium/high), cereal arm, favourite animal (hippo).\nCounts and percentages.\nChi-squared tests, Fisher’s exact test, logistic/multinomial regression.\n\n\nOrdinal\nCategories with a natural order, but unequal spacing between levels.\nLikert scale 1–5, hunger rating (low/medium/high), symptom severity.\nCounts/percentages; sometimes median (IQR) of coded scores with justification.\nMann–Whitney test, Kruskal–Wallis, ordinal logistic regression.\n\n\nContinuous (or approx. continuous)\nNumerical values where differences and averages are meaningful. Often many possible values.\nAge (years), BP change (mmHg), glucose, VAS (0–100, often treated as continuous).\nMean ± SD (if symmetric), or median (IQR) if skewed.\nt-tests, ANOVA, correlation, linear regression; non-parametric alternatives if needed.\n\n\n\nA few reminders:\n\nCoding categorical data as numbers does not turn them into continuous variables.\n\nOrdinal scales can sometimes be treated as continuous only if many levels and behaved distributions make it reasonable.\n\nVAS scores (0–100) occupy a grey zone: technically ordinal, often acceptable to treat as continuous in nutrition.\n\n\n\n2.2 1.2 Variables in our synthetic dataset\nOur synthetic dataset df contains (one row per participant):\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nsex\nCategorical (nominal)\nParticipant sex (F/M)\n\n\nage\nContinuous\nAge in years\n\n\ncoffee_arm\nCategorical (nominal)\nIntervention: low / medium / high coffee\n\n\ncereal_arm\nCategorical (nominal)\nCereal: bran / cornflakes / muesli\n\n\nfood_arm\nCategorical (nominal)\nTest food: apple / biscuit / yoghurt\n\n\nbp_change\nContinuous\nChange in blood pressure (mmHg)\n\n\nglucose\nContinuous\nPostprandial blood glucose (arbitrary units)\n\n\nappetite_vas\nContinuous / ordinal\nVAS 0–100; treated here as approx. continuous\n\n\n\nFor completeness, we can also display the helper metadata VARIABLES that describes each column.\n\nVARIABLES"
  },
  {
    "objectID": "notebooks/data_handling.html#first-look-at-the-dataset",
    "href": "notebooks/data_handling.html#first-look-at-the-dataset",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "3 2. First look at the dataset",
    "text": "3 2. First look at the dataset\nWe start with a quick overview of df:\n\ndf.head() shows the first few rows (useful to spot obvious coding issues).\n\ndf.info() summarises variables, data types, and missing values.\n\nThis is the “view” step of the analysis flow.\n\n# First few rows of the dataset\ndf.head()\n\n\n# Overall structure of the DataFrame (types, missingness)\ndf.info()\n\n\n3.1 2.1 Missing values and impossible values\nWe should also check for missing values and obviously impossible values (e.g. negative age, VAS &gt; 100, glucose = 0 in a living participant).\nOur simulator does not generate missing or impossible values, but in real data these checks are essential and sometimes the longest part of the analysis.\n\n# Count of missing values per variable\ndf.isna().sum()"
  },
  {
    "objectID": "notebooks/data_handling.html#describing-continuous-variables",
    "href": "notebooks/data_handling.html#describing-continuous-variables",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "4 3. Describing continuous variables",
    "text": "4 3. Describing continuous variables\nFor continuous variables (age, BP change, glucose, VAS) we want to describe:\n\nWhere the values tend to lie (central tendency).\n\nHow much they vary (dispersion).\n\nWhat the distribution looks like (shape).\n\nIn this section we first look at distributions, then define central tendency and dispersion, and finally compute appropriate summary statistics.\n\n4.1 3.1 Distributions and how to look at them\nMany statistical methods assume that variables follow certain distributions.\nFor this workbook, four are particularly useful:\n\nNormal distribution (bell-shaped, symmetric).\n\nLog-normal distribution (skewed; log of the variable is normal).\n\nt-distribution (like normal, but with heavier tails; used in t-tests).\n\nPoisson distribution (for counts, especially of rare events).\n\nWe do not need the formulas; we just need to recognise their shapes and know when they are plausible models.\nWe usually look at distributions in two ways:\n\nHistograms/density plots: show the shape of the data.\n\nQ–Q plots (Quantile–Quantile plots): compare the quantiles of the data to those of a reference distribution (often normal).\n\n\n4.1.1 3.1.1 Normal and log-normal distributions\n\nA variable is approximately normal when its histogram is symmetric and bell-shaped.\n\nExample: adult height, measurement error, often blood pressure in reasonably homogeneous groups.\n\nA variable is log-normal when its logarithm is approximately normal.\n\nExample: many biomarkers and concentrations, where values are strictly positive and skewed to the right.\n\n\nBelow we simulate data from a normal distribution to illustrate the shape.\n\n# Simulated example: normal distribution\nrng = np.random.default_rng(11088)\nnormal_sample = rng.normal(loc=0, scale=1, size=2000)\n\nsns.histplot(normal_sample, kde=True)\nplt.title(\"Simulated normal distribution (mean = 0, SD = 1)\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Count\")\nplt.show()\n\nBelow we simulate data from a log-normal distribution. Notice the right-skewed shape: many observations near the lower end, with a long tail of higher values.\n\n# Simulated example: log-normal distribution\nlognormal_sample = rng.lognormal(mean=0, sigma=0.6, size=2000)\n\nsns.histplot(lognormal_sample, kde=True)\nplt.title(\"Simulated log-normal distribution\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n4.1.2 3.1.2 t-distribution\nThe t-distribution appears when we:\n\nestimate means from small samples, and\n\nuse the sample SD instead of the true population SD.\n\nIt looks similar to the normal distribution but has heavier tails, especially with small degrees of freedom (df).\nThis matters for t-tests and confidence intervals based on small samples.\nBelow we plot t-distributions with different degrees of freedom and compare them to the standard normal.\n\n# t-distributions vs standard normal\nx = np.linspace(-4, 4, 400)\npdf_normal = st.norm.pdf(x, loc=0, scale=1)\npdf_t3 = st.t.pdf(x, df=3)\npdf_t10 = st.t.pdf(x, df=10)\npdf_t30 = st.t.pdf(x, df=30)\n\nplt.plot(x, pdf_normal, label=\"Normal\")\nplt.plot(x, pdf_t3, linestyle=\"--\", label=\"t, df=3\")\nplt.plot(x, pdf_t10, linestyle=\":\", label=\"t, df=10\")\nplt.plot(x, pdf_t30, linestyle=\"-.\", label=\"t, df=30\")\nplt.title(\"Normal vs t-distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\n4.1.3 3.1.3 Poisson distribution\nThe Poisson distribution is a model for counts of events in a fixed time or space, especially when events are:\n\nindependent, and\n\nindividually rare.\n\nExamples:\n\nNumber of adverse events per participant in a trial.\n\nNumber of emergency admissions per day in a small hospital.\n\nNumber of typing errors per page in a report (for some of us).\n\nIt has a single parameter λ (lambda), which is both the mean and the variance of the distribution.\nBelow we show the probabilities for a Poisson distribution with λ = 2.5.\n\n# Poisson distribution example (lambda = 2.5)\nlam = 2.5\nk_values = np.arange(0, 11)  # 0 to 10 events\npmf = st.poisson.pmf(k_values, mu=lam)\n\nplt.stem(k_values, pmf)\nplt.title(\"Poisson(λ = 2.5) distribution\")\nplt.xlabel(\"Number of events (k)\")\nplt.ylabel(\"Probability P(X = k)\")\nplt.show()\n\n\n\n4.1.4 3.1.4 Looking at our data: BP change\nNow we return to our dataset and look at the distribution of blood pressure change (bp_change).\nWe use:\n\na histogram with a smooth density estimate, and\n\na Q–Q plot against the normal distribution.\n\n\n# Histogram and density for blood pressure change\nsns.histplot(df[\"bp_change\"], kde=True)\nplt.title(\"Distribution of BP change\")\nplt.xlabel(\"BP change (mmHg)\")\nplt.ylabel(\"Count\")\nplt.show()\n\nA Q–Q plot compares the quantiles of our data to those of a perfect normal distribution.\n\nIf the points lie roughly on a straight line, the data are not wildly inconsistent with normality.\n\nSystematic curves (S-shape, heavy tails) suggest deviations such as skewness or outliers.\n\n\n# Q–Q plot to assess normality of BP change\nst.probplot(df[\"bp_change\"], dist=\"norm\", plot=plt)\nplt.title(\"Q–Q plot of BP change\")\nplt.show()\n\n\n\n\n4.2 3.2 Central tendency and dispersion\nOnce we have a sense of the shape of a distribution, we can talk about:\n\nCentral tendency – where the values tend to lie.\n\nDispersion (spread) – how much they vary around the centre.\n\nCommon choices:\n\nMean (average) and standard deviation (SD)\n\nMost useful when the distribution is not too skewed.\n\n\nMedian and interquartile range (IQR)\n\nMore robust to skewed distributions and outliers.\n\n\nChoice of summary should be guided by the distributional shape, not by habit.\n\n\n4.3 3.3 Mean, SD, median, and IQR\nDefinitions:\n\nMean: add up all observations and divide by the number of observations.\n\nStandard deviation (SD): describes how far, on average, observations are from the mean.\n\nMedian: the middle value when the data are ordered (50% below, 50% above).\n\nInterquartile range (IQR): difference between the 75th percentile (Q3) and 25th percentile (Q1).\n\nRules of thumb:\n\nIf the distribution is roughly symmetric → report mean ± SD.\n\nIf the distribution is clearly skewed → report median (IQR).\n\nIn practice, many papers report both, at least in supplementary material.\n\n\n4.4 3.4 Sample vs population and the standard error of the mean (SEM)\nIn practice we almost never observe the entire population. We observe a sample and use it to say something about the population.\n\nPopulation mean (μ): the true average in the entire population (usually unknown).\n\nSample mean (x̄): the average in our sample.\n\nIf we repeatedly took new samples of the same size and calculated the mean each time, those sample means would vary.\n\nThe standard deviation (SD) describes variability between individuals.\n\nThe standard error of the mean (SEM) describes variability between sample means.\n\nFor a sample of size n, and sample SD = s, a common estimate is:\n\\[\\text{SEM} \\approx \\frac{s}{\\sqrt{n}}.\\]\nSEM is mainly used when constructing confidence intervals and performing hypothesis tests, not for describing raw data in a Table 1.\n\n# Basic summary statistics for numeric variables\n# (mean, SD, min, max, quartiles)\ndf.describe()\n\n\n\n4.5 3.5 Descriptive statistics for key continuous outcomes\nLet us compute both mean/SD and median/IQR for the three main continuous outcomes:\n\nbp_change\n\nglucose\n\nappetite_vas\n\nThis table is close to what you might include in a results section or a Table 1.\n\ncont_vars = [\"bp_change\", \"glucose\", \"appetite_vas\"]\nrows = []\n\nfor var in cont_vars:\n    series = df[var].dropna()\n    mean = series.mean()\n    sd = series.std()\n    median = series.median()\n    q1 = series.quantile(0.25)\n    q3 = series.quantile(0.75)\n    iqr = q3 - q1\n    rows.append({\n        \"variable\": var,\n        \"mean\": mean,\n        \"sd\": sd,\n        \"median\": median,\n        \"q1\": q1,\n        \"q3\": q3,\n        \"iqr\": iqr\n    })\n\nsummary_cont = pd.DataFrame(rows)\nsummary_cont"
  },
  {
    "objectID": "notebooks/data_handling.html#categorical-variables-nominal-vs-ordinal",
    "href": "notebooks/data_handling.html#categorical-variables-nominal-vs-ordinal",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "5 4. Categorical variables: nominal vs ordinal",
    "text": "5 4. Categorical variables: nominal vs ordinal\nFor categorical variables we usually report counts and percentages.\nTwo subclasses are important:\n\nNominal (unordered)\n\nExamples: sex (F/M), cereal arm, country, favourite snack.\n\nThe labels have no inherent ranking.\n\nOrdinal (ordered)\n\nExamples: Likert scales, symptom severity (mild/moderate/severe).\n\nThere is a natural order, but the distance between categories is not known.\n\n\n\n5.1 4.1 Why not just convert categories into numbers and treat them as continuous?\nIt is tempting to code categories as numbers (e.g. apple = 1, biscuit = 2, yoghurt = 3) and then compute a mean.\nThis is usually a bad idea because:\n\nThe numerical codes are arbitrary labels, not real quantities.\n\nThe differences between codes (2 − 1 vs 3 − 2) have no scientific meaning.\n\nTreating them as continuous in a t-test or regression can give misleading results.\n\nInstead, we summarise categorical variables using counts, percentages, and contingency tables.\n\n# Distribution of participants by coffee arm (counts)\ndf[\"coffee_arm\"].value_counts().to_frame(name=\"count\")\n\n\n# Contingency table: sex by coffee arm\npd.crosstab(df[\"sex\"], df[\"coffee_arm\"], margins=True)"
  },
  {
    "objectID": "notebooks/data_handling.html#data-description-as-table-1",
    "href": "notebooks/data_handling.html#data-description-as-table-1",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "6 5. Data description as “Table 1”",
    "text": "6 5. Data description as “Table 1”\nMost clinical and nutrition papers include a Table 1 that describes the baseline characteristics of the study sample.\nTypical elements:\n\nA column for “All participants”.\n\nAdditional columns for treatment arms (or exposure groups).\n\nRows for key variables: age, sex, BMI, main outcomes, etc.\n\nContinuous variables shown as mean ± SD or median (IQR).\n\nCategorical variables shown as n (%).\n\nBelow we build a very simple Table 1 describing age and sex by coffee arm.\nThis is for illustration only – in real work you would usually format the table more nicely (e.g. for LaTeX or Word).\n\n# Simple Table 1: age and sex by coffee arm\n\ngroup_var = \"coffee_arm\"\ncontinuous_vars = [\"age\"]\ncategorical_vars = [\"sex\"]\n\narms = df[group_var].unique()\narms.sort()\n\ntable1_rows = []\n\n# Continuous variables: report mean ± SD\nfor var in continuous_vars:\n    row = {\"variable\": var, \"type\": \"continuous\"}\n    for arm in arms:\n        sub = df[df[group_var] == arm][var].dropna()\n        m = sub.mean()\n        s = sub.std()\n        row[arm] = f\"{m:.1f} ± {s:.1f}\"\n    table1_rows.append(row)\n\n# Categorical variables: report n (%)\nfor var in categorical_vars:\n    levels = df[var].dropna().unique()\n    levels.sort()\n    for level in levels:\n        row = {\n            \"variable\": f\"{var} = {level}\",\n            \"type\": \"categorical\"\n        }\n        for arm in arms:\n            sub = df[df[group_var] == arm]\n            n = (sub[var] == level).sum()\n            total = len(sub)\n            perc = 100 * n / total if total &gt; 0 else np.nan\n            row[arm] = f\"{n} ({perc:.1f}%)\"\n        table1_rows.append(row)\n\ntable1 = pd.DataFrame(table1_rows)\ntable1\n\n\n6.1 5.1 Example text for methods/results\nUsing a table like the one above, you might write in a paper:\n\n“Participants had a mean age of 22.1 ± 3.1 years.”\n\n“Overall, 40% of participants were male (n = 72/180).”\n\n“Baseline blood pressure did not differ meaningfully between coffee arms.”\n\nThe exact wording depends on the study, but the principle is always:\n\nDescribe who was studied.\n\nUse appropriate summaries for each variable type.\n\nMake it possible for the reader to judge how well the sample represents the population of interest."
  },
  {
    "objectID": "notebooks/data_handling.html#statistical-inference-and-nhst",
    "href": "notebooks/data_handling.html#statistical-inference-and-nhst",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "7 6. Statistical inference and NHST",
    "text": "7 6. Statistical inference and NHST\nSo far we have described the sample. Statistical inference is about what we can reasonably say about the underlying population.\nIn classical null hypothesis significance testing (NHST) we:\n\nFormulate a null hypothesis (H0), usually “no difference” or “no effect”.\n\nFormulate an alternative hypothesis (H1), e.g. “there is a difference”.\n\nChoose a test statistic (e.g. a t-statistic) and compute it from the data.\n\nCompute a p-value: the probability (under H0) of observing a result at least as extreme as the one we saw.\n\nCompare the p-value to a threshold α (alpha) to decide whether the result is compatible with H0.\n\nExample in this workbook:\n\nH0: mean BP change is the same in low and high coffee arms.\n\nH1: mean BP change is different in the two arms.\n\n\n7.1 We never prove H0 or H1; we simply assess how compatible the data are with H0.\n\n\n7.2 6.1 p-values and why 0.05 is not magical\n\nA p-value is not the probability that H0 is true.\n\nIt is the probability of the observed data (or more extreme) if H0 were true.\n\nCommon misunderstandings:\n\np = 0.04 does not mean there is a 96% chance that the effect is real.\n\np = 0.06 does not mean “no effect”.\n\nThe widely used threshold α = 0.05 is just a convention:\n\n0.049 and 0.051 are essentially the same in terms of evidence.\n\nTreating them as “significant” vs “non-significant” can be misleading.\n\nIn reality, we should look at effect size, uncertainty, and context.\n\nIn this workbook we deliberately use an unusual threshold α = 0.0314 to emphasise that the choice of α is arbitrary and should be justified, not blindly copied.\n\n\n7.3 6.2 Confidence intervals (CIs)\nA 95% confidence interval (CI) for a parameter (e.g. difference in means) is constructed such that, in repeated samples, 95% of such intervals would contain the true parameter.\nIn practice:\n\nIf a 95% CI for a difference excludes 0, the corresponding two-sided test at α = 0.05 is “statistically significant”.\n\nThe CI gives information about precision (width of the interval) and effect size (where the interval lies).\n\nCIs are usually more informative than a bare p-value.\n\n\n# Example: difference in mean BP change between low and high coffee arms\n\nbp_low = df[df[\"coffee_arm\"] == \"low\"][\"bp_change\"].dropna()\nbp_high = df[df[\"coffee_arm\"] == \"high\"][\"bp_change\"].dropna()\n\nmean_low = bp_low.mean()\nmean_high = bp_high.mean()\ndiff = mean_high - mean_low\n\n# Standard error for difference in means (Welch t-test style)\nse_diff = np.sqrt(bp_low.var(ddof=1)/len(bp_low) + bp_high.var(ddof=1)/len(bp_high))\n\n# 95% CI using normal approximation (for teaching; in practice use statsmodels)\nz = 1.96\nci_lower = diff - z * se_diff\nci_upper = diff + z * se_diff\n\nprint(f\"Mean BP change (low coffee):  {mean_low:6.2f} mmHg\")\nprint(f\"Mean BP change (high coffee): {mean_high:6.2f} mmHg\")\nprint(f\"Difference (high - low):      {diff:6.2f} mmHg\")\nprint(f\"Approx. 95% CI: [{ci_lower:6.2f}, {ci_upper:6.2f}] mmHg\")\n\n\n\n7.4 6.3 Simulating p-values under the null\nTo see what p-values look like when there is no true effect, we can simulate many small RCTs where both groups come from the same distribution.\nIf H0 is true and we repeat the experiment many times:\n\np-values are roughly uniformly distributed between 0 and 1.\n\nThe proportion of p-values below α is approximately α (e.g. about 5% below 0.05).\n\n\n# Simulate 10 000 null experiments (no true difference between groups)\nrng = np.random.default_rng(11088)\np_values = []\n\nn_per_group = 30\nn_sim = 10000\n\nfor _ in range(n_sim):\n    x = rng.normal(0, 1, n_per_group)\n    y = rng.normal(0, 1, n_per_group)\n    _, p = st.ttest_ind(x, y, equal_var=False)\n    p_values.append(p)\n\nalpha_1 = 0.05\nalpha_2 = 0.0314\n\nsns.histplot(p_values, bins=30)\nplt.axvline(alpha_1, linestyle=\"--\", label=\"0.05\")\nplt.axvline(alpha_2, linestyle=\":\", label=\"0.0314\")\nplt.title(\"Distribution of p-values when there is NO true effect\")\nplt.xlabel(\"p-value\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n\n# Proportion of p-values below each alpha threshold\np_values_array = np.array(p_values)\nprop_005 = np.mean(p_values_array &lt; alpha_1)\nprop_0314 = np.mean(p_values_array &lt; alpha_2)\n\nprint(f\"Proportion of p-values &lt; 0.05:   {prop_005:5.3f}\")\nprint(f\"Proportion of p-values &lt; 0.0314: {prop_0314:5.3f}\")\n\n\n\n7.5 6.4 Visualising similar data with different p-values\nIt is easy to forget that p-values form a continuum, not a cliff.\nTo make this concrete, we use a pre-simulated dataset (data/pval.csv) created as follows:\n\nTwo groups of 50 observations each were generated from the same normal distribution (mean 150, SD 20).\n\nWe repeatedly simulated new datasets until we obtained a range of two-sample t-test p-values:\n\nVery small: 0.0001, 0.001\n\n“Classic” small: 0.01, 0.02, 0.04, 0.049, 0.05, 0.051, 0.06\n\nClearly non-significant: 0.10, 0.50, 1.00\n\nFor each p-value in this list we kept one dataset and stored all individual values.\n\nIn this section we:\n\nLoad the pre-simulated data.\n\nPlot boxplots + jittered points for each p-value.\n\nZoom in on p = 0.049, 0.050, 0.051.\n\nWhat to look for\n\nThe visual differences between groups change gradually, not suddenly at 0.05.\n\n“Just significant” (0.049) and “just non-significant” (0.051) are practically indistinguishable.\n\nVery small p-values correspond to larger effects and/or more precise estimates, not some new qualitative state of the world.\n\n\n# 6.3(a) Load pre-simulated data and plot all p-values\n# ----------------------------------------------------\n\n\n# Path to the CSV (generated previously in R)\n# Prefer CTX.data_dir if available, otherwise default to ./data\ntry:\n    data_dir = CTX.data_dir\nexcept AttributeError:\n    data_dir = pathlib.Path(\"data\")\n\npval_path = data_dir / \"pval.csv\"\nprint(f\"Loading p-value data from: {pval_path}\")\n\npval_df = pd.read_csv(pval_path)\n# Define desired p-value order\n\n\n# Drop the old row index column if present\nif \"Unnamed: 0\" in pval_df.columns:\n    pval_df = pval_df.drop(columns=[\"Unnamed: 0\"])\n\n# Make p-values a categorical label for nicer x-axis\npval_df[\"pval_label\"] = pval_df[\"pval\"].map(lambda x: f\"{x:g}\")\n\npval_df.head()\n\n\n\n\n\n\n# Combined boxplot + jitter for all available p-values\n# Each code cell: one main output figure\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.boxplot(\n    data=pval_df,\n    x=\"pval\",\n    y=\"value\",\n    hue=\"class\",\n    dodge=True,\n    fliersize=0,\n    ax=ax,\n)\n\nsns.stripplot(\n    data=pval_df,\n    x=\"pval\",\n    y=\"value\",\n    hue=\"class\",\n    dodge=True,\n    jitter=0.25,\n    alpha=0.6,\n    size=2,\n    ax=ax,\n)\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles[:2], labels[:2], title=\"Group\")\n\nax.set_xlabel(\"p-value\")\nax.set_ylabel(\"Blood pressure (mmHg)\")\nax.set_title(\"What does a p-value look like?\\nTwo groups of random numbers\")\n\nplt.tight_layout()\nplt.show()\n\nReflection\n\nRoughly what proportion of p-values fall below 0.05 when H0 is true?\n\nWhat happens when we tighten the threshold to 0.0314?\n\n8 What does this tell you about treating p = 0.049 and p = 0.051 as fundamentally different?"
  },
  {
    "objectID": "notebooks/data_handling.html#basic-applications-parametric-vs-non-parametric-tests",
    "href": "notebooks/data_handling.html#basic-applications-parametric-vs-non-parametric-tests",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "9 7. Basic applications: parametric vs non-parametric tests",
    "text": "9 7. Basic applications: parametric vs non-parametric tests\nA parametric test (such as the independent-samples t-test) makes assumptions about the distribution and behaviour of the data.\nA non-parametric test usually works on ranks and makes fewer distributional assumptions.\nBefore we compare BP change between two coffee arms (low vs high), it is important to recall the key assumptions behind the t-test.\n\n9.1 Assumptions for the independent-samples t-test\n\nIndependence\nObservations in one group must be independent of observations in the other group. Within each group, participants should not influence one another. (This is usually satisfied by design.)\nApproximately normal distribution (for each group)\nThe t-test assumes that the outcome in each group is approximately normally distributed.\n\nThe test is fairly robust to mild deviations, especially with reasonable sample sizes.\n\nWhat matters is normality of the data within each group, not the difference between groups.\n\nSimilar variances (homoscedasticity)\nThe classic Student t-test assumes that the two groups have equal variances.\n\nWelch’s t-test, which we use here, relaxes this assumption and performs much better when variances differ.\n\nIn practice, Welch is almost always the safer default.\n\nScale and measurement\nThe outcome should be continuous (or close to it) so that means and standard deviations are meaningful summaries.\n\nIf these assumptions are doubtful—especially if the data are skewed or contain outliers—a non-parametric alternative such as the Mann–Whitney U test may provide a more robust comparison.\nWe will use:\n\nParametric test: Welch’s independent-samples t-test\n\nNon-parametric test: Mann–Whitney U test\n\nand a decision threshold of α = 0.0314.\nFirst, we look at the data.\n\n# ------------------------------------------------------------\n# 7(a). Descriptive statistics by coffee arm (low vs high)\n# ------------------------------------------------------------\n\ngroups = [\"low\", \"high\"]\nrows = []\n\nfor g in groups:\n    series = df.loc[df[\"coffee_arm\"] == g, \"bp_change\"].dropna()\n    rows.append({\n        \"coffee_arm\": g,\n        \"n\": len(series),\n        \"mean\": series.mean(),\n        \"sd\": series.std(),\n        \"median\": series.median(),\n        \"q1\": series.quantile(0.25),\n        \"q3\": series.quantile(0.75),\n    })\n\nbp_summary = pd.DataFrame(rows)\nbp_summary[\"IQR\"] = bp_summary[\"q3\"] - bp_summary[\"q1\"]\nbp_summary\n\nAnd now, we conduct a comparison:\n\n\n9.2 7.1 Visual comparison with notched boxplots\nA notched boxplot shows an approximate confidence interval around the median.\n\nIf the notches of two groups do not overlap, this suggests a difference in medians.\n\nIf they do overlap, the data are compatible with similar medians.\n\nHere we draw a notched boxplot of BP change (ΔSBP) by coffee arm.\n\n# ------------------------------------------------------------\n# 7(b). Notched boxplot: BP change by coffee arm\n# ------------------------------------------------------------\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Prepare the data for each group\nbp_low = df.loc[df[\"coffee_arm\"] == \"low\", \"bp_change\"].dropna()\nbp_high = df.loc[df[\"coffee_arm\"] == \"high\", \"bp_change\"].dropna()\n\nax.boxplot(\n    [bp_low, bp_high],\n    labels=[\"low\", \"high\"],\n    notch=True,          # &lt;- notches\n    widths=0.6\n)\n\nax.set_xlabel(\"Coffee arm\")\nax.set_ylabel(\"BP change (mmHg)\")\nax.set_title(\"Notched boxplot of BP change by coffee arm\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n9.3 7.2 From description to hypothesis tests\nWe have now:\n\nsummarised BP change (ΔSBP) in each coffee arm (n, mean, SD, median, IQR), and\n\nlooked at the distribution shapes using boxplots (and notched boxplots).\n\nThe next step is to ask a specific question:\n\nIs the average change in systolic blood pressure different\nbetween the low and high coffee arms?\n\nTo answer this, we use two related but conceptually different tests:\n\nWelch’s independent-samples t-test (parametric)\n\nWorks on the original values.\n\nAssumes the data in each group are approximately normal and that the mean is a sensible summary.\n\nAllows for different variances in the two groups (Welch).\n\nMann–Whitney U test (non-parametric, Wilcoxon rank-sum)\n\nWorks on the ranks of the values, not the raw numbers.\n\nDoes not assume normality.\n\nIs mainly sensitive to shifts in the central tendency between groups.\n\n\nFor this workbook we use a slightly unusual threshold α = 0.0314 rather than 0.05, to underline that:\n\nα is a choice, not a law of nature, and\n\np-values are part of a continuum, not a yes/no oracle.\n\nIn the next cell we compute both tests and compare their conclusions.\n\n# ------------------------------------------------------------\n# 7.3 Parametric vs non-parametric tests: detailed output\n# ------------------------------------------------------------\n\n# Select two arms for comparison: low vs high coffee\nbp_low = df[df[\"coffee_arm\"] == \"low\"][\"bp_change\"].dropna()\nbp_high = df[df[\"coffee_arm\"] == \"high\"][\"bp_change\"].dropna()\n\nalpha = 0.0314\n\n# --- 1. Descriptive stats for each group --------------------\ndef summarise(series):\n    return {\n        \"n\": len(series),\n        \"mean\": series.mean(),\n        \"sd\": series.std(),\n        \"median\": series.median(),\n        \"q1\": series.quantile(0.25),\n        \"q3\": series.quantile(0.75),\n    }\n\nsummary_low = summarise(bp_low)\nsummary_high = summarise(bp_high)\n\nprint(\"Descriptive statistics for BP change (ΔSBP)\")\nprint(\"===========================================\")\nprint(f\"Low coffee arm:\")\nprint(f\"  n       = {summary_low['n']}\")\nprint(f\"  mean    = {summary_low['mean']:6.2f} mmHg\")\nprint(f\"  sd      = {summary_low['sd']:6.2f} mmHg\")\nprint(f\"  median  = {summary_low['median']:6.2f} mmHg\")\nprint(f\"  IQR     = {summary_low['q1']:6.2f} to {summary_low['q3']:6.2f} mmHg\")\n\nprint(\"\\nHigh coffee arm:\")\nprint(f\"  n       = {summary_high['n']}\")\nprint(f\"  mean    = {summary_high['mean']:6.2f} mmHg\")\nprint(f\"  sd      = {summary_high['sd']:6.2f} mmHg\")\nprint(f\"  median  = {summary_high['median']:6.2f} mmHg\")\nprint(f\"  IQR     = {summary_high['q1']:6.2f} to {summary_high['q3']:6.2f} mmHg\")\n\n# --- 2. Difference in means + approx 95% CI -----------------\ndiff_means = summary_high[\"mean\"] - summary_low[\"mean\"]\n\n# Standard error of difference in means (Welch-style)\nse_diff = np.sqrt(bp_low.var(ddof=1) / summary_low[\"n\"] +\n                  bp_high.var(ddof=1) / summary_high[\"n\"])\n\n# Approximate 95% CI using normal quantiles (for teaching)\nz_95 = 1.96\nci_low = diff_means - z_95 * se_diff\nci_high = diff_means + z_95 * se_diff\n\nprint(\"\\nDifference in means (high – low):\")\nprint(\"=================================\")\nprint(f\"  Δmean   = {diff_means:6.2f} mmHg\")\nprint(f\"  SE_diff = {se_diff:6.2f} mmHg\")\nprint(f\"  Approx. 95% CI: [{ci_low:6.2f}, {ci_high:6.2f}] mmHg\")\n\n\n\n\n\n# --- 3. Hypothesis tests ------------------------------------\n# Welch t-test (parametric)\nt_stat, p_t = st.ttest_ind(bp_low, bp_high, equal_var=False)\n\n# Mann–Whitney U test (non-parametric)\nu_stat, p_u = st.mannwhitneyu(bp_low, bp_high, alternative=\"two-sided\")\n\nprint(\"\\nHypothesis tests (H0: no difference between arms)\")\nprint(\"=================================================\")\nprint(f\"Welch t-test (means, assumes ~normality):\")\nprint(f\"  t-statistic = {t_stat:6.3f}\")\nprint(f\"  p-value     = {p_t:6.4f}\")\nprint(f\"  Decision at α = {alpha}: \"\n      f\"{'reject H0' if p_t &lt; alpha else 'do NOT reject H0'}\")\n\nprint(\"\\nMann–Whitney U test (ranks, fewer assumptions):\")\nprint(f\"  U-statistic = {u_stat:6.1f}\")\nprint(f\"  p-value     = {p_u:6.4f}\")\nprint(f\"  Decision at α = {alpha}: \"\n      f\"{'reject H0' if p_u &lt; alpha else 'do NOT reject H0'}\")\n\n\n\n9.4 7.3 Interpreting the test output\nThe previous cell reports three layers of information:\n\nDescriptive statistics for each arm\nFor both the low and high coffee arms we see:\n\nsample size (n)\n\nmean and standard deviation (SD) of BP change (ΔSBP)\n\nmedian and interquartile range (IQR)\n\nThese tell us how large the changes are and how much variability there is before we think about p-values.\nDifference in means with an approximate 95% confidence interval\n\nThe quantity labelled Δmean is the difference in mean BP change\n(high – low).\n\nThe 95% confidence interval (CI) tells us which values of the true mean difference are reasonably compatible with the data.\n\nIf the CI includes 0, then “no difference” is among the values compatible with the data; if it excludes 0, the data are less compatible with “no difference”.\n\nNote that the CI here is 95%, whereas our α threshold for tests is 0.0314. This is deliberate: 95% CIs are a familiar summary of uncertainty, while α can in principle be chosen differently.\nTwo hypothesis tests with different assumptions\n\nWelch t-test\n\nWorks on the original values.\n\nAssumes data in each group are approximately normal and that the mean is a sensible summary.\n\nAccounts for unequal variances between groups.\n\nThe output shows the t-statistic and its p-value.\n\nMann–Whitney U test\n\nWorks on the ranks rather than the raw numbers.\n\nMakes fewer assumptions about the shape of the distribution.\n\nMainly detects a shift in central tendency between groups.\n\nThe output shows the U-statistic and its p-value.\n\n\nFor each test the code prints a simple decision:\n\n“reject H0” or “do NOT reject H0 at α = 0.0314”.\n\nThis should not be read as a verdict of “true” or “false”, but as:\n\nwhether the observed data are unusually extreme if there were truly no difference between arms, under the assumptions of that test.\n\n\nTry to reconcile:\n\nwhat you see in the boxplots and notched boxplots,\n\nthe difference in means and its CI, and\n\nthe p-values and decisions.\n\nThey should tell a consistent story about the size, direction, and uncertainty of the effect.\n\n\n9.5 7.4 Paired t-test\nSo far we have compared two independent groups (e.g. low vs high coffee arm). Sometimes, however, the same individuals are measured twice, for example:\n\nbefore and after a dietary intervention\n\nleft vs right hand grip strength\n\nfasting vs postprandial glucose in the same participant\n\nIn this situation, the two measurements are not independent.\nEach person acts as their own control, and the natural unit of analysis is the difference within each participant.\nA paired t-test works as follows:\n\nFor each participant, compute\n\\[\nd_i = \\text{(after)} - \\text{(before)}.\n\\]\nTreat these differences $ d_i $ as a single sample.\nTest whether the mean difference is compatible with zero.\n\nKey points:\n\nThe paired t-test is more powerful than an independent-samples test\nwhen the before/after measurements are correlated (they usually are).\n\nIt assumes the distribution of the differences is approximately normal, not the raw values themselves.\n\nThe effect it estimates is the average within-person change.\n\nPaired designs are extremely common in nutrition studies (e.g. crossover trials, pre/post biomarker measurements), and recognising when to use the paired t-test is an important skill.\nNo code is shown here; this section is purely conceptual.\nQuestions\n\nDo the conclusions from the t-test and Mann–Whitney test agree at α = 0.0314?\n\nWould your conclusion change if you (arbitrarily) switched to α = 0.05?\n\nLooking back at the distributions, does a parametric or non-parametric test seem more appropriate?"
  },
  {
    "objectID": "notebooks/data_handling.html#comparing-more-than-two-groups",
    "href": "notebooks/data_handling.html#comparing-more-than-two-groups",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "10 8. Comparing more than two groups",
    "text": "10 8. Comparing more than two groups\nSo far we have compared two groups (low vs high coffee).\nWe now extend this to three groups, using blood glucose across the three cereal arms:\n\nbran\n\ncornflakes\n\nmuesli\n\nBefore running any tests, the workflow is the same:\n\nDescribe each group (n, mean, SD, median, IQR).\n\nInspect distributions visually.\n\nChoose an appropriate test.\n\n\n\n10.1 8.1 Parametric vs non-parametric tests for 3+ groups\n\n10.1.1 One-way ANOVA (parametric)\nANOVA asks whether at least one group mean differs from the others.\nKey assumptions:\n\nIndependence of observations\nEach participant contributes one observation; groups should not influence each other.\nApproximately normal distribution within each group\nAs with the t-test, ANOVA is reasonably robust with moderate sample sizes.\nHomogeneity of variances\nThe variability within each group should be roughly similar\n(large differences in SDs make ANOVA less reliable).\nOutcome measured on a continuous scale\nMeans and SDs must be meaningful summaries for the measurement.\n\nANOVA tells you whether there is evidence of differences in means,\nbut not which groups differ. (Post-hoc tests come later in statistics, not here.)\n\n\n\n10.1.2 Kruskal–Wallis test (non-parametric)\nA rank-based alternative that makes fewer assumptions.\nKey features:\n\nWorks on ranks rather than raw glucose values.\n\nDoes not assume normality.\n\nLess sensitive to outliers and skewness.\n\nTests whether distributions differ in central tendency.\n\nIf significant, it indicates that at least one group is different,\nbut does not specify which.\n\nKruskal–Wallis is appropriate when:\n\nData are clearly skewed,\n\ngroups have different variances, or\n\nthe outcome is not well-summarised by the mean.\n\n\n\n\n\n10.2 8.2 Descriptive statistics by cereal arm\nBefore thinking about p-values, we quantify and compare the groups.\n\n# ------------------------------------------------------------\n# 8(a). Descriptive statistics for glucose by cereal arm\n# ------------------------------------------------------------\n\ngroups = df[\"cereal_arm\"].unique()\nrows = []\n\nfor g in groups:\n    series = df.loc[df[\"cereal_arm\"] == g, \"glucose\"].dropna()\n    rows.append({\n        \"cereal_arm\": g,\n        \"n\": len(series),\n        \"mean\": series.mean(),\n        \"sd\": series.std(),\n        \"median\": series.median(),\n        \"q1\": series.quantile(0.25),\n        \"q3\": series.quantile(0.75),\n    })\n\nglucose_summary = pd.DataFrame(rows)\nglucose_summary[\"IQR\"] = glucose_summary[\"q3\"] - glucose_summary[\"q1\"]\nglucose_summary\n\n\n\n10.3 8.3 Visual comparison\nA plot is often more informative than a table for 3+ groups.\nBelow we use a boxplot for each cereal arm to show:\n\nthe median (central bar),\n\nIQR (box),\n\noverall spread,\n\npotential asymmetry or outliers.\n\nIf the shapes or spreads differ markedly, ANOVA assumptions may be strained, and Kruskal–Wallis may be more appropriate.\n\n# ------------------------------------------------------------\n# 8(b). Visual comparison: glucose by cereal arm\n# ------------------------------------------------------------\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.boxplot(data=df, x=\"cereal_arm\", y=\"glucose\", ax=ax)\n\n# Optional jitter for showing individual observations\nsns.stripplot(\n    data=df, x=\"cereal_arm\", y=\"glucose\",\n    ax=ax, color=\"black\", size=2, jitter=0.20, alpha=0.5\n)\n\nax.set_xlabel(\"Cereal arm\")\nax.set_ylabel(\"Blood glucose (arbitrary units)\")\nax.set_title(\"Blood glucose by cereal arm\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n10.4 8.4 Hypothesis tests: ANOVA and Kruskal–Wallis\nNow that we have:\n\nsample sizes,\n\nmeasures of central tendency and spread,\n\nand a visual sense of distributions,\n\nwe can formally test whether glucose differs across the cereal arms.\nWe apply:\n\nOne-way ANOVA (parametric): tests differences in means\n\nKruskal–Wallis (non-parametric): tests differences in ranked distributions\n\nNeither test tells us which arms differ — only whether “at least one group is different”.\n\n# ------------------------------------------------------------\n# 8(c). ANOVA and Kruskal–Wallis tests\n# ------------------------------------------------------------\n\n# Prepare glucose values grouped by cereal arm\ngroups_glucose = [group[\"glucose\"].dropna().values\n                  for _, group in df.groupby(\"cereal_arm\")]\n\n# One-way ANOVA\nf_stat, p_anova = st.f_oneway(*groups_glucose)\n\n# Kruskal–Wallis test\nh_stat, p_kw = st.kruskal(*groups_glucose)\n\nprint(\"Hypothesis tests for glucose across cereal arms\")\nprint(\"================================================\")\nprint(f\"One-way ANOVA (differences in means):\")\nprint(f\"  F-statistic = {f_stat:6.3f}\")\nprint(f\"  p-value     = {p_anova:6.4f}\")\n\nprint(\"\\nKruskal–Wallis test (differences in distributions):\")\nprint(f\"  H-statistic = {h_stat:6.3f}\")\nprint(f\"  p-value     = {p_kw:6.4f}\")\n\n\n\n10.5 8.5 Interpretation prompts\n\nDo the ANOVA and Kruskal–Wallis tests give similar conclusions?\n\nLooking at the boxplots, do the group shapes and spreads support the ANOVA assumptions?\n\nWhich test feels more appropriate for these data? Why?\n\nIf the result is significant, what follow-up analysis might be needed\n(not in this workbook) to identify which cereal arms differ?\n\nAs always, visual inspection and descriptive statistics should guide the choice of method — p-values come last, not first."
  },
  {
    "objectID": "notebooks/data_handling.html#comparing-categories-χ²-tests-and-a-warning",
    "href": "notebooks/data_handling.html#comparing-categories-χ²-tests-and-a-warning",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "11 9. Comparing categories: χ² tests and a warning",
    "text": "11 9. Comparing categories: χ² tests and a warning\nSometimes we want to know whether two categorical variables are associated, for example:\n\nIs the proportion of participants with high appetite different across test foods?\n\nTo answer this, we use a χ² test of independence applied to a contingency table.\n\n\n11.1 9.1 What the χ² test does\nA χ² test examines whether the observed counts in a table differ from the expected counts we would see if the two variables were independent.\n\nH₀ (null): the variables are independent\n\nH₁ (alternative): the variables are associated in some way\n\nThe test statistic compares:\n\\[\n\\sum \\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]\nLarge discrepancies lead to large χ² values and small p-values.\n\n\n\n11.2 9.2 Assumptions of the χ² test\nThe χ² test is simple, but it has important assumptions:\n\nIndependence of observations\nEach participant contributes to one cell only.\nExpected counts not too small\n\nA common rule: expected counts ≥ 5 in most cells.\n\nIf many expected counts are &lt; 5, the χ² approximation becomes unreliable.\n\nCategorical variables\nχ² is designed for counts, not continuous measurements.\n\n\n\n\n11.3 9.3 When χ² is not appropriate\n\nIf expected counts are small → use Fisher’s exact test (works well for 2×2 tables).\n\nIf categories are ordered (ordinal) → χ² ignores ordering.\n\nAlternatives include Cochran–Armitage trend test, or treating the variable as ordinal in a regression model.\n\n\n\n\n\n11.4 9.4 A caution about treating categories as numbers\n⚠️ Important reminder:\n- Categorical and ordinal variables should not be analysed as continuous without strong justification.\n- For example, treating a 5-point Likert scale as 1–5 and running a t-test\nassumes equal spacing between categories — often untrue and misleading.\n\n\n\n11.5 9.5 Workflow for a χ² test\n\nCreate categories if needed (e.g., high vs not-high appetite).\n\nConstruct a contingency table with pd.crosstab.\n\nCheck expected counts.\n\nRun the χ² test of independence.\n\nInterpret the p-value in the context of study design and effect size.\n\n(Optional: effect size such as Cramér’s V can summarise strength of association.)\nIn the code cell below we create the categories, build the contingency table, and apply the χ² test.\n\n\n\n11.6 How are expected counts calculated?\nFor a χ² test of independence, “expected” counts are the counts we would observe if the two categorical variables were independent.\nIn a contingency table with:\n\nrow totals\n\ncolumn totals\n\na grand total\n\nthe expected count for each cell is calculated as:\n\\[\n\\text{Expected}_{ij}\n= \\frac{(\\text{Row total}_i)\\;(\\text{Column total}_j)}{\\text{Grand total}}.\n\\]\nIn words:\n\nIf the variables were independent, the proportion in each column\nshould be the same across all rows.\n\nSo expected counts reflect the pattern we would see if nothing interesting were going on — just the product of the marginal proportions.\nA quick example:\nSuppose we have 100 participants:\n\n40 ate muesli, 60 did not\n\n30 had high appetite, 70 did not\n\nThen the expected count for\n“muesli and high appetite” is:\n\\[\n\\frac{40 \\times 30}{100} = 12.\n\\]\nThis is the value we would see on average if cereal choice and appetite were truly independent.\nThe χ² statistic then compares:\n\nObserved counts (what we see)\n\nExpected counts (what independence predicts)\n\nLarge discrepancies → large χ² → small p-value.\n\n# Create a simple categorical outcome: high vs not-high appetite\nhigh_cutoff = 70\ndf[\"appetite_high\"] = (df[\"appetite_vas\"] &gt;= high_cutoff).astype(int)\n\n# Contingency table: appetite_high by food_arm\ntable = pd.crosstab(df[\"appetite_high\"], df[\"food_arm\"])\ntable\n\n\n# ------------------------------------------------------------\n# 9(c). Chi-squared test of independence — detailed output\n# ------------------------------------------------------------\n\nalpha = 0.0314\n\nprint(\"Observed contingency table:\")\ndisplay(table)\n\n# Run chi-squared test\nchi2, p_chi, dof, expected = st.chi2_contingency(table)\n\nexpected_df = pd.DataFrame(\n    expected,\n    index=table.index,\n    columns=table.columns\n)\n\nprint(\"\\nExpected counts (if variables were independent):\")\ndisplay(expected_df)\n\n# Check assumption: any expected counts &lt; 5?\nsmall_counts = (expected &lt; 5).sum()\ntotal_counts = expected.size\n\nprint(f\"\\nExpected count check: {small_counts} of {total_counts} cells &lt; 5\")\nif small_counts &gt; 0:\n    print(\"⚠️  Some expected counts are small. χ² approximation may be unreliable.\")\n    print(\"    Consider Fisher’s exact test for 2×2 tables or collapsing categories.\\n\")\n\n\n\n# Summary of test results\nprint(\"Chi-squared test of independence\")\nprint(\"================================\")\nprint(f\"χ² statistic = {chi2:6.3f}\")\nprint(f\"Degrees of freedom = {dof}\")\nprint(f\"p-value = {p_chi:6.4f}\")\n\nprint(f\"\\nDecision at α = {alpha}: \"\n      f\"{'reject H0 (evidence of association)' if p_chi &lt; alpha else 'do NOT reject H0 (no clear evidence of association)'}\")\n\n\nQuestions\n\nDoes the chi-squared test suggest a difference in the proportion of high appetite across test foods (using α = 0.0314)?\n\nHow would you report this result in words?\n\nWhy is a chi-squared test more appropriate here than a t-test on the raw VAS scores split into two categories?"
  },
  {
    "objectID": "notebooks/data_handling.html#statistics-tools-not-an-oracle",
    "href": "notebooks/data_handling.html#statistics-tools-not-an-oracle",
    "title": "Data Handling and Basic Analysis (FB2NRP)",
    "section": "12 10. Statistics: tools, not an oracle",
    "text": "12 10. Statistics: tools, not an oracle\nFinally, a reminder:\n\nStatistical methods help us summarise uncertainty and quantify evidence.\n\nThey do not replace judgement about study design, data quality, or plausibility.\n\nA “significant” p-value does not guarantee truth, and a “non-significant” result does not prove there is no effect.\n\nWhen using these tools in real research, always consider:\n\nAre the data appropriate for the method?\n\nAre the assumptions at least approximately met?\n\nDo the results make sense in the context of other evidence?\n\nIf a friendly statistician or methodologist (or a small hippo) looked over your analysis, would they recognise the decisions you took and why?"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html",
    "href": "howto_sandbox/Introduction-Notebook.html",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "",
    "text": "Welcome — this notebook provides a brief introduction to Colab and Python. This can be used for practice, but also for reference. Like everything else in this module, this notebook assumes no prior Python experience. You will learn how to run cells, accept Colab warnings, and make tiny edits to code."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#what-does-python-code-look-like",
    "href": "howto_sandbox/Introduction-Notebook.html#what-does-python-code-look-like",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "1 0. What does Python code look like?",
    "text": "1 0. What does Python code look like?\n\n\nWhat is Python?\n\nPython is a popular, general-purpose programming language known for being readable, beginner-friendly, and widely used in science, data analysis, and education.\nKey points - Easy to read: clear, English-like syntax; indentation (spacing) shows structure. - Interpreted: you run code line-by-line (great for notebooks and exploration). - “Batteries included”: comes with many built-in tools; huge ecosystem of libraries. - Cross-platform: works on Windows, macOS, and Linux; also runs in the cloud (Colab).\nWhy we use it in FB2NEP - Excellent libraries for data work: NumPy (numbers), pandas (tables), Matplotlib (plots). - Integrates smoothly with Jupyter/Colab, so you can mix code + text + results. - Perfect for transparent, reproducible analysis.\nDo I need to be a programmer? No. You’ll use short, explained snippets to explore concepts. You’re learning with code, not learning to code per se.\nWhich version? We use Python 3 (the modern standard). Colab already provides it; local users can install it via Miniconda/Anaconda or venv.\nTiny example:\nprint(\"Hello from Python!\")\n\nPython is a set of instructions written line by line.\nAnything after a # on a line is a comment — it’s for humans to read and Python ignores it.\nExample:\n# This is a comment — it explains the next line\nx = 2 + 3            # ← comment at the end of a line\nprint(x)             # prints 5 on the screen\nYou run code by clicking the little ▶ button to the left of a cell, or by pressing Shift + Enter.\n\nIf you see a warning like “This notebook was not authored by Google”, click Run anyway.\nColab runs code safely in the cloud and won’t change anything on your computer.\n\n\n1.1 Python has commands and data\nWhen you run a cell, Python executes commands (instructions) that act on data (values).\n\nCommands: things like print(...), len(...), sum(...), or your own functions.\n\nData: numbers, text, lists, tables, etc., usually stored in variables (names you choose).\n\nTiny example:\nname = \"Fiona\"       # data (text)\nn = 3                # data (number)\nprint(\"Hello\", name) # command acting on data\nprint(\"n + 2 =\", n + 2)\n\n\n1.2 What do we mean by “data”?\nData are the values your code works with. Common kinds you’ll see here:\n\nNumbers: 3, 2.5 (counts, measurements)\nText (strings): \"apple\", \"F\" (labels, categories)\nBooleans: True / False (yes/no flags)\nLists: [\"Mon\",\"Tue\",\"Wed\"], [2, 1, 3] (ordered collections)\nDictionaries: {\"age\": 10, \"height_cm\": 140} (named pieces of data)\nTables (pandas DataFrame): spreadsheet-like rows & columns for analysis\n\nMini examples:\nage = 10                           # number\nfruit = [\"apple\", \"banana\"]        # list of text\nheights = {\"Alex\":150, \"Sam\":146}  # dictionary\nIn FB2NEP we mostly use numbers, text labels, and tables (pandas DataFrame) because they map naturally to real datasets (participants × variables). You will also encounter missing values (shown as NaN) — that simply means “no recorded value”."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#running-a-cell",
    "href": "howto_sandbox/Introduction-Notebook.html#running-a-cell",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "2 1. Running a cell",
    "text": "2 1. Running a cell\nClick the ▶ icon to the left of a cell, or press Shift + Enter.\nTry it now — run the cell below.\n\n# ----------------------------------------------\n# Adding two numbers\n# This cell has comments that explain each step.\n# Lines starting with '#' are comments and do not run.\n# ----------------------------------------------\n# 🦛 Friendly hippo says: Maths can be fun!\n2 + 2    # add two numbers\n\nNow edit the code above (e.g. change it to 2 + 3) and run it again. You should see the new result immediately."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#what-python-are-we-using",
    "href": "howto_sandbox/Introduction-Notebook.html#what-python-are-we-using",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "3 2. What Python are we using?",
    "text": "3 2. What Python are we using?\n\n# ----------------------------------------------\n# Show Python version and executable\n# ----------------------------------------------\nimport sys, platform   # import = bring tools into Python\nprint(\"Python executable:\", sys.executable)      # where Python lives in Colab\nprint(\"Python version:\", sys.version.split()[0])  # which Python version\n\n\n3.1 What are “libraries” (a.k.a. packages/modules)?\nLibraries are add-on toolboxes for Python. They provide ready-made functions so you don’t have to write everything from scratch.\n\nTerminology (you’ll see these used loosely):\n\nLibrary / package: the installable toolbox (e.g. pandas, NumPy).\nModule: a file inside a package (e.g. matplotlib.pyplot).\n\nInstall vs import:\n\nInstall (usually once per environment): makes the library available to Python.\nImport (every time you run a notebook): brings the library into your session.\n\nWhere do they “live”?\nIn your current environment/kernel (Colab’s runtime or your local conda/venv). If you install into one environment but run a notebook with a different kernel, imports will fail.\n\nCommon libraries in FB2NEP - NumPy (import numpy as np) — fast maths and arrays. - pandas (import pandas as pd) — tables (DataFrames), reading CSV/Excel, tidying. - Matplotlib (import matplotlib.pyplot as plt) — plots and figures.\nTypical workflow 1) (If needed) install\npython    # In notebooks, prefer %pip so it targets the current kernel    %pip -q install numpy pandas matplotlib 2) Import python    import numpy as np    import pandas as pd    import matplotlib.pyplot as plt 3) Use python    arr = np.array([1, 2, 3])    df = pd.DataFrame({\"x\": [1, 2, 3]})    plt.plot(df[\"x\"]); plt.show()\n\nColab note: many libraries are already installed. If you install new ones, Runtime → Restart runtime and re-run the notebook from the top.\n\nExample: how to use libraries.\n\n# Quick library demo (imports + versions)\nimport numpy as np, pandas as pd, matplotlib\nprint(\"NumPy:\", np.__version__)\nprint(\"pandas:\", pd.__version__)\nprint(\"Matplotlib:\", matplotlib.__version__)\n\n\n\n3.2 3. Variables: storing and using information\nA variable is a named container that holds a piece of information — such as text, numbers, or data.\nYou can give variables meaningful names (e.g. name, year) and then use them in commands.\nIn this example: - name = \"Jessica\" assigns a piece of text (called a string) to the variable name. - year = datetime.now().year automatically retrieves the current year from your computer or Colab environment. - print(\"Hello\", name, \"— welcome to FB2NEP\", year) combines both variables in one output line.\nVariables make your code flexible — if you change name, the message updates automatically.\n\n# ----------------------------------------------\n# Say hello with variables\n# ----------------------------------------------\nfrom datetime import datetime   # import module to get current date/time\nname = \"Jessica\"            # a text value (a 'string')\nyear = datetime.now().year  # a number\nprint(\"Hello\", name, \"— welcome to FB2NEP\", year)   # show something on the screen"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#working-with-small-collections-of-data-lists-and-simple-calculations",
    "href": "howto_sandbox/Introduction-Notebook.html#working-with-small-collections-of-data-lists-and-simple-calculations",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "4 4. Working with small collections of data: lists and simple calculations",
    "text": "4 4. Working with small collections of data: lists and simple calculations\nIn Python, a list is an ordered collection of values. It can hold numbers, text, or a mixture of both, and is written inside square brackets [ ], with items separated by commas.\nIn this example, we have a small list of numbers representing, for instance, the number of fruit portions eaten by four participants.\nWe then use a few built-in functions to calculate basic descriptive statistics:\n\nsum(nums) — adds up all numbers in the list.\n\nlen(nums) — counts how many items are in the list (its length).\n\ntotal / n — divides the total by the number of items to obtain the mean (average).\n\nprint(...) — displays the results clearly below the cell.\n\nYou can change the numbers, re-run the cell, and observe how the total and mean change accordingly.\n\n# ----------------------------------------------\n# List of numbers — total and mean\n# ----------------------------------------------\nnums = [3, 5, 8, 12]  # a list: an ordered box of values\ntotal = sum(nums)     # sum() adds up numbers\nn     = len(nums)     # Length of list = number of observations\nmean = total / n      # average = total divided by how many\nprint(\"Numbers:\", nums)\nprint(\"Total:\", total, \"Observations:\", n, \"Mean:\", mean)"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#first-steps-with-data",
    "href": "howto_sandbox/Introduction-Notebook.html#first-steps-with-data",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "5 5. First steps with data",
    "text": "5 5. First steps with data\nNumPy is the library Python uses for numbers and maths with arrays.\nAn array is like a list, but stored very efficiently — so you can do big calculations quickly.\nFor example, you can add two arrays together in one go, instead of looping through numbers.\nPandas actually uses NumPy under the hood to store and crunch the numbers inside its tables.\nPandas is a Python library that makes it easy to handle tables of data (like spreadsheets).\nThe main building block is the DataFrame, which is like a whole sheet with rows and columns.\nYou can put numbers or text inside, and then quickly look at, sort, filter, or calculate things.\nThink of pandas as your spreadsheet inside Python — but much more powerful and flexible.\n\n# ----------------------------------------------\n# Create a tiny table (DataFrame)\n# ----------------------------------------------\nimport numpy as np       # import = bring tools into Python\nimport pandas as pd\n\n# 🦛 Hippo hint: NumPy = numbers, pandas = tables\n\n# --- NumPy examples ---\n# Make a small NumPy array (like a list, but faster for maths)\narr = np.array([1, 2, 3, 4, 5])\nprint(\"Array:\", arr)\n\n# Do maths with the whole array at once\nprint(\"Array + 10:\", arr + 10)     # add 10 to each element\nprint(\"Array squared:\", arr ** 2)  # square every element\n\n\n# --- pandas example ---\n# Create a tiny DataFrame (like a mini spreadsheet)\ndf = pd.DataFrame({\n  \"age\": [8, 9, 10, 11, 12],\n  \"height_cm\": [130, 135, 140, 145, 150]\n})\n\n# Show the DataFrame\ndf\n\nChange a number in the table creation above (e.g. one of the heights) and run again. Do you see the change?"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#a-first-plot",
    "href": "howto_sandbox/Introduction-Notebook.html#a-first-plot",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "6 6. A first plot",
    "text": "6 6. A first plot\nMatplotlib is Python’s main tool for making graphs and pictures.\nIt lets you draw lines, bars, scatter plots, histograms, and much more.\nThink of it as the drawing toolbox:\n- You tell it what data to use (x and y values).\n- You can add labels, titles, and colours.\n- When you’re ready, plt.show() displays the finished picture.\n(pandas can also plot directly, but under the hood it still uses matplotlib).\n\n# ----------------------------------------------\n# Line plot of age vs height\n# ----------------------------------------------\nimport matplotlib.pyplot as plt   # plotting library\n\n# 🦛 Friendly hippo says: A picture tells a story!\nplt.plot(df[\"age\"], df[\"height_cm\"], marker=\"o\")   # draw a line with dots\nplt.xlabel(\"Age (years)\")   # label the x-axis\nplt.ylabel(\"Height (cm)\")   # label the y-axis\nplt.title(\"Example plot\")   # add a title\nplt.show()                   # display the picture"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#save-your-own-copy-important",
    "href": "howto_sandbox/Introduction-Notebook.html#save-your-own-copy-important",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "7 7. Save your own copy (important)",
    "text": "7 7. Save your own copy (important)\nGo to File → Save a copy in Drive. That creates a personal copy you can edit without affecting the teaching version."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#if-something-breaks",
    "href": "howto_sandbox/Introduction-Notebook.html#if-something-breaks",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "8 8. If something breaks",
    "text": "8 8. If something breaks\n\nRuntime → Restart runtime, then run cells again from the top.\n\nIf a library is missing, install it with !pip install ... then restart the runtime."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#some-further-reading",
    "href": "howto_sandbox/Introduction-Notebook.html#some-further-reading",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "9 9. Some further reading",
    "text": "9 9. Some further reading\nThese are ideas you’ll meet later. It’s okay if they feel new!\n\nList: an ordered collection that you can change.\n\nTuple: like a list, but fixed once created.\n\nDictionary (dict): pairs of key: value (like a mini address book).\n\nSet: a bag of unique items, no duplicates.\n\nOOP (Object-Oriented Programming): a way to bundle data and behaviour together.\n\n\n# ----------------------------------------------\n# Advanced containers (lists, tuples, dicts, sets)\n# ----------------------------------------------\n\n# List: ordered, changeable\nfruits = [\"apple\", \"banana\", \"pear\"]\nfruits.append(\"apple\")        # duplicate allowed\n# 🦛 notice: lists keep order and allow repeats\nprint(\"List:\", fruits)\n\n# Tuple: ordered, not changeable (immutable)\ncoords = (51.44, -0.94)       # (lat, lon) for Reading-ish\nprint(\"Tuple:\", coords)\n\n# Dict: key → value mapping\nheights = {\"Alex\": 150, \"Sam\": 145}\nheights[\"Sam\"] = 146          # update a value\nprint(\"Dict:\", heights)\n\n# Set: unique items, no order\nunique_fruits = set(fruits)\nprint(\"Set (unique):\", unique_fruits)\n\n\n9.1 A tiny taste of OOP (Object-Oriented Programming)\nThink of OOP as a way to bundle data and the things you can do with that data into one unit.\n\n9.1.1 Core ideas (mapped to our example)\n\nClass → a blueprint that defines what data an object has and what it can do.\nExample: Hippo is the blueprint.\nObject / instance → a concrete thing made from a class.\nExample: h = Hippo(\"Hildegard\", favourite_food=\"lotus leaves\").\nAttributes → the data an object stores.\nExample: self.name, self.favourite_food.\nMethods → the behaviours (functions) an object can perform.\nExample: introduce(), eat(food).\n__init__ → the constructor: runs when you create a new object, initialising its attributes.\nself → the current object; it lets methods access the object’s own data.\n\n\n\n9.1.2 Why OOP can be useful\n\nEncapsulation: keep related data and actions together (tidy, easier to reason about).\nReusability: create many objects from one blueprint (e.g., lots of hippos with different names).\nExtensibility: add new methods or attributes later without breaking existing code.\n\n\n\n9.1.3 Reading the Hippo example\n\nThe class defines what every hippo has (name, favourite_food) and what every hippo does (introduce, eat).\nWhen we write h = Hippo(\"Hildegard\", ...), Python calls __init__ to set up that object’s data.\nCalling h.eat(\"cabbage\") runs the eat method on that object’s data (via self).\n\n\n\n9.1.4 When should you use OOP here?\nFor most FB2NEP notebooks, functions + data frames are enough.\nUse a small class only when it clarifies structure (e.g., simulating participants, instruments, or study arms with shared behaviour).\n\n\n\nOptional: two more OOP ideas\n\n\nInheritance: make a new class that extends another (e.g., BabyHippo(Hippo)).\nPolymorphism: different classes can share a method name but behave differently (e.g., eat for different animals).\n\nYou won’t need these for this module, but you’ll see them in larger software projects.\n\n\n\n\n9.2 Try it\n\nMake another object: h2 = Hippo(\"Fiona\"); call h2.introduce().\nChange the behaviour: inside eat, add a line to count how many times the hippo has eaten.\nAdd an attribute: e.g., age_years, and show it in introduce().\n\n\n# ----------------------------------------------\n# A tiny class: Hippo — with data and behaviours\n# ----------------------------------------------\nclass Hippo:\n  # The special __init__ method sets up new hippos\n  def __init__(self, name, favourite_food=\"water plants\"):\n    self.name = name                    # data (an attribute)\n    self.favourite_food = favourite_food\n\n  # A behaviour (a method): the hippo can introduce itself\n  def introduce(self):\n    print(f\"Hello, I'm {self.name} the hippo. I like {self.favourite_food}.\")\n\n  # Another behaviour\n  def eat(self, food):\n    if food == self.favourite_food:\n      print(f\"Yum! {self.name} loves {food}. 🦛\")\n    else:\n      print(f\"{self.name} tries {food}… not bad!\")\n\n# Make an object (an instance) from the class\nh = Hippo(\"Hildegard\", favourite_food=\"lotus leaves\")\nh.introduce()      # call a method\nh.eat(\"lotus leaves\")\nh.eat(\"cabbage\")"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#further-information-resources",
    "href": "howto_sandbox/Introduction-Notebook.html#further-information-resources",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "10 10. Further information & resources",
    "text": "10 10. Further information & resources\nIf you’d like to explore beyond this notebook:\n\n📘 Course-specific projects\nData Analysis Projects — examples, workflows, and ideas for project students.\n🌐 Official documentation:\n\nNumPy — arrays and fast maths.\n\npandas — data tables and analysis.\n\nmatplotlib — plotting and visualisation.\n\n💡 Gentle tutorials:\n\nW3Schools Python Tutorial (step-by-step basics).\n\nReal Python (clear, example-driven articles).\n\nGoogle Colab Guide (how Colab works).\n\n\n(These are optional reading — you don’t need to learn everything at once!)"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#loading-data-reference",
    "href": "howto_sandbox/Introduction-Notebook.html#loading-data-reference",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "11 11. Loading data (reference)",
    "text": "11 11. Loading data (reference)\n\nGood to know: In FB2NEP teaching notebooks, data are loaded automatically. You do not need to do this yourself for the module activities. The examples below are here in case you want to practise with your own files.\n\n\n11.1 Option A — Load a small CSV from the web (GitHub raw)\nUse this for public CSVs hosted online (fastest way to test).\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/ggkuhnle/fb2nep-epi/main/data/synthetic/fb2nep.csv\"  # replace\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n11.2 Option B — Upload a file from your computer (Colab)\nUncomment and run the lines below, then pick a CSV. Colab will store it temporarily in the session.\n\n# from google.colab import files\n# up = files.upload()                            # choose a file\n# import io\n# import pandas as pd\n# name = next(iter(up))                          # first uploaded filename\n# df = pd.read_csv(io.BytesIO(up[name]))\n# df.head()\n\n\n\n11.3 Option C — Load/save via Google Drive (persistent across Colab sessions)\nMount Drive, then read/write using a path in your Drive.\n\n# from google.colab import drive\n# drive.mount('/content/drive')\n# import pandas as pd\n# df = pd.read_csv('/content/drive/MyDrive/fb2nep/example.csv')\n# df.head()\n\n# Save back to Drive\n# df.to_csv('/content/drive/MyDrive/fb2nep/output.csv', index=False)\n\n\n\n11.4 Quick checks (useful with any dataset)\n\nWhat are the dimensions? What columns exist? Any missing values?\n\n\nprint(\"Shape:\", df.shape)\ndf.info()\ndf.isna().mean()  # fraction missing per column\n\n\n\n11.5 Optional — Save a copy locally (Colab session storage)\nFiles saved here can be downloaded from the Files pane on the left.\n\ndf.to_csv('my_output.csv', index=False)\nprint(\"Saved as my_output.csv — in Colab, open the Files pane → three dots → Download.\")"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html",
    "href": "howto_sandbox/python-cheatsheet.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#imports",
    "href": "howto_sandbox/python-cheatsheet.html#imports",
    "title": "",
    "section": "1.1 0) Imports",
    "text": "1.1 0) Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Stats & modelling (install if missing)\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula_api as smf\nIf a library is missing in Colab:\n!pip -q install statsmodels\n# then: Runtime → Restart runtime"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#load-save-data",
    "href": "howto_sandbox/python-cheatsheet.html#load-save-data",
    "title": "",
    "section": "1.2 1) Load / save data",
    "text": "1.2 1) Load / save data\n# CSV from local upload or Drive\ndf = pd.read_csv(\"my_data.csv\")\n\n# CSV from GitHub (raw)\nurl = \"https://raw.githubusercontent.com/ggkuhnke/fb2nep-eoi/main/data/synthetic/fb2nep.csv\"\ndf = pd.read_csv(url)\n\n# Save\ndf.to_csv(\"output.csv\", index=False)"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#quick-look",
    "href": "howto_sandbox/python-cheatsheet.html#quick-look",
    "title": "",
    "section": "1.3 2) Quick look",
    "text": "1.3 2) Quick look\ndf.head()\ndf.tail()\ndf.shape\ndf.info()\ndf.describe(include=\"all\")\ndf[\"sex\"].value_counts(dropna=False)\ndf.isna().mean()  # fraction missing per column"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#select-filter-transform",
    "href": "howto_sandbox/python-cheatsheet.html#select-filter-transform",
    "title": "",
    "section": "1.4 3) Select / filter / transform",
    "text": "1.4 3) Select / filter / transform\n# Columns\ndf[[\"age\", \"bmi\"]]\n\n# Rows\ndf[df[\"age\"] &gt;= 50]\n\n# New columns\ndf[\"bmi_sq\"] = df[\"bmi\"] ** 2\n\n# Rename\ndf = df.rename(columns={\"cholesterol\": \"chol\"})\n\n# Sort\ndf = df.sort_values([\"age\", \"bmi\"], ascending=[True, False])"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#grouping-summaries",
    "href": "howto_sandbox/python-cheatsheet.html#grouping-summaries",
    "title": "",
    "section": "1.5 4) Grouping & summaries",
    "text": "1.5 4) Grouping & summaries\ndf.groupby(\"group\")[\"bmi\"].mean()\ndf.groupby([\"group\", \"sex\"])[\"sbp\"].agg([\"mean\", \"std\", \"count\"])\n\n# Crosstab\npd.crosstab(df[\"group\"], df[\"sex\"], margins=True, normalize=\"index\")"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#plotting-quick",
    "href": "howto_sandbox/python-cheatsheet.html#plotting-quick",
    "title": "",
    "section": "1.6 5) Plotting (quick)",
    "text": "1.6 5) Plotting (quick)\ndf[\"bmi\"].hist(bins=20)\nplt.title(\"BMI distribution\")\nplt.xlabel(\"BMI\"); plt.ylabel(\"Count\")\nplt.show()\n\n# Boxplot by group\ndf.boxplot(column=\"sbp\", by=\"group\")\nplt.suptitle(\"\"); plt.title(\"SBP by group\"); plt.xlabel(\"group\"); plt.ylabel(\"SBP\")\nplt.show()\n\n# Scatter\nplt.scatter(df[\"bmi\"], df[\"sbp\"])\nplt.xlabel(\"BMI\"); plt.ylabel(\"SBP\"); plt.show()"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#basic-stats",
    "href": "howto_sandbox/python-cheatsheet.html#basic-stats",
    "title": "",
    "section": "1.7 6) Basic stats",
    "text": "1.7 6) Basic stats\n# Two-sample t-test\na = df.loc[df[\"group\"] == \"A\", \"sbp\"]\nb = df.loc[df[\"group\"] == \"B\", \"sbp\"]\nstats.ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n\n# Chi-square test on a 2x2\ntab = pd.crosstab(df[\"group\"], df[\"sex\"])\nstats.chi2_contingency(tab)"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#simple-models-statsmodels",
    "href": "howto_sandbox/python-cheatsheet.html#simple-models-statsmodels",
    "title": "",
    "section": "1.8 7) Simple models (statsmodels)",
    "text": "1.8 7) Simple models (statsmodels)\n# OLS regression\nmodel = smf.ols(\"sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit()\nprint(model.summary())\n\n# Logistic regression (binary outcome)\n# e.g., 'high_sbp' is 0/1\nlogit = smf.logit(\"high_sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit()\nprint(logit.summary())"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#jupyter-basics",
    "href": "howto_sandbox/python-cheatsheet.html#jupyter-basics",
    "title": "",
    "section": "1.9 8) Jupyter basics",
    "text": "1.9 8) Jupyter basics\n\nRun cell: Shift + Enter\nInsert cell above/below: A / B\nInterrupt: stop button (■) or Kernel/Runtime → Interrupt\nRestart: Kernel/Runtime → Restart\nMarkdown cell: text with # headings, **bold**, lists, etc."
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#reproducibility",
    "href": "howto_sandbox/python-cheatsheet.html#reproducibility",
    "title": "",
    "section": "1.10 9) Reproducibility",
    "text": "1.10 9) Reproducibility\nSEED = 11088\nnp.random.seed(SEED)\n\nRecord: dataset version, random seed, and exact code you ran."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FB2NRP - Data handling",
    "section": "",
    "text": "This page links to the FB2NEP practical workbooks.\nEach badge opens the notebook in Google Colab in a new browser tab.\nYou may also download the notebooks and run them locally in Jupyter.",
    "crumbs": [
      "Home",
      "FB2NRP - Data handling"
    ]
  },
  {
    "objectID": "index.html#practical-workbooks",
    "href": "index.html#practical-workbooks",
    "title": "FB2NRP - Data handling",
    "section": "",
    "text": "This page links to the FB2NEP practical workbooks.\nEach badge opens the notebook in Google Colab in a new browser tab.\nYou may also download the notebooks and run them locally in Jupyter.",
    "crumbs": [
      "Home",
      "FB2NRP - Data handling"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html",
    "href": "howto_sandbox/index.html",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "",
    "text": "This page shows you how to open and use the module notebooks in Google Colab (or locally in Jupyter). It includes direct links and easy to understand guidance. If you have never used Python before, you are in the right place."
  },
  {
    "objectID": "howto_sandbox/index.html#quick-links",
    "href": "howto_sandbox/index.html#quick-links",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "1 Quick links",
    "text": "1 Quick links\nOpen the notebooks directly in Colab:\n\nIntroduction Notebook\n · View on GitHub\nPlayground (Sandbox)\n · View on GitHub\n\nPrintable cheat‑sheet:\n- python‑cheatsheet.md (open “Raw”, then save/print)\n\nIf you fork or use a different repository, update the links by replacing ggkuhnle/fb2nep-epi with your repo slug.\n\n\n\nWhat does “fork” mean on GitHub?\n\nA fork is your own copy of someone else’s GitHub repository under your GitHub account.\nYou can change it freely without affecting the original. Your fork can still receive updates from the original (“upstream”).\nWhy fork? - You want your own version of the materials. - You don’t have write permission on the original repo. - You plan to customise and maybe propose changes back later (via pull requests).\nHow to fork (GitHub web): 1. Open the original repo (e.g. https://github.com/ggkuhnle/fb2nep-epi). 2. Click Fork (top-right) → choose your account → Create fork. 3. You now have https://github.com/&lt;YOUR-USER&gt;/&lt;YOUR-REPO&gt;.\nUpdate links after forking Replace the owner/repo part with your fork’s repo slug: - GitHub view:\nhttps://github.com/&lt;YOUR-USER&gt;/&lt;YOUR-REPO&gt;/blob/main/path/to/notebook.ipynb - Colab badge:\nhttps://colab.research.google.com/github/&lt;YOUR-USER&gt;/&lt;YOUR-REPO&gt;/blob/main/path/to/notebook.ipynb\nFork vs. clone vs. branch (quick) - Fork: makes your own repo on GitHub. - Clone: makes a local copy on your computer. - Branch: a line of development inside one repo (yours or the original).\nKeeping your fork up-to-date - On GitHub: open your fork and click Sync fork / Fetch upstream. - With Git (advanced):\n```bash git remote add upstream https://github.com/ggkuhnle/fb2nep-epi.git git fetch upstream git checkout main git merge upstream/main git push"
  },
  {
    "objectID": "howto_sandbox/index.html#how-to-open-a-notebook-in-colab-stepbystep",
    "href": "howto_sandbox/index.html#how-to-open-a-notebook-in-colab-stepbystep",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "2 How to open a notebook in Colab (step‑by‑step)",
    "text": "2 How to open a notebook in Colab (step‑by‑step)\n\nWhat is Colab? Colab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. Colab is especially well suited to machine learning, data science, and education.\n\n\nRight-clicking will open Colab in a new browser window.\n\n\nClick an Open in Colab button above. Colab will load the notebook from GitHub.\nAt the top‑right, click Connect if it isn’t already connected. This starts a fresh, temporary Python session (a “runtime”).\n\nYou may see a warning like “This notebook was not authored by Google.”\n\nClick Run anyway. Our notebooks are plain text and safe to run.\n\n\nYou might also see “Warning: This notebook requires permissions to run.”\n\nClick Run anyway. Colab sandboxes code; nothing runs on your computer.\n\n\nTo run a cell, click the small ▶ button on its left, or press Shift + Enter.\n\nTo run every cell from top to bottom, go to Runtime → Run all.\n\nTo save your own editable copy, go to File → Save a copy in Drive. You now have a personal copy you can edit freely.\n\n\n2.1 Useful Colab options\n\nRuntime → Restart runtime resets the session if things get stuck.\n\nRuntime → Change runtime type (we use Python 3; no GPU needed).\n\nFile → Download lets you save the executed notebook as .ipynb or PDF."
  },
  {
    "objectID": "howto_sandbox/index.html#working-locally-in-jupyter-optional",
    "href": "howto_sandbox/index.html#working-locally-in-jupyter-optional",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "3 Working locally in Jupyter (optional)",
    "text": "3 Working locally in Jupyter (optional)\nIf you prefer local execution: 1. Install Anaconda or Miniconda, then open Jupyter Lab.\n2. Download the .ipynb files from GitHub and open them in Jupyter Lab.\n3. Run cells with Shift + Enter as in Colab."
  },
  {
    "objectID": "howto_sandbox/index.html#troubleshooting",
    "href": "howto_sandbox/index.html#troubleshooting",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "4 Troubleshooting",
    "text": "4 Troubleshooting\n\nModuleNotFoundError (e.g. statsmodels): run the cell that starts with !pip install ..., then Runtime → Restart runtime, and re‑run the code.\n\nKernel crashed / out of memory: restart the runtime. Don’t open huge datasets in this sandbox.\n\nLong‑running cells: click the stop icon (■) next to the cell number, or Runtime → Interrupt execution."
  },
  {
    "objectID": "howto_sandbox/index.html#what-you-will-learn-here",
    "href": "howto_sandbox/index.html#what-you-will-learn-here",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "5 What you will learn here",
    "text": "5 What you will learn here\n\nHow to run Python cells safely in the cloud (Colab).\n\nHow to make small edits and immediately see their effect.\n\nHow to generate simple synthetic data and make basic plots & analyses you’ll reuse in FB2NEP.\n\nNow jump into the Introduction Notebook first, then play in the Playground."
  },
  {
    "objectID": "howto_sandbox/Playground.html",
    "href": "howto_sandbox/Playground.html",
    "title": "Playground (Sandbox)",
    "section": "",
    "text": "This is a safe space to experiment. It generates a small synthetic dataset so you can practise plotting and simple analyses used in FB2NEP.\n# Reset & verify environment (run first if anything is odd)\nimport sys, numpy as np, pandas as pd, matplotlib\nprint(\"Python:\", sys.version.split()[0])\nprint(\"NumPy:\", np.__version__, \"| pandas:\", pd.__version__, \"| Matplotlib:\", matplotlib.__version__)\nSEED = 11088  # FB2NEP reproducibility convention\nnp.random.seed(SEED)\n# Setup: import required libraries for the sandbox\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npd.set_option(\"display.max_columns\", 50)\n# If a package is missing in Colab, uncomment and run, then Runtime → Restart runtime\n# %pip -q install statsmodels scipy"
  },
  {
    "objectID": "howto_sandbox/Playground.html#parameters-change-these-and-re-run",
    "href": "howto_sandbox/Playground.html#parameters-change-these-and-re-run",
    "title": "Playground (Sandbox)",
    "section": "1 1) Parameters — change these and re-run",
    "text": "1 1) Parameters — change these and re-run\n\n# ▶ Try changing N or the effect sizes and re-run the next cells\nN = 400          # sample size\nRATIO_F = 0.6    # fraction female\nGROUP_EFFECT = -3.5  # mean SBP difference (B vs A), in mmHg\n\nnp.random.seed(SEED)"
  },
  {
    "objectID": "howto_sandbox/Playground.html#generate-a-simple-dataset",
    "href": "howto_sandbox/Playground.html#generate-a-simple-dataset",
    "title": "Playground (Sandbox)",
    "section": "2 2) Generate a simple dataset",
    "text": "2 2) Generate a simple dataset\n\nages = np.random.normal(45, 12, N).round(1)\nbmi  = np.random.normal(26, 4, N).round(1)\n\nsex = np.where(np.random.rand(N) &lt; RATIO_F, \"F\", \"M\")\ngroup = np.where(np.random.rand(N) &lt; 0.5, \"A\", \"B\")\n\n# SBP depends on age, BMI, sex, and group (B has lower mean by GROUP_EFFECT)\nbase = 110 + 0.35*ages + 0.9*bmi + np.where(sex==\"M\", 4.0, 0.0)\nsbp = base + np.where(group==\"B\", GROUP_EFFECT, 0.0) + np.random.normal(0, 8, N)\n\n# Total cholesterol (mmol/L) loosely related to age/BMI\nchol = 3.8 + 0.015*ages + 0.05*bmi + np.random.normal(0, 0.4, N)\n\n# Binary outcome: high SBP (≥140)\nhigh_sbp = (sbp &gt;= 140).astype(int)\n\ndf = pd.DataFrame({\n    \"age\": ages,\n    \"bmi\": bmi,\n    \"sex\": sex,\n    \"group\": group,\n    \"sbp\": sbp.round(1),\n    \"chol\": chol.round(2),\n    \"high_sbp\": high_sbp\n})\ndf.head()"
  },
  {
    "objectID": "howto_sandbox/Playground.html#quick-exploration",
    "href": "howto_sandbox/Playground.html#quick-exploration",
    "title": "Playground (Sandbox)",
    "section": "3 3) Quick exploration",
    "text": "3 3) Quick exploration\nRun the following cells; then try changing parameters above (e.g. N, GROUP_EFFECT) and re-run.\n\nprint(\"Shape:\", df.shape)\ndf.info()\n\n\ndf.describe(include=\"all\")\n\n\ndf['sex'].value_counts(), df['group'].value_counts()\n\n\ndf.isna().mean()  # missingness per column"
  },
  {
    "objectID": "howto_sandbox/Playground.html#plots",
    "href": "howto_sandbox/Playground.html#plots",
    "title": "Playground (Sandbox)",
    "section": "4 4) Plots",
    "text": "4 4) Plots\n\n# Histogram of SBP\ndf['sbp'].hist(bins=25)\nplt.xlabel(\"SBP (mmHg)\"); plt.ylabel(\"Count\"); plt.title(\"SBP distribution\")\nplt.tight_layout()\nplt.show()\n\n\n# Boxplot by group\ndf.boxplot(column=\"sbp\", by=\"group\")\nplt.suptitle(\"\")\nplt.title(\"SBP by group\"); plt.xlabel(\"group\"); plt.ylabel(\"SBP (mmHg)\")\nplt.tight_layout()\nplt.show()\n\n\n# Scatter: BMI vs SBP\nplt.scatter(df['bmi'], df['sbp'])\nplt.xlabel(\"BMI\"); plt.ylabel(\"SBP (mmHg)\"); plt.title(\"BMI vs SBP\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "howto_sandbox/Playground.html#table-1-style-summary",
    "href": "howto_sandbox/Playground.html#table-1-style-summary",
    "title": "Playground (Sandbox)",
    "section": "5 5) Table 1-style summary",
    "text": "5 5) Table 1-style summary\n\n# Continuous variables by group\ncont = df.groupby('group')[['age','bmi','sbp','chol']].agg(['mean','std','count'])\ncont\n\n\n# Categorical variables by group\npd.crosstab(df['group'], df['sex'], margins=True, normalize='index')"
  },
  {
    "objectID": "howto_sandbox/Playground.html#basic-hypothesis-tests",
    "href": "howto_sandbox/Playground.html#basic-hypothesis-tests",
    "title": "Playground (Sandbox)",
    "section": "6 6) Basic hypothesis tests",
    "text": "6 6) Basic hypothesis tests\n\n# Two-sample t-test: SBP between A and B\na = df.loc[df['group']=='A','sbp']\nb = df.loc[df['group']=='B','sbp']\nstats.ttest_ind(a, b, equal_var=False)\n\n\n# Chi-square test: sex distribution by group\ntab = pd.crosstab(df['group'], df['sex'])\ntab, stats.chi2_contingency(tab)[:2]  # (table, (chi2, p))"
  },
  {
    "objectID": "howto_sandbox/Playground.html#simple-models",
    "href": "howto_sandbox/Playground.html#simple-models",
    "title": "Playground (Sandbox)",
    "section": "7 7) Simple models",
    "text": "7 7) Simple models\nWe’ll use statsmodels with formula syntax. C(var) treats a variable as categorical.\n\n# OLS: SBP ~ age + BMI + sex + group\nols = smf.ols(\"sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit()\nols.summary()\n\n\n# Logistic regression: high_sbp (0/1) ~ predictors\nlogit = smf.logit(\"high_sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit(disp=False)\nlogit.summary()"
  },
  {
    "objectID": "howto_sandbox/Playground.html#data-io-helpers-optional-colab",
    "href": "howto_sandbox/Playground.html#data-io-helpers-optional-colab",
    "title": "Playground (Sandbox)",
    "section": "8 8) Data I/O helpers (optional, Colab)",
    "text": "8 8) Data I/O helpers (optional, Colab)\n\n# Upload a CSV from your computer (Colab only)\n# from google.colab import files\n# up = files.upload()  # pick a file\n# import pandas as pd\n# df = pd.read_csv(next(iter(up.keys())))\n# df.head()\n\n\n# Mount your Google Drive (persistent files across sessions)\n# from google.colab import drive\n# drive.mount('/content/drive')\n# Example save path:\n# df.to_csv('/content/drive/MyDrive/fb2nep/sandbox_output.csv', index=False)"
  },
  {
    "objectID": "howto_sandbox/Playground.html#export-your-work",
    "href": "howto_sandbox/Playground.html#export-your-work",
    "title": "Playground (Sandbox)",
    "section": "9 9) Export your work",
    "text": "9 9) Export your work\n\n# Save your sandbox dataset/results locally in this session\ndf.to_csv(\"sandbox_output.csv\", index=False)\nprint(\"Saved as sandbox_output.csv. In Colab, open the Files panel (left) → three dots → Download.\")"
  },
  {
    "objectID": "howto_sandbox/Playground.html#your-turn-try-these",
    "href": "howto_sandbox/Playground.html#your-turn-try-these",
    "title": "Playground (Sandbox)",
    "section": "10 10) Your turn — try these",
    "text": "10 10) Your turn — try these\n\nChange GROUP_EFFECT to +2.0 (so group B has higher SBP) and re-generate. What happens to the t-test and model coefficients?\n\nIncrease N to 2000. Do p-values change? Why?\n\nAdd C(group):C(sex) interaction to the OLS formula. Does it help?\n\nCreate a new variable waist = 2.5*bmi + noise and see how it relates to SBP."
  }
]